{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import tomotopy as tp\n",
    "import sys\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"abstracts_processed.csv\"\n",
    "with open(input_path, \"rb\") as fp:   \n",
    "    # Unpickling\n",
    "    documents = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1 - HDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the hyperparameters\n",
    "tw = tp.TermWeight.ONE # term weighting scheme in TermWeight. The default value is TermWeight.ONE\n",
    "initial_k = 2\n",
    "min_cf=3 # minimum collection frequency of words. Words with a smaller collection frequency than min_cf are excluded from the model. The default value is 0, which means no words are excluded.\n",
    "min_df=0 # minimum document frequency of words. Words with a smaller document frequency than min_df are excluded from the model. The default value is 0, which means no words are excluded\n",
    "rm_top=5 # the number of top words to be removed. If you want to remove too common words from model, you can set this value to 1 or more. The default value is 0, which means no top words are removed.\n",
    "alpha = 0.1 # hyperparameter of Dirichlet distribution for document-topic\n",
    "eta = 0.01 # hyperparameter of Dirichlet distribution for topic-word\n",
    "gamma = 0.1 # concentration coeficient of Dirichlet Process for table-topic\n",
    "transform = None # a callable object to manipulate arbitrary keyword arguments for a specific topic model\n",
    "seed = 41 # random seed\n",
    "model_burn_in = 500 \n",
    "train_updates = 10000\n",
    "train_iter = 10\n",
    "save_path = \"hdp_model.bin\" #.bin format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tp.HDPModel(tw=tw, min_cf=min_cf, min_df=min_df, rm_top=rm_top, initial_k=initial_k, alpha=alpha, \n",
    "                    eta=eta, gamma=gamma, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding documents to the model \n",
    "for doc in documents: model.add_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.time()\n",
    "# training**\n",
    "model.burn_in = model_burn_in\n",
    "# initialising \n",
    "model.train(iter=0)\n",
    "print('Num docs:', len(model.docs), ', Vocab size:', len(model.used_vocabs), ', Num words:', model.num_words)\n",
    "print('Removed top words:', model.removed_top_words)\n",
    "print('Training...', file=sys.stderr, flush=True)\n",
    "# actual training \n",
    "t = []\n",
    "LLs = []\n",
    "for i in range(0, train_updates, train_iter):\n",
    "    model.train(train_iter)\n",
    "    if i%1000==0:print('Iteration: {}'.format(i))\n",
    "    t.append(i)\n",
    "    LLs.append(model.ll_per_word)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time elapsed: \"+ str(round(end - start,1))+\" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_HDP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2 - Online HDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import HdpModel\n",
    "from gensim.corpora import Dictionary\n",
    "random.seed = 11 #set the seed right away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test (80-20)\n",
    "print(len(documents))\n",
    "set1_size = int(0.3*len(documents)) \n",
    "set2_size = int(0.7*len(documents)) \n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "set1_docs = documents[0:set1_size]\n",
    "set2_docs = documents[set1_size:]\n",
    "\n",
    "len(set1_docs) + len(set2_docs) == len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the input in gensim format \n",
    "from copy import deepcopy\n",
    "\n",
    "def prepare_gensim_input(docs, old_dict=None):\n",
    "    # TODO: add possibility to remove extremes (too frequent/rare words)\n",
    "    dictionary = Dictionary(docs)\n",
    "    if old_dict is not None: \n",
    "        old_dict_copy = deepcopy(old_dict)\n",
    "        old_dict_copy.merge_with(dictionary)\n",
    "        dictionary = old_dict_copy\n",
    "    corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in docs] # bag of words corpus \n",
    "    return dictionary, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary1, corpus1 = prepare_gensim_input(set1_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OHDP model\n",
    "\n",
    "Online HDP provides <u>the speed of online variational Bayes with the modeling flexibility of the HDP</u>.<br>\n",
    "The idea behind Online variational Bayes in general is to optimize the variational objective function with stochastic optimization.<br>\n",
    "The challenge we face is that the existing coordinate ascent variational Bayes algorithms for the HDP require complicated approximation methods or numerical optimization. \n",
    "\n",
    "\n",
    "Look [here](https://radimrehurek.com/gensim/models/hdpmodel.html) for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYERARAMETERS \n",
    "alpha = 1 #(int, optional) – Second level concentration - below one leads to  more sparse solutions\n",
    "gamma = 1 #(int, optional) – First level concentration\n",
    "eta = 0.01 #(float, optional) – The topic Dirichlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "hdp = HdpModel(corpus1, dictionary1, max_chunks=None, \n",
    "               max_time=None, chunksize=256, kappa=1.0, tau=64.0, \n",
    "               K=15, T=150, alpha=alpha, gamma=gamma, eta=eta, scale=1.0, \n",
    "               var_converge=0.0001, outputdir=None, random_state=None)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time elapsed: \"+ str(round(end - start,1))+\" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hdp.get_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print 20 topics with top 10 most probable words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_string, strip_punctuation, strip_numeric\n",
    "\n",
    "def print_hdp_topics(model, num_words):\n",
    "    hdp_topics = model.show_topics(num_topics=200, num_words=num_words)\n",
    "\n",
    "    topics = []\n",
    "    filters = [lambda x: x.lower(), strip_punctuation, strip_numeric]\n",
    "    \n",
    "    print(\"Number of topics found: \"+ str(len(hdp_topics)))\n",
    "    print(\"\")\n",
    "    \n",
    "    for idx, topic in enumerate(hdp_topics):\n",
    "        print(\"Topic \"+str(idx)+\" -------\")\n",
    "        t = preprocess_string(topic[1], filters)\n",
    "        print(\" - \".join(t))\n",
    "        print(\"\")\n",
    "        topics.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_hdp_topics(hdp, num_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In depth exploration of results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2topics(topics_document, num_topics):\n",
    "    \"\"\" \n",
    "        topics_document : list of tuples (topic_id,weight)\n",
    "        \n",
    "    This function creates one row of the documents2topics matrix by \n",
    "    extracting all the topics relative to the input document. \"\"\"\n",
    "    \n",
    "    res = pd.DataFrame(columns=range(num_topics))\n",
    "    for topic_weight in topics_document[1:]:\n",
    "        res.loc[0, topic_weight[0]] = topic_weight[1]\n",
    "    return res\n",
    "\n",
    "def docs2topics(model, corpus):\n",
    "    \"\"\" Builds the docs2topics matrix. \"\"\"\n",
    "    K = len(hdp.get_topics()) # get number of topics\n",
    "    topicsfordocs = [model[doc] for doc in corpus]\n",
    "    matrix = pd.concat([doc2topics(doc, K) for doc in topicsfordocs]).reset_index(drop=True).fillna(0)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents2topics = docs2topics(hdp, corpus1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents2topics.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.sum(documents2topics, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which document are about topic 15\n",
    "documents2topics.sort_values(15, ascending=False)[15].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents2topics.loc[documents2topics.idxmax(axis=1).sort_values().index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of nonzeros in the rows \n",
    "np.mean(documents2topics.astype(bool).sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of nonzeros in the columns \n",
    "print(np.mean(documents2topics.astype(bool).sum(axis=0)))\n",
    "print(np.min(documents2topics.astype(bool).sum(axis=0)))\n",
    "print(np.max(documents2topics.astype(bool).sum(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns; \n",
    "sns.set(rc={'figure.figsize':(5,10)})\n",
    "sns.heatmap(documents2topics.loc[documents2topics.idxmax(axis=1).sort_values().index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the topics \n",
    "\n",
    "**Idea**: filter out \"bad\" topics, so as to retain only the topics pertinent to the collection. <br>\n",
    "Definition of bad: topics whose max weight across all the documents is below a tbd threshold.\n",
    "\n",
    "**Also**: we could try filter out the topics that are \"too general\", where the measure of generality is a function of the number of documents they appear and their respective weights there. \n",
    "\n",
    "**Note**: we probably need to re-adjust mixing weights after the filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(documents2topics.max(axis=0).sort_values() == 0) # number of topics not appearing in ANY document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(5,5))\n",
    "h= plt.hist(documents2topics.max(axis=0).sort_values(), bins=20, density=True, cumulative=True) \n",
    "plt.show(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "h= plt.hist(documents2topics.astype(bool).sum(axis=0)) \n",
    "plt.show(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents2topics.loc[:,(documents2topics.astype(bool).sum(axis=0) > 400)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the model - online step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary2, corpus2 = prepare_gensim_input(set2_docs, old_dict=dictionary1)\n",
    "# hdp.evaluate_test_corpus(test_corpus) - Returns the value of total likelihood obtained by evaluating the model for all documents in the test corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdp_new = deepcopy(hdp)\n",
    "\n",
    "# update the dictionary\n",
    "hdp_new.id2word = dictionary2\n",
    "\n",
    "# the m_lambda matrix is a (topic x word) matrix \n",
    "# we need to add a new column for each new word before updating the model \n",
    "new_words = len(dictionary2)-len(dictionary1) #the new words\n",
    "# we initialise the new words with weight 0 in the model\n",
    "m_lambda_new = np.hstack([hdp.m_lambda,np.zeros((150, new_words))])\n",
    "# TODO: check this with the gensim guys \n",
    "hdp_new.m_lambda = m_lambda_new\n",
    "\n",
    "# the m_Elogbeta matrix is a (topic x word) matrix \n",
    "m_Elogbeta_new = np.hstack([hdp.m_Elogbeta,np.zeros((150, new_words))])\n",
    "hdp_new.m_Elogbeta = m_Elogbeta_new\n",
    "\n",
    "# the m_timestamp vector is a (wordx1) matrix that 'Helps to keep track and perform lazy updates on lambda.'\n",
    "m_timestamp_new = np.hstack([hdp.m_timestamp,np.zeros((new_words))]).astype(int)\n",
    "hdp_new.m_timestamp = m_timestamp_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdp_new.update(corpus2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_hdp_topics(hdp_new, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the new topics distribution for the whole collection \n",
    "_,corpus_full = prepare_gensim_input(documents, old_dict=dictionary2)\n",
    "documents2topics_new = docs2topics(hdp_new, corpus_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(documents2topics_new.max(axis=0).sort_values() == 0) # number of topics not appearing in ANY document (second corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "h= plt.hist(documents2topics_new.max(axis=0).sort_values(), bins=20, density=True, cumulative=True) \n",
    "plt.show(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a closer look at the differences btw the old and the new topics distributions \n",
    "documents2topics_new.loc[1,:].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdp_new.print_topic(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents2topics.loc[0,:].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdp.print_topic(112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
