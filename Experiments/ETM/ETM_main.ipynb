{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "\n",
    "#/usr/bin/python\n",
    "from __future__ import print_function\n",
    "import time \n",
    "import argparse\n",
    "import pickle \n",
    "import numpy as np \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Torch-bearer\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from etm import ETM\n",
    "from utils import *\n",
    "from utils import nearest_neighbors, get_topic_coherence, get_topic_diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20494"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts = pd.read_csv(\"abstracts_eng.csv\") #Replace with latest version\n",
    "collection = list(abstracts['abstract'])\n",
    "len(collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training in batches \n",
    "The idea is to simulate the real-time data stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20494\n"
     ]
    }
   ],
   "source": [
    "seed = 11\n",
    "random.seed(seed)\n",
    "#random.shuffle(collection)\n",
    "\n",
    "print(len(collection))\n",
    "streaming_batch_size = 1000\n",
    "batch1 = collection[:streaming_batch_size]\n",
    "batch2 = collection[streaming_batch_size:2*streaming_batch_size]\n",
    "batch3 = collection[2*streaming_batch_size:3*streaming_batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "#TODO: remove punctuation\n",
    "stop_words = stopwords.words('english') # note: the words in this list are only lower case but distilbert tokenizer incorporates lower casing so we should be fine! read more here: https://huggingface.co/transformers/_modules/transformers/tokenization_distilbert_fast.html\n",
    "nouns = {x.name().split('.', 1)[0] for x in wn.all_synsets('n')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stops = []\n",
    "with open(\"stops.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in enumerate(f):\n",
    "        word = line[1].split()[0]\n",
    "        custom_stops.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The network components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The embedding layer\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## BERT -contextualised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the embeddings we'll use a pre-trained contextual model \n",
    "# Here we can play a bit to get a sense of its working \n",
    "\n",
    "#!pip install transformers \n",
    "from transformers import DistilBertTokenizerFast, DistilBertModel\n",
    "from transformers import BertTokenizerFast, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why DistilBer? <br>\n",
    "The DistilBERT model was proposed in the blog post [Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT](https://medium.com/huggingface/distilbert-8cf3380435b5), and the paper DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERTâ€™s performances as measured on the GLUE language understanding benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased', return_dict=True, output_hidden_states=True)\n",
    "inputs = tokenizer(\"Hello\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[1][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.last_hidden_state[0].data.cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(torch.stack(outputs[1][3:6], dim=0), dim=0).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now there's a bit of work to do to prepare the embedding matrix. <br>\n",
    "We want to use Bert to get the embedding for each word in our corpus. However, being Bert a contextual embedding we could end up with more than one embedding vector for each word. To solve this problem we associate each word a different token for each different embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will work during the developing phase on a subset of the collection because \n",
    "# otherwise the memory requirements will be to high.\n",
    "# We should later apply the same computation that I apply here to all the batches inside the \n",
    "# collection.\n",
    "subset_size = 10 # len(collection)\n",
    "collection_subset = collection[0:subset_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_collection = tokenizer(collection_subset, return_tensors=\"pt\", truncation=False, padding=True)\n",
    "tokenised_collection[\"input_ids\"][0] # we have added padding so that we can process the whole collection in a batch - we gain in speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_collection = model(**tokenised_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally start to build our vocabulary. <br>\n",
    "We'll look at each word and add it to the vocabulary only if there's not already the same embedding_vector in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = embedded_collection.last_hidden_state.size()\n",
    "padding_size = size[1]\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {} # vocabulary in the form (int,word) pairs\n",
    "idx2bertIdx = {} # each index in our vocabulary is mapped to the Bert vocabulary index \n",
    "set_of_embeddings = set()\n",
    "\n",
    "\n",
    "idx = 0 # initialise dictionary index \n",
    "new_token_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the idea of this for loop is the following: \n",
    "\n",
    "# input : [[4535,564,2342,...],[423423,32432,...],...] sequence of bert tokens for our collection \n",
    "#         [[embedding1, embedding2, ...],[...],...] and respective embedding vectors \n",
    "\n",
    "\n",
    "# output: [[0, 1, 2, 1, 4, ...], [0, 4, ...], ...] sequence of our tokens for our collection \n",
    "#         [[ embedding of token 0 ]\n",
    "#          [ embedding of token 1 ]\n",
    "#          [ embedding of token 2 ]\n",
    "#          [ embedding of token 3 ]\n",
    "#          [ embedding of token 4 ]\n",
    "#                   \n",
    "#                    ...\n",
    "#\n",
    "#          [ embedding of last token ]]\n",
    "\n",
    "# Note: different tokens in our vocabulary (say 1 and 3) can refer to the same word if the embedding vector is different \n",
    "\n",
    "# The model will take as input the new tokens' collection (in some processed form maybe) and the embedding matrix\n",
    "start = time.time()\n",
    "for i in range(subset_size):\n",
    "    t1 = time.time()\n",
    "    embedded_doc = embedded_collection.last_hidden_state[i][tokenised_collection[\"attention_mask\"][i].bool()] # removing padding using the attention mask \n",
    "    tokens_ids = tokenised_collection[\"input_ids\"][i]\n",
    "    new_token_ids_doc = []\n",
    "    for j,emb_vector in enumerate(embedded_doc):\n",
    "        token_id = tokens_ids[j].cpu().numpy() # bert current token \n",
    "        word = tokenizer.convert_ids_to_tokens([token_id])[0] # corresponding word\n",
    "        bool_list = [torch.all(torch.eq(emb_vector, other)) for other in set_of_embeddings]\n",
    "        if not any(bool_list): \n",
    "            # add new embedding to the set \n",
    "            set_of_embeddings.add(emb_vector)\n",
    "            # increase the index and save the word in the dictionary \n",
    "            idx2word[idx] = word # save it in our vocabulary\n",
    "            idx2bertIdx[idx] = token_id # save in id to bert id mapping\n",
    "            new_token_ids_doc += [idx]\n",
    "            idx += 1\n",
    "        else: # find the right id for the word and add it to our new tokenisation\n",
    "            word_id = list(idx2word.values()).index(word)\n",
    "            new_token_ids_doc += [word_id]\n",
    "    new_token_ids += [new_token_ids_doc]\n",
    "    t2 = time.time()\n",
    "    print(\"Document \"+str(i)+\" done. Time: \"+str(round(t2-t1,2))+\" s.\")\n",
    "end = time.time()\n",
    "print(\"Total time: \"+str(round(end-start,2))+\" s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set_of_embeddings) == len(idx2word.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now we write the loop that will process one batch in our collection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add 2 functionalities here: \n",
    "- **Cosine similarity clustering**. We add a filtering of the embedding vectors for each word based on cosine similarity, i.e. we only keep one of the vectors that are \"*too close*\" to each other. \n",
    "- **Stop-words removal**. We avoid to represent the stop-word in our embedding since they wouldn't anyway carry meaning for the topic and they would occupy a lot of memory since for [this](http://ai.stanford.edu/blog/contextual/) article they tend to be the most context dependent words in contextual embeddings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subset_cosine(doc_subset, tokenizer, model, set_of_embeddings, \n",
    "                          idx2word, new_token_ids, threshold=0.9):\n",
    "    \"\"\" \n",
    "    Processing of a subset of the batch using cosine similarity clustering. \n",
    "    \n",
    "    Parameters \n",
    "    ----- \n",
    "    doc_subset: list of documents (aka list of strings)\n",
    "    tokenizer: instance of Bert tokenizer \n",
    "    model: instance of Bert model \n",
    "    set_of_embeddings: set containing the embedding vectors already in the vocabulary \n",
    "    idx2word: vocabulary mapping our token ids to the corresponding word. \n",
    "            Notice that each word can be mapped to multiple token ids.\n",
    "    new_token_ids: representation of the collection with our token ids. \n",
    "    threshold: cosine similarity threshold. \n",
    "            Vectors with cosine similarity above the threshold are considered equal. \n",
    "            \n",
    "    Returns \n",
    "    -----\n",
    "    Updated versions of set_of_embeddings, idx2word and new_token_ids\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    tokenised_collection = tokenizer(doc_subset, return_tensors=\"pt\", padding=True)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(\"tokenisation done\")\n",
    "    pos_emb=tokenised_collection[\"input_ids\"]*0 # giving all the words same positional embedding\n",
    "    with torch.no_grad():\n",
    "        embedded_collection = model(**tokenised_collection, position_ids=pos_emb)\n",
    "        embedded_collection.requires_grad = False\n",
    "        lower_hiddens = torch.sum(torch.stack(embedded_collection[2][0:4], dim=0), dim=0)\n",
    "    # extract lower layers hidden states\n",
    "    #lower_hiddens = torch.sum(torch.stack(embedded_collection[1][3:6], dim=0), dim=0)\n",
    "\n",
    "    print(\"embeddings done\")\n",
    "    \n",
    "    \n",
    "    ##  preparing the variables we need ---------\n",
    "    if len(idx2word) == 0:idx = 0\n",
    "    else:idx = len(set_of_embeddings)\n",
    "        \n",
    "    cos = torch.nn.CosineSimilarity(dim = 0)\n",
    "    \n",
    "    subset_size = len(doc_subset)\n",
    "    start = time.time()\n",
    "    \n",
    "    ## processing the collection document by document ----------\n",
    "    for i in range(subset_size):\n",
    "        t1 = time.time()\n",
    "        embedded_doc = lower_hiddens[i][tokenised_collection[\"attention_mask\"][i].bool()] # removing padding using the attention mask \n",
    "        tokens_ids = tokenised_collection[\"input_ids\"][i]\n",
    "        new_token_ids_doc = []\n",
    "        \n",
    "        \n",
    "        for j,emb_vector in enumerate(embedded_doc):\n",
    "            \n",
    "            token_id = tokens_ids[j].cpu().numpy() # bert current token \n",
    "            word = tokenizer.convert_ids_to_tokens([token_id])[0] # corresponding word\n",
    "            # jump to the next token if the word is a stopword \n",
    "            if word not in nouns: continue \n",
    "            \n",
    "            if word not in idx2word.values(): # we add the embedding anyway if we haven't encountered that word previously \n",
    "                # add new embedding to the set \n",
    "                set_of_embeddings.add(emb_vector)\n",
    "                # increase the index and save the word in the dictionary \n",
    "                idx2word[idx] = word # save it in our vocabulary\n",
    "                new_token_ids_doc += [idx]\n",
    "                idx += 1\n",
    "            else: # find the right id for the word and add it to our new tokenisation\n",
    "                word_occurrences = [position for position, v in enumerate(list(idx2word.values())) if v == word]\n",
    "                word_embeddings = [list(set_of_embeddings)[occ] for occ in word_occurrences]\n",
    "                bool_list = [cos(emb_vector, other) >= threshold for other in word_embeddings] \n",
    "                if not any(bool_list): \n",
    "                    # add new embedding to the set \n",
    "                    set_of_embeddings.add(emb_vector)\n",
    "                    # increase the index and save the word in the dictionary \n",
    "                    idx2word[idx] = word # save it in our vocabulary\n",
    "                    new_token_ids_doc += [idx]\n",
    "                    idx += 1\n",
    "                else: \n",
    "                    word_id = list(idx2word.values()).index(word)\n",
    "                    new_token_ids_doc += [word_id]\n",
    "                \n",
    "        new_token_ids += [new_token_ids_doc]\n",
    "        t2 = time.time()\n",
    "        #if i%(subset_size//3)==0:print(\"Document \"+str(i)+\" done. Time: \"+str(round(t2-t1,2))+\" s.\")\n",
    "            \n",
    "    end = time.time()\n",
    "    print(\"Total time for the subset: \"+str(round(end-start,2))+\" s.\")\n",
    "    \n",
    "    return set_of_embeddings, idx2word, new_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING on a few example sentences \n",
    "idx2word = {} \n",
    "set_of_embeddings = set()\n",
    "idx = 0 \n",
    "new_token_ids = []\n",
    "\n",
    "sen1 = \"This mouse loves cheese\"\n",
    "sen2 = \"I am using this mouse as a pointer\"\n",
    "sen3 = \"This mouse is hungry\"\n",
    "doc = [sen1, sen2, sen3]\n",
    "set_of_embeddings, idx2word, new_token_ids = process_subset_cosine(doc, tokenizer, model, set_of_embeddings, idx2word, new_token_ids, threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch, subset_size, tokeniser, model):\n",
    "    \"\"\" Processing of a batch of documents in the collection. \"\"\"\n",
    "    \n",
    "    ## initialisation \n",
    "    idx2word = {} \n",
    "    set_of_embeddings = set()\n",
    "    idx = 0 \n",
    "    new_token_ids = []\n",
    "    subset_size = 25 \n",
    "    streaming_batch_size =len(batch)\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    ## processing the batch one subset at a time\n",
    "    for s in range(0,streaming_batch_size,subset_size):\n",
    "        print(\"Processing subset \"+str(s//subset_size + 1))\n",
    "        if s+subset_size < len(batch):batch_subset = batch[s:s+subset_size]\n",
    "        else: batch_subset = batch[s:]\n",
    "        set_of_embeddings, idx2word, new_token_ids = process_subset_cosine(batch_subset, tokenizer, model, \n",
    "                                                                           set_of_embeddings, idx2word, \n",
    "                                                                           new_token_ids, threshold=0.7)\n",
    "        print(\"Number of word vectors so far: \"+str(len(idx2word)))    \n",
    "        print()\n",
    "        \n",
    "    end = time.time()\n",
    "    print(\"Total time: \"+str(round(end-start,2))+\" s.\")\n",
    "    \n",
    "    return set_of_embeddings, idx2word, new_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased', return_dict=True, output_hidden_states=True)\n",
    "set_of_embeddings, idx2word, new_token_ids = process_batch(batch2, 25, tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "### Static word embeddings from Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_vocab_and_embedding(batch, tokeniser, model):\n",
    "    \"\"\" \n",
    "        Processing of a batch of documents in the collection.\n",
    "        We will directly go through all the batch in one pass and extract a vocabulary and a fixed embedding.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## initialisation \n",
    "    batch_size = len(batch)\n",
    "    idx2word = {} \n",
    "    set_of_embeddings = set()\n",
    "    idx = 0 \n",
    "    new_token_ids = []\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for i,doc in enumerate(batch): \n",
    "        new_token_ids_doc = []\n",
    "        \n",
    "        for word in word_tokenize(doc.lower()): \n",
    "            \n",
    "            # first check whether it's a stop-word \n",
    "            if word in stop_words or word in string.punctuation: continue\n",
    "            # then check if it's already in the dictionary \n",
    "            if word in idx2word.values():\n",
    "                word_id = list(idx2word.values()).index(word)\n",
    "                new_token_ids_doc += [word_id]\n",
    "            else: # we need to create the word in the vocabulary and in the embedding \n",
    "                token = tokenizer(word, return_tensors=\"pt\")\n",
    "                with torch.no_grad():\n",
    "                    emb_vector = model(**token)\n",
    "                    emb_vector = torch.sum(emb_vector[1][0][0][1:-1],axis=0) # selecting everything but BOS and EOS tags\n",
    "                # add new embedding to the set \n",
    "                set_of_embeddings.add(emb_vector)\n",
    "                # increase the index and save the word in the dictionary \n",
    "                idx2word[idx] = word # save it in our vocabulary\n",
    "                new_token_ids_doc += [idx]\n",
    "                idx += 1\n",
    "        \n",
    "        new_token_ids += [new_token_ids_doc]\n",
    "        \n",
    "        if i%(batch_size//3)==0:\n",
    "            print(str(i+1)+ \" documents done. Number of words found: \"+str(idx))\n",
    "            tmp = time.time()\n",
    "            print(\"Time so far: \"+str(round(tmp-start,2))+\" s.\")\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"Total time: \"+str(round(end-start,2))+\" s.\")\n",
    "    \n",
    "    return set_of_embeddings, idx2word, new_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased', return_dict=True, output_hidden_states=True)\n",
    "model.eval()\n",
    "print(\"ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 documents done. Number of words found: 85\n",
      "Time so far: 2.67 s.\n",
      "34 documents done. Number of words found: 1778\n",
      "Time so far: 60.04 s.\n",
      "67 documents done. Number of words found: 2874\n",
      "Time so far: 95.97 s.\n",
      "100 documents done. Number of words found: 3865\n",
      "Time so far: 128.34 s.\n",
      "Total time: 128.34 s.\n"
     ]
    }
   ],
   "source": [
    "set_of_embeddings, idx2word, new_token_ids = get_batch_vocab_and_embedding(batch2, tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe embeddings :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the glove embeddings and building the corresponding vocabulary \n",
    "idx2word = {}\n",
    "set_of_embeddings = []\n",
    "with open(\"glove.6B.300d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    idx = 0\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if word in stop_words or word in custom_stops: continue\n",
    "        try: \n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            set_of_embeddings.append(vector)\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            continue\n",
    "        idx2word[idx] = word\n",
    "        idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(idx2word)) -len(set_of_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(s) for s in set_of_embeddings]\n",
    "lens_in = ([i == 300 for i in lens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "embs_clean = list(compress(set_of_embeddings, lens_in))\n",
    "idx2word_clean = dict(compress(idx2word.items(), lens_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(idx2word_clean) == len(embs_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to binary \n",
    "with open(\"glove_vocab\", \"wb\") as fp: \n",
    "    pickle.dump(idx2word, fp)\n",
    "with open(\"glove_embedding\", \"wb\") as fp: \n",
    "    pickle.dump(set_of_embeddings, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenising the collection\n",
    "def tokenise_batch(batch, idx2word):\n",
    "    \"\"\" \n",
    "        Tokenisation of a batch of documents in the collection given a pre-computed vocabulary.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## initialisation \n",
    "    batch_size = len(batch)\n",
    "    total_words = 0\n",
    "    matched_words = 0\n",
    "    new_token_ids = []\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for i,doc in enumerate(batch): \n",
    "        new_token_ids_doc = []\n",
    "        \n",
    "        for word in word_tokenize(doc.lower()): \n",
    "            total_words +=1\n",
    "            \n",
    "            if word in idx2word.values():\n",
    "                matched_words+=1\n",
    "                word_id = list(idx2word.values()).index(word)\n",
    "                new_token_ids_doc += [word_id]\n",
    "        \n",
    "        new_token_ids += [new_token_ids_doc]\n",
    "        \n",
    "    \n",
    "        \n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"Total time: \"+str(round(end-start,2))+\" s.\")\n",
    "    print(\"Proportion of matched words: \"+str(round(matched_words/total_words,2)))\n",
    "    \n",
    "    return new_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_token_ids = tokenise_batch(batch, idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limiting the vocabulary \n",
    "We are now going to insert preprocessing steps to limit the vocabulary that we feed to etm in order to be able to work with larger batches of data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading from binary the glove vocabulary and embedding \n",
    "glove_vocab_path = \"glove_vocab2\"\n",
    "glove_embedding_path = \"glove_embedding2\"\n",
    "with open(glove_vocab_path, \"rb\") as fp:  \n",
    "    glove_vocab = pickle.load(fp)\n",
    "with open(glove_embedding_path, \"rb\") as fp: \n",
    "    glove_embedding = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following Blei indications we are going to do the following filtering steps: \n",
    "1. stop-words (base + extended)\n",
    "2. document frequency > 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_DF(batch):\n",
    "    \"\"\" Computes document frequencies for every word in the collection.\"\"\"\n",
    "    tokenised_batch = list(itertools.chain(*[set(word_tokenize(doc.lower())) for doc in batch]))\n",
    "    word2counts = Counter(tokenised_batch)\n",
    "    word2frequecies = {k: v/len(batch) for k, v in word2counts.items()}\n",
    "    return word2frequecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenising the collection\n",
    "def filter_vocab(batch, glove_vocab, glove_emb):\n",
    "    \"\"\" \n",
    "        Process the collection filtering out stopwords and too frequent words. \n",
    "        Returns the tokenised collection according to the filtered vocabulary. \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    ## initialisation \n",
    "    idx = 0\n",
    "    batch_size = len(batch)\n",
    "    total_words = 0\n",
    "    matched_words = 0\n",
    "    new_token_ids = []\n",
    "    idx2word = {}\n",
    "    embeddings = []\n",
    "    DFs = compute_DF(batch)\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for i,doc in enumerate(batch): \n",
    "        new_token_ids_doc = []\n",
    "        \n",
    "        for word in word_tokenize(doc.lower()): \n",
    "                     \n",
    "            if word in idx2word.values(): # the word is already present in our vocabulary \n",
    "                word_id = list(idx2word.values()).index(word)\n",
    "                new_token_ids_doc += [word_id]\n",
    "            else : # new word!\n",
    "                # check whether we want it \n",
    "                if word in stop_words or word in custom_stops or word in string.punctuation: continue \n",
    "                if DFs[word]>0.7: continue # filtering out words that appear in more than 70% of the documents \n",
    "                # okay we want it \n",
    "                total_words +=1\n",
    "                # now check whether we have an embedding in glove for it \n",
    "                if word in glove_vocab.values(): \n",
    "                    # and we have it! \n",
    "                    matched_words+=1 \n",
    "                    new_token_ids_doc += [idx]\n",
    "                    idx2word[idx]=word\n",
    "                    emb = glove_emb[list(glove_vocab.values()).index(word)]\n",
    "                    embeddings.append(emb)\n",
    "                    idx+=1\n",
    "\n",
    "        new_token_ids += [new_token_ids_doc]\n",
    "        \n",
    "        if i%(batch_size//3)==0:\n",
    "            print(str(i+1)+ \" documents done. Number of words found: \"+str(idx))\n",
    "            tmp = time.time()\n",
    "            print(\"Time so far: \"+str(round(tmp-start,2))+\" s.\")\n",
    "            \n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"Total time: \"+str(round(end-start,2))+\" s.\")\n",
    "    print(\"Proportion of matched words: \"+str(round(matched_words/total_words,2)))\n",
    "    \n",
    "    return new_token_ids, idx2word, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 documents done. Number of words found: 19\n",
      "Time so far: 0.31 s.\n",
      "334 documents done. Number of words found: 6671\n",
      "Time so far: 164.99 s.\n",
      "667 documents done. Number of words found: 9743\n",
      "Time so far: 274.83 s.\n",
      "1000 documents done. Number of words found: 11534\n",
      "Time so far: 363.93 s.\n",
      "Total time: 363.93 s.\n",
      "Proportion of matched words: 0.67\n"
     ]
    }
   ],
   "source": [
    "new_token_ids, idx2word, embeddings = filter_vocab(batch1, glove_vocab, glove_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the embedding matrix from complete 'set_of_embeddings'\n",
    "embedding = torch.stack(torch.tensor())\n",
    "embedding.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = embedding.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_output_path = \"vocab_etm\"\n",
    "embedding_output_path = \"embedding_etm\"\n",
    "new_collection_output_path = \"new_collection_etm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to binary \n",
    "with open(vocab_output_path, \"wb\") as fp: \n",
    "    pickle.dump(idx2word, fp)\n",
    "with open(embedding_output_path, \"wb\") as fp: \n",
    "    pickle.dump(embedding, fp)\n",
    "with open(new_collection_output_path, \"wb\") as fp: \n",
    "    pickle.dump(new_token_ids, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.767.883 (approx (MB)) bytes for 2k words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading from binary \n",
    "with open(vocab_output_path, \"rb\") as fp:  \n",
    "    idx2word = pickle.load(fp)\n",
    "with open(embedding_output_path, \"rb\") as fp: \n",
    "    embedding = pickle.load(fp)\n",
    "with open(new_collection_output_path, \"rb\") as fp: \n",
    "    new_token_ids = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "So now we finally have a vocabulary and an embedding matrix: everything is ready for our model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11534"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(idx2word)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the get_batch function so as to get the documents x vocabulary matrix \n",
    "# required by the model starting from the complete 'new_token_ids' list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(corpus, ind, vocab_size, device, emsize=300):\n",
    "    \"\"\"\n",
    "    This function takes as input a list of tokenised documents (corpus)\n",
    "    and the indices of the documents in the batch (ind)\n",
    "    and returns as output the torch tensor to feed into the net. \n",
    "    The list of documents defines the batch to work on. \n",
    "    \"\"\"\n",
    "    batch_size = len(ind)\n",
    "    data_batch = np.zeros((batch_size, vocab_size))\n",
    "    \n",
    "    for i, doc_id in enumerate(ind):\n",
    "        doc = corpus[doc_id]\n",
    "        L = len(doc)\n",
    "        if doc_id != -1:\n",
    "            for word_id in doc:\n",
    "                counts = doc.count(word_id)\n",
    "                data_batch[i, word_id] = counts\n",
    "    data_batch = torch.from_numpy(data_batch).float().to(device)\n",
    "    return data_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import utils\n",
    "import etm\n",
    "utils = importlib.reload(utils)\n",
    "etm = importlib.reload(etm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "### model-related arguments\n",
    "num_topics = 10\n",
    "rho_size = 300 # dimension of rho \n",
    "emb_size = 300 # dimension of embeddings \n",
    "t_hidden_size = 600 # dimension of hidden space of q(theta)\n",
    "theta_act = 'relu' # either tanh, softplus, relu, rrelu, leakyrelu, elu, selu, glu\n",
    "train_embeddings = False\n",
    "seed = 11\n",
    "\n",
    "### optimization-related arguments\n",
    "lr = 0.05\n",
    "lr_factor = 1.1 #divide learning rate by this\n",
    "epochs = 50 \n",
    "mode = \"train\"\n",
    "enc_drop = 0.0 # dropout rate on encoder\n",
    "clip = 0.0 # gradient clipping\n",
    "nonmono = 10 # number of bad hits allowed ...?\n",
    "weight_decay = 1.2e-6\n",
    "anneal_lr = False # whether to anneal the learning rate or not\n",
    "bow_norm = True # normalize the bows or not \n",
    "_optimizer = \"adagrad\"\n",
    "\n",
    "### evaluation, visualization, and logging-related arguments\n",
    "num_docs_train = 80\n",
    "num_words = 10 #number of words for topic viz'\n",
    "log_interval = 2 #when to log training\n",
    "visualize_every = 10 #when to visualize results\n",
    "tc = False # whether to compute topic coherence or not\n",
    "td = False # whether to compute topic diversity or not\n",
    "\n",
    "### data and file related arguments\n",
    "save_path = './results'\n",
    "batch_size = 80\n",
    "eval_batch_size = 20 #input batch size for evaluation\n",
    "load_from = \"\" #the name of the ckpt to run evaluation from\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device=\"cpu\"\n",
    "# Setting the random seed \n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "if mode == 'eval':\n",
    "    ckpt = load_from \n",
    "else:\n",
    "    ckpt = os.path.join(save_path, \n",
    "        'etm_K_{}_Htheta_{}_Optim_{}_Clip_{}_ThetaAct_{}_Lr_{}_Bsz_{}_RhoSize_{}_trainEmbeddings_{}'.format(\n",
    "         num_topics, t_hidden_size, _optimizer, clip, theta_act, lr, batch_size, rho_size, train_embeddings))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.0, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (alphas): Linear(in_features=300, out_features=10, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=11534, out_features=600, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=600, out_features=600, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=600, out_features=10, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=600, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "etm_model = etm.ETM(num_topics = num_topics, \n",
    "            vocab_size = vocab_size, \n",
    "            t_hidden_size = t_hidden_size, \n",
    "            rho_size = rho_size, \n",
    "            emsize = emb_size, \n",
    "            theta_act = theta_act, \n",
    "            embeddings = embedding,\n",
    "            train_embeddings = train_embeddings, \n",
    "            enc_drop = enc_drop).to(device)\n",
    "\n",
    "print('model: {}'.format(etm_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _optimizer == 'adam':\n",
    "    optimizer = optim.Adam(etm_model.parameters(), lr=lr, weight_decay=lr_factor)\n",
    "elif _optimizer == 'adagrad':\n",
    "    optimizer = optim.Adagrad(etm_model.parameters(), lr=lr, weight_decay=lr_factor)\n",
    "elif _optimizer == 'adadelta':\n",
    "    optimizer = optim.Adadelta(etm_model.parameters(), lr=lr, weight_decay=lr_factor)\n",
    "elif _optimizer == 'rmsprop':\n",
    "    optimizer = optim.RMSprop(etm_model.parameters(), lr=lr, weight_decay=lr_factor)\n",
    "elif _optimizer == 'asgd':\n",
    "    optimizer = optim.ASGD(etm_model.parameters(), lr=lr, t0=0, lambd=0., weight_decay=lr_factor)\n",
    "else:\n",
    "    print('Defaulting to vanilla SGD')\n",
    "    optimizer = optim.SGD(etm_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = new_token_ids[:num_docs_train]\n",
    "test_corpus = new_token_ids[num_docs_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, corpus, num_docs_train=num_docs_train, batch_size=batch_size, vocab_size=vocab_size, \n",
    "          bow_norm=bow_norm, clip=clip, log_interval=log_interval):\n",
    "    \"\"\" Just the training function ... \"\"\"\n",
    "    \n",
    "    model.train() #setting the model in training mode\n",
    "    # preparing all the data structures \n",
    "    acc_loss = 0\n",
    "    acc_kl_theta_loss = 0\n",
    "    cnt = 0\n",
    "    indices = torch.randperm(num_docs_train)\n",
    "    indices = torch.split(indices, batch_size)\n",
    "    \n",
    "    for idx, ind in enumerate(indices): # all our batches \n",
    "        optimizer.zero_grad()\n",
    "        data_batch = get_batch(corpus, ind, vocab_size, device)\n",
    "        sums = data_batch.sum(1).unsqueeze(1) # what are we summing ?? \n",
    "        \n",
    "        # maybe normalising the input \n",
    "        if bow_norm: normalized_data_batch = data_batch / sums\n",
    "        else: normalized_data_batch = data_batch\n",
    "        # loss on the batch \n",
    "        recon_loss, kld_theta = model(data_batch, normalized_data_batch)\n",
    "        total_loss = recon_loss + 0.01*kld_theta # weighting more the reconstruction loss \n",
    "        total_loss.backward() # compute backpropagation\n",
    "        # maybe clip the gradient \n",
    "        if clip > 0: torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step() # finally update the weights \n",
    "        # accumulate the total loss \n",
    "        acc_loss += torch.sum(recon_loss).item()\n",
    "        acc_kl_theta_loss += torch.sum(kld_theta).item()\n",
    "        cnt += 1\n",
    "        \n",
    "        # visualisation/print time! ('cur' stands for current ...)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            cur_loss = round(acc_loss / cnt, 2) \n",
    "            cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n",
    "            cur_real_loss = round(cur_loss + cur_kl_theta, 2)\n",
    "            print('Epoch: {} .. batch: {}/{} .. LR: {} .. KL_theta: {} .. Rec_loss: {} .. NELBO: {}'.format(\n",
    "                epoch, idx, len(indices), optimizer.param_groups[0]['lr'], cur_kl_theta, cur_loss, cur_real_loss))\n",
    "    \n",
    "    # Wrapping up the results of the epoch! \n",
    "    cur_loss = round(acc_loss / cnt, 2) \n",
    "    cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n",
    "    cur_real_loss = round(cur_loss + cur_kl_theta, 2)\n",
    "    print('-'*50)\n",
    "    print('Epoch----->{} .. LR: {} .. KL_theta: {} .. Rec_loss: {} .. NELBO: {}'.format(\n",
    "            epoch, optimizer.param_groups[0]['lr'], cur_kl_theta, cur_loss, cur_real_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(m, num_topics=num_topics, num_words=num_words, \n",
    "              vocab=glove_vocab, embeddings=glove_embedding, \n",
    "              show_emb=True, tokenizer=tokenizer, bert_model=model):\n",
    "    \"\"\" This is a cool visualisation function. \n",
    "    Takes as input the model so far and shows the discovered embeddings! \"\"\"\n",
    "    \n",
    "    # We're going to save our results here \n",
    "    # TODO: parametrize this path \n",
    "    if not os.path.exists('./results'):\n",
    "        os.makedirs('./results')\n",
    "\n",
    "    m.eval() #set the net in evaluation mode \n",
    "    # set a few words to query \n",
    "    queries = ['insurance', 'weather', 'particles', 'religion', 'man', 'love', \n",
    "                'intelligence', 'money', 'politics', 'health', 'people', 'family']\n",
    "\n",
    "    ## visualize topics using monte carlo (sampling from the posterior I guess)\n",
    "    with torch.no_grad(): # no gradients computation - makes forward pass lighter \n",
    "        print('-'*50)\n",
    "        print('Visualize topics...')\n",
    "        topics_words = []\n",
    "        gammas = m.get_beta() # topics distributions \n",
    "        for k in range(num_topics):\n",
    "            gamma = gammas[k]\n",
    "            top_words = list(gamma.cpu().numpy().argsort()[-num_words+1:][::-1])\n",
    "            topic_words = [vocab[a] for a in top_words] \n",
    "            topics_words.append(' '.join(topic_words))\n",
    "            print('Topic {}: {}'.format(k, topic_words))\n",
    "\n",
    "        if show_emb:\n",
    "            ## visualize word embeddings by using V to get nearest neighbors\n",
    "            print('-'*50)\n",
    "            print('Visualize word embeddings by using output embedding matrix')\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            for word in queries:\n",
    "                word_id = list(vocab.values()).index(word)\n",
    "                query = embeddings[word_id]\n",
    "                nns = utils.nearest_neighbors(q=query, \n",
    "                                        embeddings=embeddings, vocab=list(vocab.values()))\n",
    "                print('word: {} .. neighbors: {}'.format(word, nns)) # utility function \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(m, corpus, num_docs_test, tc=tc, td=td, \n",
    "             eval_batch_size=eval_batch_size, vocab_size=vocab_size, \n",
    "             bow_norm=bow_norm):\n",
    "    \"\"\"\n",
    "    Evaluating the trained model on the test set using either perplexity, or coherence and diversity. \n",
    "    Compute perplexity on document completion.\n",
    "    \"\"\"\n",
    "    \n",
    "    m.eval() # set model in evaluation mode \n",
    "    with torch.no_grad():\n",
    "        indices = torch.split(torch.tensor(range(num_docs_test)), eval_batch_size)\n",
    "        \n",
    "        ## get \\beta here\n",
    "        beta = m.get_beta()\n",
    "\n",
    "        ### do dc and tc here\n",
    "        acc_loss = 0\n",
    "        cnt = 0\n",
    "        \n",
    "        for idx, ind in enumerate(indices):\n",
    "            data_batch = get_batch(corpus, ind, vocab_size, device)\n",
    "            sums = data_batch.sum(1).unsqueeze(1)\n",
    "            if bow_norm: normalized_data_batch = data_batch / sums\n",
    "            else: normalized_data_batch = data_batch\n",
    "                \n",
    "            ## get theta\n",
    "            theta, _ = m.get_theta(normalized_data_batch)\n",
    "            ## get prediction loss\n",
    "            res = torch.mm(theta, beta)\n",
    "            preds = torch.log(res)\n",
    "            recon_loss = -(preds * data_batch).sum(1)\n",
    "            loss = recon_loss / sums.squeeze()\n",
    "            loss = loss.mean().item()\n",
    "            acc_loss += loss\n",
    "            cnt += 1\n",
    "        \n",
    "        # Calculate final loss \n",
    "        cur_loss = acc_loss / cnt\n",
    "        ppl_dc = round(math.exp(cur_loss), 1)\n",
    "        print('Eval Doc Completion PPL: {}'.format(ppl_dc))\n",
    "        \n",
    "        \n",
    "        if tc or td: # calculate topic coherence or topic diversity \n",
    "            beta = beta.data.cpu().numpy()\n",
    "            if tc:\n",
    "                print('Computing topic coherence...')\n",
    "                get_topic_coherence(beta, train_tokens, vocab)\n",
    "            if td:\n",
    "                print('Computing topic diversity...')\n",
    "                get_topic_diversity(beta, 25)\n",
    "        return ppl_dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code that actually launches the training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Visualizing model quality before training...\n",
      "--------------------------------------------------\n",
      "Visualize topics...\n",
      "Topic 0: ['west', 'held', 'system', 'killed', 'women', 'bush', 'john', 'fashion', 'street']\n",
      "Topic 1: ['member', 'province', 'global', 'home', 'considered', 'married', '&amp;', 'accused', 'soldiers']\n",
      "Topic 2: ['times', 'case', 'thursday', 'soccer', 'hamas', 'headed', 'area', 'policies', 'prices']\n",
      "Topic 3: ['warning', 'marriage', 'exports', 'scott', 'mary', 'bowl', 'heat', 'understand', 'terror']\n",
      "Topic 4: ['married', 'considered', 'accused', 'things', 'soldiers', 'professional', 'determine', 'giving', 'global']\n",
      "Topic 5: ['times', 'case', 'hamas', 'building', 'thursday', 'area', 'prices', 'german', 'general']\n",
      "Topic 6: ['friday', 'remain', 'protection', 'foundation', 'brought', 'book', 'center', 'married', \"'\"]\n",
      "Topic 7: ['held', '%', 'west', 'killed', 'meeting', 'past', 'rose', 'women', 'countries']\n",
      "Topic 8: ['times', 'headed', 'area', 'planning', 'hamas', 'soldiers', 'role', 'leaving', 'book']\n",
      "Topic 9: ['times', 'planning', 'leaving', 'role', 'giving', 'headed', 'area', 'sports', 'fame']\n",
      "--------------------------------------------------\n",
      "Visualize word embeddings by using output embedding matrix\n",
      "word: insurance .. neighbors: ['insurance', 'insurers', 'premiums', 'insurer', 'pension', 'insured', 'care', 'savings', 'benefits', 'liability']\n",
      "word: weather .. neighbors: ['weather', 'inclement', 'rain', 'temperatures', 'rainy', 'conditions', 'storms', 'winter', 'winds', 'rains']\n",
      "word: particles .. neighbors: ['particles', 'particle', 'molecules', 'electrons', 'photons', 'subatomic', 'atoms', 'protons', 'droplets', 'microscopic']\n",
      "word: religion .. neighbors: ['religion', 'religions', 'religious', 'christianity', 'beliefs', 'faith', 'belief', 'spirituality', 'catholicism', 'islam']\n",
      "word: man .. neighbors: ['man', 'woman', 'person', 'boy', 'men', 'girl', 'guy', 'father', 'young', 'life']\n",
      "word: love .. neighbors: ['love', 'loves', 'passion', 'loved', 'romantic', 'lovers', 'lover', 'affection', 'romance', 'loving']\n",
      "word: intelligence .. neighbors: ['intelligence', 'cia', 'information', 'security', 'counterterrorism', 'operatives', 'fbi', 'military', 'secret', 'spy']\n",
      "word: money .. neighbors: ['money', 'funds', 'cash', 'fund', 'donations', 'pay', 'amount', 'paying', 'paid', 'millions']\n",
      "word: politics .. neighbors: ['politics', 'political', 'politicians', 'religion', 'culture', 'ideology', 'partisan', 'liberal', 'debate', 'social']\n",
      "word: health .. neighbors: ['health', 'care', 'healthcare', 'education', 'medical', 'hospitals', 'welfare', 'nutrition', 'benefits', 'social']\n",
      "word: people .. neighbors: ['people', 'thousands', 'hundreds', 'citizens', 'residents', 'families', 'americans', 'persons', 'children', 'person']\n",
      "word: family .. neighbors: ['family', 'families', 'relatives', 'father', 'parents', 'mother', 'friends', 'daughter', 'son', 'wife']\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch----->1 .. LR: 0.05 .. KL_theta: 4.45 .. Rec_loss: 711.23 .. NELBO: 715.68\n",
      "Eval Doc Completion PPL: 6735.4\n",
      "--------------------------------------------------\n",
      "Epoch----->2 .. LR: 0.05 .. KL_theta: 4.55 .. Rec_loss: 709.92 .. NELBO: 714.47\n",
      "Eval Doc Completion PPL: 6736.8\n",
      "--------------------------------------------------\n",
      "Epoch----->3 .. LR: 0.05 .. KL_theta: 4.69 .. Rec_loss: 711.6 .. NELBO: 716.29\n",
      "Eval Doc Completion PPL: 6741.7\n",
      "--------------------------------------------------\n",
      "Epoch----->4 .. LR: 0.05 .. KL_theta: 4.77 .. Rec_loss: 711.09 .. NELBO: 715.86\n",
      "Eval Doc Completion PPL: 6727.2\n",
      "--------------------------------------------------\n",
      "Epoch----->5 .. LR: 0.05 .. KL_theta: 4.92 .. Rec_loss: 710.99 .. NELBO: 715.91\n",
      "Eval Doc Completion PPL: 6823.1\n",
      "--------------------------------------------------\n",
      "Epoch----->6 .. LR: 0.05 .. KL_theta: 5.13 .. Rec_loss: 711.32 .. NELBO: 716.45\n",
      "Eval Doc Completion PPL: 6740.2\n",
      "--------------------------------------------------\n",
      "Epoch----->7 .. LR: 0.05 .. KL_theta: 5.26 .. Rec_loss: 708.82 .. NELBO: 714.08\n",
      "Eval Doc Completion PPL: 6690.4\n",
      "--------------------------------------------------\n",
      "Epoch----->8 .. LR: 0.05 .. KL_theta: 5.22 .. Rec_loss: 710.68 .. NELBO: 715.9\n",
      "Eval Doc Completion PPL: 6703.0\n",
      "--------------------------------------------------\n",
      "Epoch----->9 .. LR: 0.05 .. KL_theta: 5.35 .. Rec_loss: 708.07 .. NELBO: 713.42\n",
      "Eval Doc Completion PPL: 6676.3\n",
      "--------------------------------------------------\n",
      "Epoch----->10 .. LR: 0.05 .. KL_theta: 5.43 .. Rec_loss: 711.48 .. NELBO: 716.91\n",
      "Eval Doc Completion PPL: 6750.4\n",
      "--------------------------------------------------\n",
      "Visualize topics...\n",
      "Topic 0: ['west', 'held', 'system', 'women', 'bush', 'fashion', 'killed', 'john', '90']\n",
      "Topic 1: ['member', 'province', 'global', 'home', 'married', 'considered', 'accused', '&amp;', '19']\n",
      "Topic 2: ['times', 'case', 'soccer', 'thursday', 'headed', 'hamas', 'area', 'review', 'prices']\n",
      "Topic 3: ['warning', 'marriage', 'exports', 'mary', 'scott', 'bowl', 'heat', 'understand', 'terror']\n",
      "Topic 4: ['married', 'considered', 'accused', 'things', 'solid', 'soldiers', 'professional', '2013', 'global']\n",
      "Topic 5: ['case', 'thursday', 'times', 'hamas', 'soccer', 'building', 'area', '?', 'prices']\n",
      "Topic 6: ['friday', 'remain', 'protection', 'foundation', 'brought', 'book', 'center', 'married', 'leaving']\n",
      "Topic 7: ['held', '%', 'west', 'killed', 'meeting', 'past', 'rose', 'women', 'countries']\n",
      "Topic 8: ['times', 'planning', 'area', 'soldiers', 'headed', 'leaving', 'role', 'issued', 'book']\n",
      "Topic 9: ['times', 'planning', 'leaving', 'headed', 'area', 'sports', 'culture', 'issued', 'role']\n",
      "--------------------------------------------------\n",
      "Epoch----->11 .. LR: 0.05 .. KL_theta: 5.52 .. Rec_loss: 710.62 .. NELBO: 716.14\n",
      "Eval Doc Completion PPL: 6614.7\n",
      "--------------------------------------------------\n",
      "Epoch----->12 .. LR: 0.05 .. KL_theta: 5.93 .. Rec_loss: 709.61 .. NELBO: 715.54\n",
      "Eval Doc Completion PPL: 6594.7\n",
      "--------------------------------------------------\n",
      "Epoch----->13 .. LR: 0.05 .. KL_theta: 5.84 .. Rec_loss: 711.04 .. NELBO: 716.88\n",
      "Eval Doc Completion PPL: 6665.0\n",
      "--------------------------------------------------\n",
      "Epoch----->14 .. LR: 0.05 .. KL_theta: 6.31 .. Rec_loss: 711.22 .. NELBO: 717.53\n",
      "Eval Doc Completion PPL: 6742.9\n",
      "--------------------------------------------------\n",
      "Epoch----->15 .. LR: 0.05 .. KL_theta: 6.31 .. Rec_loss: 707.81 .. NELBO: 714.12\n",
      "Eval Doc Completion PPL: 6742.3\n",
      "--------------------------------------------------\n",
      "Epoch----->16 .. LR: 0.05 .. KL_theta: 6.33 .. Rec_loss: 711.25 .. NELBO: 717.58\n",
      "Eval Doc Completion PPL: 6704.5\n",
      "--------------------------------------------------\n",
      "Epoch----->17 .. LR: 0.05 .. KL_theta: 6.82 .. Rec_loss: 710.85 .. NELBO: 717.67\n",
      "Eval Doc Completion PPL: 6758.5\n",
      "--------------------------------------------------\n",
      "Epoch----->18 .. LR: 0.05 .. KL_theta: 7.0 .. Rec_loss: 708.53 .. NELBO: 715.53\n",
      "Eval Doc Completion PPL: 6712.1\n",
      "--------------------------------------------------\n",
      "Epoch----->19 .. LR: 0.05 .. KL_theta: 7.03 .. Rec_loss: 709.64 .. NELBO: 716.67\n",
      "Eval Doc Completion PPL: 6657.0\n",
      "--------------------------------------------------\n",
      "Epoch----->20 .. LR: 0.05 .. KL_theta: 7.09 .. Rec_loss: 707.17 .. NELBO: 714.26\n",
      "Eval Doc Completion PPL: 6749.4\n",
      "--------------------------------------------------\n",
      "Visualize topics...\n",
      "Topic 0: ['west', 'fashion', 'system', 'bush', '90', 'women', 'return', 'john', 'date']\n",
      "Topic 1: ['member', 'global', 'province', 'married', 'home', 'considered', 'accused', '&amp;', 'soldiers']\n",
      "Topic 2: ['times', 'case', 'area', 'headed', 'hamas', 'soccer', 'thursday', 'review', 'german']\n",
      "Topic 3: ['warning', 'marriage', 'mary', 'bowl', 'exports', 'scott', 'heat', 'terror', 'understand']\n",
      "Topic 4: ['married', 'considered', '2013', 'things', 'solid', 'soldiers', 'accused', 'professional', 'giving']\n",
      "Topic 5: ['thursday', 'case', 'soccer', 'times', 'hamas', 'area', 'building', '?', 'prices']\n",
      "Topic 6: ['friday', 'remain', 'protection', 'foundation', 'brought', 'book', 'leaving', 'center', 'married']\n",
      "Topic 7: ['held', 'west', '%', 'killed', 'meeting', 'past', 'women', 'rose', 'countries']\n",
      "Topic 8: ['times', 'area', 'planning', 'leaving', 'headed', 'role', 'soldiers', 'book', 'experience']\n",
      "Topic 9: ['times', 'leaving', 'culture', 'planning', 'headed', 'solid', 'sports', 'fame', 'area']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Epoch----->21 .. LR: 0.05 .. KL_theta: 7.07 .. Rec_loss: 710.81 .. NELBO: 717.88\n",
      "Eval Doc Completion PPL: 6680.5\n",
      "--------------------------------------------------\n",
      "Epoch----->22 .. LR: 0.05 .. KL_theta: 7.7 .. Rec_loss: 707.32 .. NELBO: 715.02\n",
      "Eval Doc Completion PPL: 6617.6\n",
      "--------------------------------------------------\n",
      "Epoch----->23 .. LR: 0.05 .. KL_theta: 7.46 .. Rec_loss: 708.26 .. NELBO: 715.72\n",
      "Eval Doc Completion PPL: 6594.1\n",
      "--------------------------------------------------\n",
      "Epoch----->24 .. LR: 0.05 .. KL_theta: 7.51 .. Rec_loss: 709.52 .. NELBO: 717.03\n",
      "Eval Doc Completion PPL: 6774.8\n",
      "--------------------------------------------------\n",
      "Epoch----->25 .. LR: 0.05 .. KL_theta: 7.71 .. Rec_loss: 708.5 .. NELBO: 716.21\n",
      "Eval Doc Completion PPL: 6709.0\n",
      "--------------------------------------------------\n",
      "Epoch----->26 .. LR: 0.05 .. KL_theta: 7.91 .. Rec_loss: 710.31 .. NELBO: 718.22\n",
      "Eval Doc Completion PPL: 6881.8\n",
      "--------------------------------------------------\n",
      "Epoch----->27 .. LR: 0.05 .. KL_theta: 8.22 .. Rec_loss: 709.57 .. NELBO: 717.79\n",
      "Eval Doc Completion PPL: 6677.7\n",
      "--------------------------------------------------\n",
      "Epoch----->28 .. LR: 0.05 .. KL_theta: 8.62 .. Rec_loss: 708.42 .. NELBO: 717.04\n",
      "Eval Doc Completion PPL: 6698.0\n",
      "--------------------------------------------------\n",
      "Epoch----->29 .. LR: 0.05 .. KL_theta: 8.9 .. Rec_loss: 708.19 .. NELBO: 717.09\n",
      "Eval Doc Completion PPL: 6530.0\n",
      "--------------------------------------------------\n",
      "Epoch----->30 .. LR: 0.05 .. KL_theta: 9.09 .. Rec_loss: 708.02 .. NELBO: 717.11\n",
      "Eval Doc Completion PPL: 6682.0\n",
      "--------------------------------------------------\n",
      "Visualize topics...\n",
      "Topic 0: ['fashion', 'west', '90', 'bush', 'return', 'system', 'date', 'women', 'prosecutor']\n",
      "Topic 1: ['member', 'global', 'province', 'married', 'home', 'considered', 'accused', '&amp;', 'soldiers']\n",
      "Topic 2: ['times', 'case', 'area', 'headed', 'hamas', 'leaving', 'building', 'review', 'soccer']\n",
      "Topic 3: ['warning', 'marriage', 'exports', 'mary', 'bowl', 'scott', 'heat', 'terror', 'understand']\n",
      "Topic 4: ['married', '2013', 'soldiers', 'considered', 'accused', 'things', 'solid', 'planning', 'giving']\n",
      "Topic 5: ['thursday', 'case', 'soccer', 'times', 'hamas', 'area', 'noted', 'prices', '?']\n",
      "Topic 6: ['friday', 'remain', 'protection', 'foundation', 'leaving', 'book', 'brought', 'center', 'central']\n",
      "Topic 7: ['held', 'west', '%', 'killed', 'meeting', 'women', 'rose', 'past', 'countries']\n",
      "Topic 8: ['times', 'area', 'leaving', 'headed', 'planning', 'case', 'culture', 'visit', 'role']\n",
      "Topic 9: ['times', 'culture', 'solid', 'leaving', 'producers', 'planning', 'allegedly', 'ministers', 'run']\n",
      "--------------------------------------------------\n",
      "Epoch----->31 .. LR: 0.05 .. KL_theta: 8.98 .. Rec_loss: 707.19 .. NELBO: 716.17\n",
      "Eval Doc Completion PPL: 6641.0\n",
      "--------------------------------------------------\n",
      "Epoch----->32 .. LR: 0.05 .. KL_theta: 9.17 .. Rec_loss: 709.09 .. NELBO: 718.26\n",
      "Eval Doc Completion PPL: 6805.8\n",
      "--------------------------------------------------\n",
      "Epoch----->33 .. LR: 0.05 .. KL_theta: 8.75 .. Rec_loss: 709.39 .. NELBO: 718.14\n",
      "Eval Doc Completion PPL: 6589.1\n",
      "--------------------------------------------------\n",
      "Epoch----->34 .. LR: 0.05 .. KL_theta: 9.24 .. Rec_loss: 707.97 .. NELBO: 717.21\n",
      "Eval Doc Completion PPL: 6673.0\n",
      "--------------------------------------------------\n",
      "Epoch----->35 .. LR: 0.05 .. KL_theta: 9.47 .. Rec_loss: 707.55 .. NELBO: 717.02\n",
      "Eval Doc Completion PPL: 6623.8\n",
      "--------------------------------------------------\n",
      "Epoch----->36 .. LR: 0.05 .. KL_theta: 9.66 .. Rec_loss: 709.07 .. NELBO: 718.73\n",
      "Eval Doc Completion PPL: 6763.3\n",
      "--------------------------------------------------\n",
      "Epoch----->37 .. LR: 0.05 .. KL_theta: 9.69 .. Rec_loss: 709.24 .. NELBO: 718.93\n",
      "Eval Doc Completion PPL: 6657.0\n",
      "--------------------------------------------------\n",
      "Epoch----->38 .. LR: 0.05 .. KL_theta: 10.73 .. Rec_loss: 707.71 .. NELBO: 718.44\n",
      "Eval Doc Completion PPL: 6706.9\n",
      "--------------------------------------------------\n",
      "Epoch----->39 .. LR: 0.05 .. KL_theta: 10.79 .. Rec_loss: 707.67 .. NELBO: 718.46\n",
      "Eval Doc Completion PPL: 6675.4\n",
      "--------------------------------------------------\n",
      "Epoch----->40 .. LR: 0.05 .. KL_theta: 10.31 .. Rec_loss: 707.82 .. NELBO: 718.13\n",
      "Eval Doc Completion PPL: 6644.3\n",
      "--------------------------------------------------\n",
      "Visualize topics...\n",
      "Topic 0: ['fashion', '90', 'west', 'return', 'bush', 'date', 'women', 'prosecutor', 'period']\n",
      "Topic 1: ['member', 'global', 'province', 'married', 'considered', 'home', 'accused', '&amp;', 'soldiers']\n",
      "Topic 2: ['times', 'headed', 'case', 'area', 'leaving', 'hamas', 'german', 'review', 'bosnia']\n",
      "Topic 3: ['warning', 'marriage', 'mary', 'exports', 'scott', 'bowl', 'terror', 'heat', 'understand']\n",
      "Topic 4: ['married', '2013', 'soldiers', 'things', 'times', 'considered', 'planning', 'solid', 'issued']\n",
      "Topic 5: ['case', 'thursday', 'soccer', 'noted', 'times', 'area', 'hamas', '?', 'policies']\n",
      "Topic 6: ['friday', 'remain', 'protection', 'foundation', 'brought', 'book', 'leaving', 'center', 'central']\n",
      "Topic 7: ['held', 'west', '%', 'killed', 'meeting', 'women', 'rose', 'past', 'countries']\n",
      "Topic 8: ['times', 'area', 'leaving', 'headed', 'planning', 'culture', 'visit', 'case', 'soldiers']\n",
      "Topic 9: ['times', 'culture', 'planning', 'solid', 'producers', 'sports', 'leaving', 'ministers', 'allegedly']\n",
      "--------------------------------------------------\n",
      "Epoch----->41 .. LR: 0.05 .. KL_theta: 11.01 .. Rec_loss: 706.79 .. NELBO: 717.8\n",
      "Eval Doc Completion PPL: 6751.0\n",
      "--------------------------------------------------\n",
      "Epoch----->42 .. LR: 0.05 .. KL_theta: 10.31 .. Rec_loss: 707.08 .. NELBO: 717.39\n",
      "Eval Doc Completion PPL: 6663.5\n",
      "--------------------------------------------------\n",
      "Epoch----->43 .. LR: 0.05 .. KL_theta: 10.55 .. Rec_loss: 707.83 .. NELBO: 718.38\n",
      "Eval Doc Completion PPL: 6740.6\n",
      "--------------------------------------------------\n",
      "Epoch----->44 .. LR: 0.05 .. KL_theta: 10.86 .. Rec_loss: 708.07 .. NELBO: 718.93\n",
      "Eval Doc Completion PPL: 6605.7\n",
      "--------------------------------------------------\n",
      "Epoch----->45 .. LR: 0.05 .. KL_theta: 11.78 .. Rec_loss: 708.19 .. NELBO: 719.97\n",
      "Eval Doc Completion PPL: 6905.9\n",
      "--------------------------------------------------\n",
      "Epoch----->46 .. LR: 0.05 .. KL_theta: 10.79 .. Rec_loss: 708.47 .. NELBO: 719.26\n",
      "Eval Doc Completion PPL: 6545.7\n",
      "--------------------------------------------------\n",
      "Epoch----->47 .. LR: 0.05 .. KL_theta: 12.09 .. Rec_loss: 706.83 .. NELBO: 718.92\n",
      "Eval Doc Completion PPL: 6794.3\n",
      "--------------------------------------------------\n",
      "Epoch----->48 .. LR: 0.05 .. KL_theta: 11.53 .. Rec_loss: 706.75 .. NELBO: 718.28\n",
      "Eval Doc Completion PPL: 6643.0\n",
      "--------------------------------------------------\n",
      "Epoch----->49 .. LR: 0.05 .. KL_theta: 11.35 .. Rec_loss: 706.08 .. NELBO: 717.43\n",
      "Eval Doc Completion PPL: 6706.5\n"
     ]
    }
   ],
   "source": [
    "# Initialising the data structures \n",
    "best_epoch = 0\n",
    "best_val_ppl = 1e9\n",
    "all_val_ppls = []\n",
    "\n",
    "# Let's get a sense of how bad the model is before training \n",
    "print('\\n')\n",
    "print('Visualizing model quality before training...')\n",
    "visualize(etm_model)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs):\n",
    "    \n",
    "    train(etm_model, epoch, train_corpus) # train \n",
    "    num_docs_test = len(new_token_ids) - num_docs_train\n",
    "    val_ppl = evaluate(etm_model, test_corpus, num_docs_test) # evaluate \n",
    "    \n",
    "    # only saving the model if it's the best so far \n",
    "    if val_ppl < best_val_ppl: \n",
    "        with open(ckpt, 'wb') as f:\n",
    "            torch.save(etm_model, f)\n",
    "        best_epoch = epoch \n",
    "        best_val_ppl = val_ppl\n",
    "        \n",
    "    else:\n",
    "        ## check whether to anneal lr (aka decreasing it by a constant factor )\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        if anneal_lr and (len(all_val_ppls) > nonmono and val_ppl > min(all_val_ppls[:-nonmono]) and lr > 1e-5):\n",
    "            optimizer.param_groups[0]['lr'] /= lr_factor\n",
    "            \n",
    "    #maybe visualise \n",
    "    if epoch % visualize_every == 0:\n",
    "        visualize(etm_model, show_emb=False)\n",
    "        \n",
    "    #save perplexities \n",
    "    all_val_ppls.append(val_ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the code that launches the final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Doc Completion PPL: 6761.7\n",
      "\n",
      "The 10 most used topics are [6 1 7 0 5 3 4 2 8 9]\n",
      "\n",
      "\n",
      "Topic 0: ['und', 'der', 'wird', 'zu', 'auf', 'fÃ¼r', 'eine', 'politik', 'nicht']\n",
      "Topic 1: ['models', 'model', 'data', 'results', 'analysis', 'methods', 'based', 'techniques', 'research']\n",
      "Topic 2: ['economic', 'political', 'china', 'russia', 'nuclear', 'cooperation', 'policy', 'relations', 'states']\n",
      "Topic 3: ['plateau', 'glacier', 'glaciers', 'glacial', 'volcanic', 'cretaceous', 'cordillera', 'moraines', 'oceanic']\n",
      "Topic 4: ['methods', 'analysis', 'techniques', 'processes', 'research', 'systems', 'measurement', 'strategies', 'data']\n",
      "Topic 5: ['economic', 'political', 'nuclear', 'strategic', 'china', 'policy', 'states', 'global', 'security']\n",
      "Topic 6: ['â€™', 'chapter', 'â€œ', 'â€', 'â€“', 'development', 'â€˜', 'methods', 'structure']\n",
      "Topic 7: ['der', 'die', 'und', 'zu', 'von', 'sich', 'den', 'auf', 'das']\n",
      "Topic 8: ['economic', 'cooperation', 'policy', 'scientific', 'nuclear', 'research', 'strategy', 'social', 'development']\n",
      "Topic 9: ['economic', 'scientific', 'social', 'strategy', 'strategies', 'cooperation', 'policy', 'framework', 'principles']\n"
     ]
    }
   ],
   "source": [
    "# load trained model and evaluate it  \n",
    "#with open(ckpt, 'rb') as f:\n",
    "#    etm_model = torch.load(f)\n",
    "etm_model = etm_model.to(device)\n",
    "etm_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    ## ---------------\n",
    "    ## Idea : get document completion perplexities\n",
    "    test_ppl = evaluate(etm_model, test_corpus, num_docs_test)\n",
    "\n",
    "    ## ----------------\n",
    "    ## Idea : get most used topics\n",
    "    indices = torch.tensor(range(num_docs_test)) # training documents indices \n",
    "    indices = torch.split(indices, batch_size)\n",
    "    #just initialising data structures \n",
    "    thetaAvg = torch.zeros(1, num_topics).to(device)\n",
    "    thetaWeightedAvg = torch.zeros(1, num_topics).to(device)\n",
    "    cnt = 0\n",
    "    for idx, ind in enumerate(indices):\n",
    "        data_batch = get_batch(test_corpus,ind, vocab_size, device) # TODO: fix here \n",
    "        sums = data_batch.sum(1).unsqueeze(1) \n",
    "        cnt += sums.sum(0).squeeze().cpu().numpy()\n",
    "        # maybe normalise \n",
    "        if bow_norm:normalized_data_batch = data_batch / sums\n",
    "        else: normalized_data_batch = data_batch\n",
    "        # get the theta \n",
    "        theta, _ = etm_model.get_theta(normalized_data_batch)\n",
    "        thetaAvg += theta.sum(0).unsqueeze(0) /num_docs_train\n",
    "        weighed_theta = sums * theta\n",
    "        thetaWeightedAvg += weighed_theta.sum(0).unsqueeze(0)\n",
    "        # let's print the progress as we go \n",
    "        if idx % 100 == 0 and idx > 0:\n",
    "            print('batch: {}/{}'.format(idx, len(indices)))\n",
    "    # finally the results are in \n",
    "    thetaWeightedAvg = thetaWeightedAvg.squeeze().cpu().numpy() / cnt\n",
    "    print('\\nThe 10 most used topics are {}'.format(thetaWeightedAvg.argsort()[::-1][:10]))\n",
    "\n",
    "    # Now we show the topics\n",
    "    # A nice visualisation is always welcome \n",
    "    beta = etm_model.get_beta()\n",
    "    topic_indices = list(np.random.choice(num_topics, 10)) # 10 random topics\n",
    "    print('\\n')\n",
    "    for k in range(num_topics):#topic_indices:\n",
    "        gamma = beta[k]\n",
    "        top_words = list(gamma.cpu().numpy().argsort()[-num_words+1:][::-1])\n",
    "        topic_words = [idx2word[a] for a in top_words]\n",
    "        print('Topic {}: {}'.format(k, topic_words))\n",
    "\n",
    "    # Why not, also showing a few embeddings \n",
    "    if train_embeddings:\n",
    "        # get embeddings from the model \n",
    "        try:rho_etm = etm_model.rho.weight.cpu()\n",
    "        except:rho_etm = etm_model.rho.cpu()\n",
    "        queries = ['andrew', 'woman', 'computer', 'sports', 'religion', 'man', 'love', \n",
    "                        'intelligence', 'money', 'politics', 'health', 'people', 'family']\n",
    "        print('\\n')\n",
    "        print('ETM embeddings...')\n",
    "        for word in queries:\n",
    "            print('word: {} .. etm neighbors: {}'.format(word, nearest_neighbors(word, rho_etm, vocab)))\n",
    "        print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative approach: Static BERT embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: Lower hidden layers of BERT are less contextualized. We attempt to build a static embedding where we project the vectors from a hidden layer to the first principal component of PCA for each word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "#TODO: remove punctuation\n",
    "stop_words = stopwords.words('english') # note: the words in this list are only lower case but distilbert tokenizer incorporates lower casing so we should be fine! read more here: https://huggingface.co/transformers/_modules/transformers/tokenization_distilbert_fast.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subset_static(doc_subset, tokenizer, model, \n",
    "                          idx2word, new_token_ids, word2uniquevec, word2manyvec, selected_layer = 1):\n",
    "    \"\"\" \n",
    "    Processing of a subset of the batch, returning a dictionary of the unseen words with all its embedded vectors. \n",
    "    \n",
    "    Parameters \n",
    "    ----- \n",
    "    doc_subset: list of documents (aka list of strings)\n",
    "    tokenizer: instance of Bert tokenizer \n",
    "    model: instance of Bert model \n",
    "    idx2word: vocabulary mapping our token ids to the corresponding word. \n",
    "            Notice that each word can be mapped to multiple token ids.\n",
    "    new_token_ids: representation of the collection with our token ids.\n",
    "    word2uniquevec: global dictionary mapping words to one vector\n",
    "    word2manyvec: batch local dictionary mapping (the words not in word2uniquevec) to all vectors found \n",
    "        in the embedding space for that word\n",
    "    selected_layer: hidden layer to be selected for near-static embeddings (should be small) \n",
    "            \n",
    "    Returns \n",
    "    -----\n",
    "    Updated versions of idx2word, new_token_ids, word2manyvec\n",
    "    \n",
    "    \"\"\"\n",
    "    tokenised_collection = tokenizer(doc_subset, return_tensors=\"pt\", padding=True)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(\"tokenisation done\")\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            embedded_collection = model(**tokenised_collection)\n",
    "    except Exception as e:\n",
    "        print(\"There was a problem with this subset, we'll skip it!\")\n",
    "        print(e)\n",
    "        return idx2word, new_token_ids, word2uniquevec\n",
    "    embedded_collection.requires_grad = False\n",
    "    # extract lower layers hidden states\n",
    "    #lower_hiddens = torch.sum(torch.stack(embedded_collection[1][3:6], dim=0), dim=0)\n",
    "    lower_hiddens = embedded_collection[1][selected_layer].cpu()\n",
    "\n",
    "    print(\"embeddings done\")\n",
    "    \n",
    "    \n",
    "    ##  preparing the variables we need ---------\n",
    "    if len(idx2word) == 0:\n",
    "        idx = 0\n",
    "    else:\n",
    "        idx = len(idx2word)\n",
    "    subset_size = len(doc_subset)\n",
    "    start = time.time()\n",
    "    \n",
    "    ## processing the collection document by document ----------\n",
    "    for i in range(subset_size):\n",
    "        t1 = time.time()\n",
    "        embedded_doc = lower_hiddens[i][tokenised_collection[\"attention_mask\"][i].bool()] # removing padding using the attention mask \n",
    "        tokens_ids = tokenised_collection[\"input_ids\"][i]\n",
    "        new_token_ids_doc = []\n",
    "        \n",
    "        \n",
    "        for j,emb_vector in enumerate(embedded_doc):\n",
    "            \n",
    "            token_id = tokens_ids[j].cpu().numpy() # bert current token \n",
    "            word = tokenizer.convert_ids_to_tokens([token_id])[0] # corresponding word\n",
    "    \n",
    "            # jump to the next token if the word is a stopword \n",
    "            if word in stop_words or word.startswith(\"##\") or word in string.punctuation: continue   \n",
    "            \n",
    "            if word not in word2uniquevec.keys(): # only consider words without unique vector representation (i.e. new words)\n",
    "                if word not in word2manyvec.keys(): # create new entry if we encounter word for the first time\n",
    "                    word2manyvec[word] = np.array([emb_vector.detach().numpy()])\n",
    "                    idx2word[idx] = word # save it in our vocabulary\n",
    "                    new_token_ids_doc += [idx]\n",
    "                    idx += 1\n",
    "                else: # append to list if already encountered the word in batch\n",
    "                    word2manyvec[word] = np.append(word2manyvec[word], [emb_vector.detach().numpy()], axis = 0) # note: using numpy array as lists manipulate inplace\n",
    "                    word_id = list(idx2word.values()).index(word)\n",
    "                    new_token_ids_doc += [word_id]\n",
    "            else:\n",
    "                word_id = list(idx2word.values()).index(word)\n",
    "                new_token_ids_doc += [word_id]\n",
    "                \n",
    "        new_token_ids += [new_token_ids_doc]\n",
    "        t2 = time.time()\n",
    "        try:\n",
    "            if i % (subset_size // 3) == 0:\n",
    "                print(\"Document \" + str(i) + \" done. Time: \" + str(round(t2 - t1, 2)) + \" s.\")\n",
    "        except Exception as _:\n",
    "            pass  # case subset_size//3 = 0 we get a division by 0 (subset_size must be < 3)\n",
    "       \n",
    "            \n",
    "    end = time.time()\n",
    "    print(\"Total time for the subset: \"+str(round(end-start,2))+\" s.\")\n",
    "    \n",
    "    return idx2word, new_token_ids, word2manyvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch_static(batch, subset_size, tokeniser, model, idx2word, word2uniquevec):\n",
    "    \"\"\" Processing of a batch of documents in the collection. \"\"\"\n",
    "    from sklearn.decomposition import PCA #, FastICA\n",
    "    \n",
    "    ## initialisation \n",
    "    new_token_ids = [] \n",
    "    word2manyvec = {}\n",
    "    streaming_batch_size = len(batch)\n",
    "\n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    ## processing the batch one subset at a time\n",
    "    for s in range(0,streaming_batch_size,subset_size):\n",
    "        print(\"Processing subset \"+str(s//subset_size + 1))\n",
    "        if s+subset_size < len(batch):\n",
    "            batch_subset = batch[s:s+subset_size]\n",
    "        else: \n",
    "            batch_subset = batch[s:]\n",
    "        # iteratively update idx2word, new_token_ids, word2manyvec throughout the batch\n",
    "        idx2word, new_token_ids, word2manyvec = process_subset_static(batch_subset, tokenizer, model, \n",
    "                                                                        idx2word, new_token_ids, \n",
    "                                                                        word2uniquevec, word2manyvec, selected_layer = 1)\n",
    "        print(\"Number of word vectors so far: \"+str(len(idx2word)))    \n",
    "    \n",
    "    # for every new word discover, find unique vector representation by taking first PCA prinicipal component\n",
    "    for word, veclist in word2manyvec.items():\n",
    "        if len(veclist) == 1: # otherwise we would just get the first standard unit vector from PCA\n",
    "            word2uniquevec[word] = torch.Tensor(veclist[0])\n",
    "        else: \n",
    "            pca = PCA(n_components = 1)\n",
    "            pca.fit(veclist)\n",
    "            word2uniquevec[word] = torch.Tensor(pca.components_[0])\n",
    "            print(type(pca.components_[0]))\n",
    "            \n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"Total time: \"+str(round(end-start,2))+\" s.\")\n",
    "    \n",
    "    return word2uniquevec, idx2word, new_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TESTING on a few example sentences \n",
    "\n",
    "word2uniquevec = {}\n",
    "idx2word = {}\n",
    "subset_size = 1\n",
    "sen1 = \"This mouse loves cheese\"\n",
    "sen2 = \"I am using this mouse as a pointer\"\n",
    "sen3 = \"This mouse is hungry\"\n",
    "batch1 = [sen1, sen2, sen3]\n",
    "sen4 = \"The mouse moved out of the house\"\n",
    "sen5 = \"I grew up in Italy\"\n",
    "sen6 = \"I am giving up on this shit\"\n",
    "batch2 = [sen4, sen5,sen6]\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased', return_dict=True, output_hidden_states=True)\n",
    "model.eval()\n",
    "#model.to(device)\n",
    "\n",
    "word2uniquevec, idx2word, new_token_ids1 = process_batch_static(batch1, subset_size, tokenizer, model, idx2word, \n",
    "                                                                          word2uniquevec)\n",
    "\n",
    "\n",
    "word2uniquevec, idx2word, new_token_ids2 = process_batch_static(batch2, subset_size, tokenizer, model, idx2word, \n",
    "                                                                          word2uniquevec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2uniquevec[\"mouse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(torch.Tensor(set_of_embeddings)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set_of_embeddings = list(word2uniquevec.values())\n",
    "embedding = torch.stack(set_of_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(word2uniquevec.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: to get set_of_embeddings as previously we just take the values of word2uniquevec\n",
    "word2uniquevec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
