{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling on abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import numpy as np\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, RegexpTokenizer,PunktSentenceTokenizer, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#abstracts = pd.read_csv(\"abstracts.csv\")\n",
    "abstracts = pd.read_csv(\"abstracts_eng.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "abstracts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_list = list(abstracts['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(abs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**21421** abstracts in total  \n",
    "**20494** abstracts in english"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Step 1 \n",
    "- tokenization \n",
    "- punctuation removal \n",
    "- lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised = []\n",
    "count = 0\n",
    "for abstract in abs_list:\n",
    "    raw = abstract\n",
    "    tokens = gensim.utils.simple_preprocess(str(raw), deacc=True)\n",
    "    tokenised.append(tokens)\n",
    "    count += len(tokens)\n",
    "print(str(count)+\" tokens created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "for doc in tokenised: \n",
    "    c+=doc\n",
    "len(set(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have **83831** / **71429** (de/en) unique words in the beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Step 2 \n",
    "- removing stopwords \n",
    "- (removing other words based on different strategies - like word length thresholding) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "cleaned = [[word for word in doc if word not in stop_words] for doc in tokenised]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider extending the stopwords ...\n",
    "# stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider removing words with less than [x] characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "for doc in cleaned: \n",
    "    c+=doc\n",
    "len(set(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after stopwords removal we have **83695** / **71293** terms (136 less)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Step 3 \n",
    "- stemming \n",
    "- lemmatizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "lemmatized = [[lemmatiser.lemmatize(word_stemmer.stem(word)) for word in doc] for doc in cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "for doc in lemmatized: \n",
    "    c+=doc\n",
    "len(set(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And after stemming and lemmatization we have **61182** / **50948** terms (22,513 less)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we could also lemmatise keeping only noun, adjective, verb, adverb\n",
    "\n",
    "data_lemmatized = lemmatization(bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "abstract_clean[23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Build n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be done before lemmatization and stemming in a lot of tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-grams creation hyperparameters \n",
    "# leaving here the hyperparameters so that we can tune them properly\n",
    "# min_count (float, optional) – Ignore all words and bigrams with total collected count lower than this value.\n",
    "b_min_c = 5 \n",
    "t_min_c = 5\n",
    "# threshold (float, optional) – Represent a score threshold for forming the phrases (higher means fewer phrases)\n",
    "b_thre = 50\n",
    "t_thre = 5\n",
    "# scoring ({'default', 'npmi', function}, optional) –Specify how potential phrases are scored\n",
    "# for now we go with default storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(lemmatized, min_count=b_min_c, threshold=b_thre) \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Two interesting results from the bigram model : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1: no change --> no bigrams found \n",
    "lemmatized[0]==bigram_mod[lemmatized[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 2: some change but we actually lose vocabulary ...\n",
    "len(bigram_mod[lemmatized[110]])-len(lemmatized[110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOREOVER, we have german words inside!!\n",
    "print(bigram_mod[lemmatized[110]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = gensim.models.Phrases(bigram[lemmatized], min_count=t_min_c, threshold=t_thre)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Take a look at some of the trigrams\n",
    "print(trigram_mod[lemmatized[31]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "for doc in bigrammed: \n",
    "    c+=doc\n",
    "len(set(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5470** bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "for doc in trigrammed: \n",
    "    c+=doc\n",
    "len(set(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12894** trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After grouping words that occur commonly together we have 69312 / **69312** terms (17,741 more)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Analyse the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = trigrammed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = flatten(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(counts.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3.7 install tomotopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tomotopy as tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Training example\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining hyperparameters\n",
    "tw = tp.TermWeight.ONE # term weighting scheme in TermWeight. The default value is TermWeight.ONE\n",
    "k = 150 # number of topics...\n",
    "min_cf=3 # minimum collection frequency of words. Words with a smaller collection frequency than min_cf are excluded from the model. The default value is 0, which means no words are excluded.\n",
    "min_df=0 # minimum document frequency of words. Words with a smaller document frequency than min_df are excluded from the model. The default value is 0, which means no words are excluded\n",
    "rm_top=5 # the number of top words to be removed. If you want to remove too common words from model, you can set this value to 1 or more. The default value is 0, which means no top words are removed.\n",
    "alpha = None # hyperparameter of Dirichlet distribution for document-topic\n",
    "eta = None # hyperparameter of Dirichlet distribution for topic-word\n",
    "seed = 41 # random seed\n",
    "model_burn_in = 100 \n",
    "train_updates = 1000\n",
    "train_iter = 10\n",
    "save_path = \"lda_model150.bin\" #.bin format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the model\n",
    "model = tp.LDAModel(tw=tp.TermWeight.ONE, min_cf=min_cf, rm_top=rm_top, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding documents to the model \n",
    "for doc in cleaned: model.add_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training**\n",
    "model.burn_in = model_burn_in\n",
    "# initialising \n",
    "model.train(iter=0)\n",
    "print('Num docs:', len(model.docs), ', Vocab size:', len(model.used_vocabs), ', Num words:', model.num_words)\n",
    "print('Removed top words:', model.removed_top_words)\n",
    "print('Training...', file=sys.stderr, flush=True)\n",
    "# actual training \n",
    "time = []\n",
    "LLs = []\n",
    "for i in range(0, train_updates, train_iter):\n",
    "    model.train(train_iter)\n",
    "    if i%100==0:print('Iteration: {}'.format(i))\n",
    "    time.append(i)\n",
    "    LLs.append(model.ll_per_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time,LLs)\n",
    "plt.ylabel('Log-likelihood')\n",
    "plt.xlabel('Iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading a model\n",
    "model1000 = tp.LDAModel.load(\"./lda1000.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tp.LDAModel.load(\"./lda_model150.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1000.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Saving...', file=sys.stderr, flush=True)\n",
    "model.save(save_path, full=True) # If full is True, the model with its all documents and state will be saved. If you want to train more after, use full model. If False, only topic parameters of the model will be saved. This model can be only used for inference of an unseen document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a better look at the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in range(model.k):\n",
    "    print('Topic #{}'.format(k))\n",
    "    for word, prob in model.get_topic_words(k):\n",
    "        print('\\t', word, prob, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Hyperparameter tuning by optimizing log-likelihood  \n",
    "\n",
    "---\n",
    "\n",
    "Note: log-likelihood is generally not considered a good measure for topic model performance..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LDA(documents, k, min_cf=0, min_df=0, rm_top=0, alpha=0.1, eta=0.01, model_burn_in=100, \n",
    "              train_updates = 1000, train_iter = 10):\n",
    "    \n",
    "    # instantiate\n",
    "    model = tp.LDAModel(tw=tp.TermWeight.ONE, min_df=min_df, min_cf=min_cf, rm_top=rm_top, k=k, alpha = alpha, \n",
    "                        eta = eta)\n",
    "    \n",
    "    # add documents to model\n",
    "    for doc in documents: model.add_doc(doc)\n",
    "    \n",
    "    # training**\n",
    "    model.burn_in = model_burn_in\n",
    "    # initialising \n",
    "    model.train(iter=0)\n",
    "    print('Num docs:', len(model.docs), ', Vocab size:', len(model.used_vocabs), ', Num words:', model.num_words)\n",
    "    print('Removed top words:', model.removed_top_words)\n",
    "    print('Training...', file=sys.stderr, flush=True)\n",
    "    # actual training \n",
    "    time = []\n",
    "    LLs = []\n",
    "    for i in range(0, train_updates, train_iter):\n",
    "        model.train(train_iter)\n",
    "        if i%100==0:print('Iteration: {}'.format(i))\n",
    "        time.append(i)\n",
    "        LLs.append(model.ll_per_word)\n",
    "    \n",
    "    return model, LLs, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple loop for minimizing perplexity on the training set\n",
    "\n",
    "topics = [10,20,30]\n",
    "perplexity_score = np.array([])\n",
    "for k in topics:\n",
    "    print(\"Training for \"+str(k)+\" topics\")\n",
    "    model, LLs, time = train_LDA(cleaned, k = k, train_updates = 600)\n",
    "    perplexity_score = np.append(perplexity_score, model.perplexity)\n",
    "    print(\"Perplexity = \"+str(model.perplexity))\n",
    "\n",
    "topics[np.argmin(perplexity_score)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(topics,perplexity_score)\n",
    "plt.ylabel('Perplexity')\n",
    "plt.xlabel('Number of topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in train and test set\n",
    "\n",
    "print(len(cleaned))\n",
    "train_size = int(0.8*len(cleaned))\n",
    "\n",
    "random.shuffle(cleaned)\n",
    "train_docs = cleaned[0:train_size]\n",
    "test_docs = cleaned[train_size:]\n",
    "\n",
    "assert len(train_docs) + len(test_docs) == len(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_LL(test_docs, model):\n",
    "    \n",
    "    # make a list of documents of type required by tp\n",
    "    test_set = []\n",
    "    for doc in test_docs:\n",
    "        test_set.append(model.make_doc(doc))\n",
    "    \n",
    "    # return topic distribution and log-likelihood of new documents\n",
    "    topic_dist, likelihood = model.infer(test_set)\n",
    "    \n",
    "    # use mean log-likelihood as performance measure\n",
    "    return np.mean(likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop for maximizing mean likelihood of test set\n",
    "\n",
    "topics = [10,20,30]\n",
    "log_likelihoods = np.array([])\n",
    "for k in topics:\n",
    "    print(\"Training for \"+str(k)+\" topics\")\n",
    "    model, LLs, time = train_LDA(train_docs, k = k, train_updates = 800)\n",
    "    log_likelihoods = np.append(log_likelihoods, get_test_LL(test_docs, model))\n",
    "    print(\"Log likelihood = \"+str(get_test_LL(test_docs, model)))\n",
    "\n",
    "topics[np.argmax(log_likelihoods)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(topics,log_likelihoods)\n",
    "plt.ylabel('Likelihood')\n",
    "plt.xlabel('Number of topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(likelihoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Topic coherence \n",
    "\n",
    "---\n",
    "\n",
    "Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_word = lambda x: x[0] # get_topic_words returns both the word and its probability in the topic\n",
    "topics = [[extract_word(tw) for tw in model.get_topic_words(k, 20)] for k in range(1,num_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(cleaned)\n",
    "BoW_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in cleaned] # bag of words corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BoW_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the coherence preprocessing operations into two functions\n",
    "extract_word = lambda x: x[0] # get_topic_words returns both the word and its probability in the topic\n",
    "\n",
    "def get_topics(model, num_topics):\n",
    "    return [[extract_word(tw) for tw in model.get_topic_words(k, 20)] for k in range(num_topics)]\n",
    "\n",
    "def get_corpus(dictionary, texts):\n",
    "    return [dictionary.doc2bow(doc, allow_update=True) for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics (list of list of str, optional) – List of tokenized topics\n",
    "# texts (list of list of str, optional) – Tokenized texts, needed for coherence models that use sliding window based (i.e. coherence=`c_something`) probability estimator .\n",
    "# corpus (iterable of list of (int, number), optional) – Corpus in BoW format.\n",
    "# dictionary (Dictionary, optional) – Gensim dictionary mapping of id word to create corpus. If model.id2word is present, this is not needed. If both are provided, passed dictionary will be used.\n",
    "# window_size (int, optional) – Is the size of the window to be used for coherence measures using boolean sliding window as their probability estimator. For ‘u_mass’ this doesn’t matter. If None - the default window sizes are used which are: ‘c_v’ - 110, ‘c_uci’ - 10, ‘c_npmi’ - 10.\n",
    "# coherence ({'u_mass', 'c_v', 'c_uci', 'c_npmi'}, optional) – Coherence measure to be used. Fastest method - ‘u_mass’, ‘c_uci’ also known as c_pmi. For ‘u_mass’ corpus should be provided, if texts is provided, it will be converted to corpus using the dictionary. For ‘c_v’, ‘c_uci’ and ‘c_npmi’ texts should be provided (corpus isn’t needed)\n",
    "# topn (int, optional) – Integer corresponding to the number of top words to be extracted from each topic.\n",
    "cm = CoherenceModel(topics=topics, corpus=BoW_corpus, dictionary=dictionary, texts=cleaned, coherence='c_v')\n",
    "coherence = cm.get_coherence()  # get coherence value\n",
    "coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: there are different types of coherence measures, we need to decide which to use!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.1.4 Complete grid search \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in train, test and validation set\n",
    "print(len(cleaned))\n",
    "train_size = int(0.7*len(cleaned)) #70% for training\n",
    "test_size = int(0.3*len(cleaned)) #30% for testing \n",
    "\n",
    "random.shuffle(cleaned)\n",
    "\n",
    "train_docs = cleaned[0:train_size]\n",
    "test_docs = cleaned[train_size:]\n",
    "\n",
    "len(train_docs) + len(test_docs) == len(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the grid\n",
    "\n",
    "ks = [50, 100, 150, 200, 300, 350, 450]\n",
    "#alpha = [1/k, 10/k, 0.1/k, None]\n",
    "#eta = [1/w, 10/w, 0.1/w, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test_pp(ll, docs):\n",
    "    \"\"\" pp = exp(-ll/ct)\"\"\"\n",
    "    ct = sum([len(docs) for doc in docs])\n",
    "    pp = np.exp(-1*ll/ct)\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Grid search of best topic number (this needs to run for a while)\n",
    "# We collect LL, perplexity and coherence scores, saving them in variables \n",
    "\n",
    "import time\n",
    "\n",
    "pps = []\n",
    "best_models = []\n",
    "# number of words in our vocabulary\n",
    "c = []\n",
    "for doc in train_docs: \n",
    "    c+=doc\n",
    "w = len(set(c))\n",
    "\n",
    "# define training parameters \n",
    "model_burn_in=250\n",
    "train_updates = 1000\n",
    "train_iter = 10\n",
    "\n",
    "for k in ks:\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    for alpha in [1/k, 10/k, 0.1/k, None]:      \n",
    "        for eta in [1/w, 10/w, 0.1/w, None]:\n",
    "    \n",
    "            print(\"K= \"+str(k)+\", alpha = \"+str(alpha)+\", eta=\"+str(eta)+\" -----------------------------\")\n",
    "            model, LLs, _ = train_LDA(train_docs, k, \n",
    "                                      alpha=alpha,\n",
    "                                      eta=eta, \n",
    "                                      model_burn_in=model_burn_in,\n",
    "                                      train_updates = train_updates, \n",
    "                                      train_iter = train_iter)\n",
    "\n",
    "            # LL\n",
    "            ll = get_test_LL(test_docs, model)\n",
    "            ## PP\n",
    "            # TODO: obtain perplexity on the test set\n",
    "            pp = compute_test_pp(ll, test_docs)\n",
    "            pps += [pp]\n",
    "            print(\"Test perplexity = \"+str(pp))\n",
    "\n",
    "            \n",
    "            end = time.time()\n",
    "\n",
    "            print(\"Time elapsed: \"+ str(round(end - start,1))+\" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the results\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10,12))\n",
    "axs[0].plot(ks,log_likelihoods)\n",
    "axs[1].plot(ks,perplexities)\n",
    "axs[2].plot(ks,coherences)\n",
    "axs[0].set_title(\"Test log-likelihood\")\n",
    "axs[1].set_title(\"Train perplexity\")\n",
    "axs[2].set_title(\"Train coherence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savin big model \n",
    "model.save(\"lda1000.bin\", full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the biggest model a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = get_topics(model, k)\n",
    "corpus = get_corpus(dictionary, train_docs)\n",
    "cm = CoherenceModel(topics=topics, corpus=corpus, dictionary=dictionary, texts=train_docs, coherence='c_v')\n",
    "cm.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(10,20):\n",
    "    print('Topic #{}'.format(k))\n",
    "    for word, prob in model.get_topic_words(k):\n",
    "        print('\\t', word, prob, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: evaluate the model on the validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_test_coherence(model, test_docs, dictionary):\n",
    "    \"\"\" Get topics over test set and compute coherence\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nice visualisation \n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3.7 install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: you need to have trained a model to use the following code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_term_dists = np.stack([model.get_topic_word_dist(k) for k in range(model.k)])\n",
    "doc_topic_dists = np.stack([doc.get_topic_dist() for doc in model.docs])\n",
    "doc_lengths = np.array([len(doc.words) for doc in model.docs])\n",
    "vocab = list(model.used_vocabs)\n",
    "term_frequency = model.used_vocab_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesn't work ...\n",
    "doc = pyLDAvis.prepare(\n",
    "    topic_term_dists, \n",
    "    doc_topic_dists, \n",
    "    doc_lengths, \n",
    "    vocab, \n",
    "    term_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 CTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "tw = tp.TermWeight.ONE # term weighting scheme in TermWeight. The default value is TermWeight.ONE\n",
    "k = 5 # number of topics\n",
    "min_cf=3 # minimum collection frequency of words. Words with a smaller collection frequency than min_cf are excluded from the model. The default value is 0, which means no words are excluded.\n",
    "rm_top=5 # the number of top words to be removed. If you want to remove too common words from model, you can set this value to 1 or more. The default value is 0, which means no top words are removed.\n",
    "alpha = None # hyperparameter of Dirichlet distribution for document-topic\n",
    "eta = None # hyperparameter of Dirichlet distribution for topic-word\n",
    "seed = 41 # random seed\n",
    "model_burn_in = 100 \n",
    "train_updates = 500\n",
    "train_iter = 10\n",
    "save_path = \"ctm_model.bin\" #.bin format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CTM(documents, k, min_cf=0, rm_top=0, smoothing_alpha=0.1, eta=0.01, model_burn_in=100, \n",
    "              train_updates = 1000, train_iter = 10):\n",
    "    \n",
    "    # instantiate\n",
    "    model = tp.CTModel(tw=tp.TermWeight.ONE, min_cf=min_cf, rm_top=rm_top, k=k, smoothing_alpha = smoothing_alpha,\n",
    "                      eta = eta)\n",
    "    \n",
    "    # add documents to model\n",
    "    for doc in documents: model.add_doc(doc)\n",
    "    \n",
    "    # training**\n",
    "    model.burn_in = model_burn_in\n",
    "    # initialising \n",
    "    model.train(iter=0)\n",
    "    print('Num docs:', len(model.docs), ', Vocab size:', len(model.used_vocabs), ', Num words:', model.num_words)\n",
    "    print('Removed top words:', model.removed_top_words)\n",
    "    print('Training...', file=sys.stderr, flush=True)\n",
    "    # actual training \n",
    "    time = []\n",
    "    LLs = []\n",
    "    for i in range(0, train_updates, train_iter):\n",
    "        model.train(train_iter)\n",
    "        if i%100==0:print('Iteration: {}'.format(i))\n",
    "        time.append(i)\n",
    "        LLs.append(model.ll_per_word)\n",
    "    \n",
    "    return model, LLs, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "model, LLs, _ = train_CTM(cleaned, k=k, min_cf=min_cf, rm_top=rm_top, smoothing_alpha=alpha, \n",
    "                                 eta=eta, model_burn_in=model_burn_in, \n",
    "                                 train_updates = train_updates, train_iter = train_iter)\n",
    "end = time.time()\n",
    "print(\"Time elapsed: \"+ str(round(end - start,1))+\" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare time to LDA with same hyperparameters\n",
    "\n",
    "start = time.time()\n",
    "train_LDA(cleaned, k=k, min_cf=min_cf, rm_top=rm_top, alpha=alpha, \n",
    "                                 eta=eta, model_burn_in=model_burn_in, \n",
    "                                 train_updates = train_updates, train_iter = train_iter)\n",
    "end = time.time()\n",
    "print(\"Time elapsed: \"+ str(round(end - start,1))+\" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(model.k):\n",
    "    print('Topic #{}'.format(k))\n",
    "    for word, prob in model.get_topic_words(k):\n",
    "        print('\\t', word, prob, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at topic correlations\n",
    "model.get_correlations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3.7 install pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization tool for topic correlations\n",
    "\n",
    "from pyvis.network import Network\n",
    "\n",
    "g = Network(width=800, height=800, font_color=\"#333\")\n",
    "correl = model.get_correlations().reshape([-1])\n",
    "correl.sort()\n",
    "top_tenth = model.k * (model.k - 1) // 10\n",
    "top_tenth = correl[-model.k - top_tenth]\n",
    "\n",
    "for k in range(model.k):\n",
    "    label = \"#{}\".format(k)\n",
    "    title= ' '.join(word for word, _ in model.get_topic_words(k, top_n=6))\n",
    "    print('Topic', label, title)\n",
    "    g.add_node(k, label=label, title=title, shape='ellipse')\n",
    "    for l, correlation in zip(range(k - 1), model.get_correlations(k)):\n",
    "        if correlation < top_tenth: continue\n",
    "        g.add_edge(k, l, value=float(correlation), title='{:.02}'.format(correlation))\n",
    "\n",
    "g.barnes_hut(gravity=-1000, spring_length=20)\n",
    "g.show_buttons()\n",
    "g.show(\"topic_network.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Complete grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions \n",
    "\n",
    "def get_test_LL(test_docs, model):\n",
    "    \n",
    "    # make a list of documents of type required by tp\n",
    "    test_set = []\n",
    "    for doc in test_docs:\n",
    "        test_set.append(model.make_doc(doc))\n",
    "    \n",
    "    # return topic distribution and log-likelihood of new documents\n",
    "    topic_dist, likelihood = model.infer(test_set)\n",
    "    \n",
    "    # use mean log-likelihood as performance measure\n",
    "    return np.mean(likelihood)\n",
    "\n",
    "def compute_test_pp(ll, docs):\n",
    "    \"\"\" pp = exp(-ll/ct)\"\"\"\n",
    "    ct = sum([len(docs) for doc in docs])\n",
    "    pp = np.exp(-1*ll/ct)\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in train, test and validation set\n",
    "print(len(cleaned))\n",
    "train_size = int(0.7*len(cleaned)) #70% for training\n",
    "test_size = int(0.3*len(cleaned)) #30% for testing \n",
    "\n",
    "random.shuffle(cleaned)\n",
    "\n",
    "train_docs = cleaned[0:train_size]\n",
    "test_docs = cleaned[train_size:]\n",
    "\n",
    "len(train_docs) + len(test_docs) == len(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the grid\n",
    "\n",
    "c = []\n",
    "for doc in train_docs: \n",
    "    c+=doc\n",
    "w = len(set(c))\n",
    "\n",
    "ks = [50, 100, 150, 200, 300, 350, 450]\n",
    "etas = [1/w, 10/w, 0.1/w, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search of best topic number (this needs to run for a while)\n",
    "# We collect LL, perplexity and coherence scores, saving them in variables \n",
    "\n",
    "import time\n",
    "\n",
    "pps = []\n",
    "best_models = []\n",
    "# number of words in our vocabulary\n",
    "\n",
    "# define training parameters \n",
    "model_burn_in=250\n",
    "train_updates = 1000\n",
    "train_iter = 10\n",
    "\n",
    "for k in ks:\n",
    "    \n",
    "    start = time.time()\n",
    "      \n",
    "    for eta in etas:\n",
    "\n",
    "        print(\"K= \"+str(k)+\", eta=\"+str(eta)+\" -----------------------------\")\n",
    "        model, LLs, _ = train_CTM(train_docs, k,\n",
    "                                  eta=eta, \n",
    "                                  model_burn_in=model_burn_in,\n",
    "                                  train_updates = train_updates, \n",
    "                                  train_iter = train_iter)\n",
    "\n",
    "        # LL\n",
    "        ll = get_test_LL(test_docs, model)\n",
    "        ## PP\n",
    "        pp = compute_test_pp(ll, test_docs)\n",
    "        pps += [pp]\n",
    "        print(\"Test perplexity = \"+str(pp))\n",
    "\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        print(\"Time elapsed: \"+ str(round(end - start,1))+\" s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Pachinko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "tw = tp.TermWeight.ONE # term weighting scheme in TermWeight. The default value is TermWeight.ONE\n",
    "k1 = 1 # the number of super topics \n",
    "k2 = 5 # the number of sub topics\n",
    "min_cf=3 # minimum collection frequency of words. Words with a smaller collection frequency than min_cf are excluded from the model. The default value is 0, which means no words are excluded.\n",
    "rm_top=5 # the number of top words to be removed. If you want to remove too common words from model, you can set this value to 1 or more. The default value is 0, which means no top words are removed.\n",
    "alpha = None # hyperparameter of Dirichlet distribution for document-topic\n",
    "eta = None # hyperparameter of Dirichlet distribution for topic-word\n",
    "seed = 41 # random seed\n",
    "model_burn_in = 100 \n",
    "train_updates = 500\n",
    "train_iter = 10\n",
    "save_path = \"pachinko_model.bin\" #.bin format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: only possible to have two layers in topic tree??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_PA(documents, k1, k2, min_cf=0, rm_top=0, alpha=0.1, eta=0.01, model_burn_in=100, \n",
    "              train_updates = 1000, train_iter = 10):\n",
    "    \n",
    "    # instantiate\n",
    "    model = tp.PAModel(tw=tp.TermWeight.ONE, min_cf=min_cf, rm_top=rm_top, k1=k1, k2=k2, alpha = alpha, eta = eta)\n",
    "    \n",
    "    # add documents to model\n",
    "    for doc in documents: model.add_doc(doc)\n",
    "    \n",
    "    # training**\n",
    "    model.burn_in = model_burn_in\n",
    "    # initialising \n",
    "    model.train(iter=0)\n",
    "    print('Num docs:', len(model.docs), ', Vocab size:', len(model.used_vocabs), ', Num words:', model.num_words)\n",
    "    print('Removed top words:', model.removed_top_words)\n",
    "    print('Training...', file=sys.stderr, flush=True)\n",
    "    # actual training \n",
    "    time = []\n",
    "    LLs = []\n",
    "    for i in range(0, train_updates, train_iter):\n",
    "        model.train(train_iter)\n",
    "        if i%100==0:print('Iteration: {}'.format(i))\n",
    "        time.append(i)\n",
    "        LLs.append(model.ll_per_word)\n",
    "    \n",
    "    return model, LLs, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "model, LLs, iters = train_PA(cleaned, k1=k1, k2=k2, min_cf=min_cf, rm_top=rm_top, alpha=alpha, \n",
    "                                 eta=eta, model_burn_in=model_burn_in, \n",
    "                                 train_updates = train_updates, train_iter = train_iter)\n",
    "end = time.time()\n",
    "print(\"Time elapsed: \"+ str(round(end - start,1))+\" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k1 in range(model.k1):\n",
    "    print('Topic #{}'.format(k1))\n",
    "    for word, prob in model.get_topic_words(k1):\n",
    "        print('\\t', word, prob, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_sub_topics(super_topic_id = 0), \"\\n\") # this returns an ordered list of the probabilities of the sub-topics\n",
    "print(model.get_sub_topics(super_topic_id = 1), \"\\n\")\n",
    "print(model.get_sub_topics(super_topic_id = 2))\n",
    "\n",
    "# super topics have more or less same prob for sub topics => requires tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Complete grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions \n",
    "\n",
    "def get_test_LL(test_docs, model):\n",
    "    \n",
    "    # make a list of documents of type required by tp\n",
    "    test_set = []\n",
    "    for doc in test_docs:\n",
    "        test_set.append(model.make_doc(doc))\n",
    "    \n",
    "    # return topic distribution and log-likelihood of new documents\n",
    "    topic_dist, likelihood = model.infer(test_set)\n",
    "    \n",
    "    # use mean log-likelihood as performance measure\n",
    "    return np.mean(likelihood)\n",
    "\n",
    "def compute_test_pp(ll, docs):\n",
    "    \"\"\" pp = exp(-ll/ct)\"\"\"\n",
    "    ct = sum([len(docs) for doc in docs])\n",
    "    pp = np.exp(-1*ll/ct)\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in train, test and validation set\n",
    "print(len(cleaned))\n",
    "train_size = int(0.7*len(cleaned)) #70% for training\n",
    "test_size = int(0.3*len(cleaned)) #30% for testing \n",
    "\n",
    "random.shuffle(cleaned)\n",
    "\n",
    "train_docs = cleaned[0:train_size]\n",
    "test_docs = cleaned[train_size:]\n",
    "\n",
    "len(train_docs) + len(test_docs) == len(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the grid\n",
    "\n",
    "c = []\n",
    "for doc in train_docs: \n",
    "    c+=doc\n",
    "w = len(set(c))\n",
    "\n",
    "k2s = [50, 100, 150, 200, 300, 350, 450]\n",
    "etas = [1/w, 10/w, 0.1/w, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search of best topic number (this needs to run for a while)\n",
    "# We collect LL, perplexity and coherence scores, saving them in variables \n",
    "\n",
    "import time\n",
    "\n",
    "pps = []\n",
    "best_models = []\n",
    "# number of words in our vocabulary\n",
    "c = []\n",
    "for doc in train_docs: \n",
    "    c+=doc\n",
    "w = len(set(c))\n",
    "\n",
    "# define training parameters \n",
    "model_burn_in=250\n",
    "train_updates = 1000\n",
    "train_iter = 10\n",
    "\n",
    "for k2 in k2s:\n",
    "    for k1 in [int(k2/5), int(k2/10), int(k2/20)]:\n",
    "    \n",
    "        start = time.time()\n",
    "\n",
    "        for alpha in [1/k1, 0.1/k1, 0.01/k1]:      \n",
    "            for eta in etas:\n",
    "\n",
    "                print(\"K1= \"+str(k1)+ \", K2= \" + str(k2) + \", alpha = \"+str(alpha)+\", eta=\"+str(eta)+\" -----------------------------\")\n",
    "                model, LLs, _ = train_PA(train_docs, k1 = k1, k2 = k2, \n",
    "                                          alpha=alpha,\n",
    "                                          eta=eta, \n",
    "                                          model_burn_in=model_burn_in,\n",
    "                                          train_updates = train_updates, \n",
    "                                          train_iter = train_iter)\n",
    "\n",
    "                # LL\n",
    "                ll = get_test_LL(test_docs, model)\n",
    "                ## PP\n",
    "                pp = compute_test_pp(ll, test_docs)\n",
    "                pps += [pp]\n",
    "                print(\"Test perplexity = \"+str(pp))\n",
    "\n",
    "\n",
    "                end = time.time()\n",
    "\n",
    "                print(\"Time elapsed: \"+ str(round(end - start,1))+\" s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Enriching the data\n",
    "\n",
    "In this section we merge the data extracted from the topic modeling back into the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(cleaned)\n",
    "topics = get_topics(model, k)\n",
    "corpus = get_corpus(dictionary, cleaned)\n",
    "cm = CoherenceModel(topics=topics, corpus=cleaned, dictionary=dictionary, texts=cleaned, coherence='c_v')\n",
    "cm.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic2words = [model.get_topic_word_dist(k) for k in range(model.k)]\n",
    "doc2topics = [doc.get_topic_dist() for doc in model.docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_topic_words(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
