{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy as tp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import nltk\n",
    "from nltk import word_tokenize, RegexpTokenizer,PunktSentenceTokenizer, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics embeddings pipeline \n",
    "Here we implement the embedding pipeline for the topics extracted by the collection. <br>\n",
    "The results will be the following: \n",
    "1. Topic embeddings \n",
    "2. Document embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading topic model and collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tp.LDAModel.load(\"lda_model150.bin\")\n",
    "data = pd.read_csv(\"abstracts_eng.csv\")\n",
    "collection = list(data['abstract'])\n",
    "num_topics = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = collection[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproducing the preprocessing steps taken to build the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "word_stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "def normalisation(document, stemming = True, lemmatising = True, min_word_len = 3):\n",
    "    tokens = gensim.utils.simple_preprocess(str(document), deacc=True, max_len = sys.maxsize)\n",
    "    cleaned = [word for word in tokens if word not in stop_words]\n",
    "    if stemming:\n",
    "        cleaned = [word_stemmer.stem(word) for word in cleaned]\n",
    "    if lemmatising:\n",
    "        cleaned = [lemmatiser.lemmatize(word) for word in cleaned]\n",
    "    cleaned = [word for word in cleaned if (min_word_len<=len(word))]\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the batch \n",
    "batch_normalised = [normalisation(doc) for doc in batch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_topics(document, model, min_score=0.8):\n",
    "    \"\"\" \n",
    "    Extracting top n topics for each document. \n",
    "    Selects the n most likely topics whose p(topic|document) sum to min_score.\n",
    "    \"\"\"\n",
    "    # inserting the document in the model\n",
    "    new_doc = model.make_doc(document)\n",
    "    _,_ = model.infer(new_doc)\n",
    "    # ordering from most probable topic to least one \n",
    "    dist = new_doc.get_topic_dist()\n",
    "    indices = np.flip(np.argsort(dist))\n",
    "    score = 0\n",
    "    indices_kept = []\n",
    "    probs_kept = []\n",
    "    for index in indices:\n",
    "        if score > min_score: break\n",
    "        score += dist[index]\n",
    "        indices_kept.append(index)\n",
    "        probs_kept.append(dist[index])\n",
    "    return list(zip(indices_kept, probs_kept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting topics for the original collection\n",
    "batch2topics = [get_top_topics(doc, model) for doc in batch_normalised]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(topic, model, min_score=0.8):\n",
    "    \"\"\"\n",
    "    Extracting top n words for each document. \n",
    "    Selects the n most likely words whose p(word|topic) sum to min_score.\n",
    "    \"\"\"\n",
    "    dist = model.get_topic_word_dist(topic)\n",
    "    indices = np.flip(np.argsort(dist))\n",
    "    score = 0\n",
    "    word_kept = []\n",
    "    word_prob_kept = []\n",
    "    for index in indices:\n",
    "        if score > min_score: break\n",
    "        score += dist[index]\n",
    "        word_kept.append(model.used_vocabs[index])\n",
    "        word_prob_kept.append(dist[index])\n",
    "    return list(zip(word_kept, word_prob_kept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics2words = [get_top_words(i, model, min_score=0.25) for i in range(num_topics)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embedding of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Glove Embeddings to embed the words in the topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import numpy as np \n",
    "import os \n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading from binary the glove vocabulary and embedding \n",
    "glove_vocab_path = \"glove_vocab\"\n",
    "glove_embedding_path = \"glove_embedding\"\n",
    "with open(glove_vocab_path, \"rb\") as fp:  \n",
    "    glove_vocab = pickle.load(fp)\n",
    "with open(glove_embedding_path, \"rb\") as fp: \n",
    "    glove_embedding = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vocab_normalised = {k:lemmatiser.lemmatize(word_stemmer.stem(v)) for k,v in glove_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_embeddings_topic(topic, vocab, embedding):\n",
    "    \"\"\" Topic is represented as a list of tuples (word, word weight)\"\"\"\n",
    "    matched = 0 \n",
    "    total = 0\n",
    "    topic_embeddings = []\n",
    "    topic_weights = []\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for item in topic: \n",
    "        word, weight = item\n",
    "        total+=1\n",
    "        #check if the word appears in vocabulary \n",
    "        if word in vocab.values(): \n",
    "            matched+=1\n",
    "            emb = embedding[list(vocab.values()).index(word)]\n",
    "            topic_embeddings += [emb.numpy()]\n",
    "            topic_weights += [weight]\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"Total time: \"+str(round(end-start,2))+\" s.\")\n",
    "    print(\"Proportion of matched words: \"+str(round(matched/total,2)))\n",
    "    return topic_embeddings, topic_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 0.34 s.\n",
      "Proportion of matched words: 0.92\n"
     ]
    }
   ],
   "source": [
    "topic_embeddings, topic_weights = get_list_embeddings_topic(topics2words[0], glove_vocab_normalised, glove_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_convex_topic_embedding(topic_weights, topic_embeddings):\n",
    "    \"\"\" Creates a topic embeddings as convex combination of embedding vectors according \n",
    "    to the weights provided.\"\"\"\n",
    "    weight_vec = np.asarray(topic_weights)\n",
    "    topic_vec = np.asarray(topic_embeddings)\n",
    "    normalized_weights = weight_vec / np.sqrt(np.sum(weight_vec**2))\n",
    "    return normalized_weights.dot(topic_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_emb = get_convex_topic_embedding(topic_weights, topic_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_convex_topics_embeddings(batch_topics, vocab, embedding):\n",
    "    \"\"\" Runs the above 2 functions to get the embedding for each topic in the batch.\"\"\"\n",
    "    topics_embs = []\n",
    "    for topic2word in topics2words:\n",
    "        topic_embeddings, topic_weights = get_list_embeddings_topic(topic2word, glove_vocab_normalised, glove_embedding)\n",
    "        topic_emb = get_convex_topic_embedding(topic_weights, topic_embeddings)\n",
    "        topics_embs += [topic_emb]\n",
    "    return topics_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 0.35 s.\n",
      "Proportion of matched words: 0.92\n",
      "Total time: 0.18 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.26 s.\n",
      "Proportion of matched words: 0.94\n",
      "Total time: 0.51 s.\n",
      "Proportion of matched words: 0.9\n",
      "Total time: 0.24 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.32 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.35 s.\n",
      "Proportion of matched words: 0.86\n",
      "Total time: 0.03 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.3 s.\n",
      "Proportion of matched words: 0.94\n",
      "Total time: 0.33 s.\n",
      "Proportion of matched words: 0.95\n",
      "Total time: 0.34 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.29 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.43 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.14 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.26 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.58 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.25 s.\n",
      "Proportion of matched words: 0.75\n",
      "Total time: 0.44 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.42 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.35 s.\n",
      "Proportion of matched words: 0.86\n",
      "Total time: 0.39 s.\n",
      "Proportion of matched words: 0.96\n",
      "Total time: 0.29 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.09 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.28 s.\n",
      "Proportion of matched words: 0.89\n",
      "Total time: 0.21 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.25 s.\n",
      "Proportion of matched words: 0.89\n",
      "Total time: 0.34 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.41 s.\n",
      "Proportion of matched words: 0.96\n",
      "Total time: 0.31 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.25 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.3 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.34 s.\n",
      "Proportion of matched words: 0.92\n",
      "Total time: 0.59 s.\n",
      "Proportion of matched words: 0.98\n",
      "Total time: 0.38 s.\n",
      "Proportion of matched words: 0.93\n",
      "Total time: 0.38 s.\n",
      "Proportion of matched words: 0.87\n",
      "Total time: 0.31 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.28 s.\n",
      "Proportion of matched words: 0.9\n",
      "Total time: 0.19 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.3 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.5 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.47 s.\n",
      "Proportion of matched words: 0.96\n",
      "Total time: 0.24 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.22 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.3 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.22 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.32 s.\n",
      "Proportion of matched words: 0.96\n",
      "Total time: 0.43 s.\n",
      "Proportion of matched words: 0.83\n",
      "Total time: 0.47 s.\n",
      "Proportion of matched words: 0.88\n",
      "Total time: 0.42 s.\n",
      "Proportion of matched words: 0.97\n",
      "Total time: 0.09 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.41 s.\n",
      "Proportion of matched words: 0.83\n",
      "Total time: 0.08 s.\n",
      "Proportion of matched words: 0.86\n",
      "Total time: 0.33 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.24 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.44 s.\n",
      "Proportion of matched words: 0.94\n",
      "Total time: 0.35 s.\n",
      "Proportion of matched words: 0.96\n",
      "Total time: 0.32 s.\n",
      "Proportion of matched words: 0.95\n",
      "Total time: 0.53 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.51 s.\n",
      "Proportion of matched words: 0.95\n",
      "Total time: 0.52 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.09 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.3 s.\n",
      "Proportion of matched words: 0.91\n",
      "Total time: 0.67 s.\n",
      "Proportion of matched words: 0.89\n",
      "Total time: 0.25 s.\n",
      "Proportion of matched words: 0.94\n",
      "Total time: 0.28 s.\n",
      "Proportion of matched words: 0.94\n",
      "Total time: 0.37 s.\n",
      "Proportion of matched words: 0.89\n",
      "Total time: 0.55 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.36 s.\n",
      "Proportion of matched words: 0.92\n",
      "Total time: 0.23 s.\n",
      "Proportion of matched words: 0.87\n",
      "Total time: 0.33 s.\n",
      "Proportion of matched words: 0.9\n",
      "Total time: 0.39 s.\n",
      "Proportion of matched words: 0.88\n",
      "Total time: 0.28 s.\n",
      "Proportion of matched words: 0.94\n",
      "Total time: 0.55 s.\n",
      "Proportion of matched words: 0.75\n",
      "Total time: 0.31 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.14 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.2 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.51 s.\n",
      "Proportion of matched words: 0.97\n",
      "Total time: 0.26 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.35 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.19 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.54 s.\n",
      "Proportion of matched words: 0.92\n",
      "Total time: 0.3 s.\n",
      "Proportion of matched words: 0.86\n",
      "Total time: 0.24 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.14 s.\n",
      "Proportion of matched words: 0.9\n",
      "Total time: 0.27 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.46 s.\n",
      "Proportion of matched words: 0.86\n",
      "Total time: 0.52 s.\n",
      "Proportion of matched words: 0.88\n",
      "Total time: 0.14 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.46 s.\n",
      "Proportion of matched words: 0.86\n",
      "Total time: 0.43 s.\n",
      "Proportion of matched words: 0.92\n",
      "Total time: 0.18 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.07 s.\n",
      "Proportion of matched words: 0.8\n",
      "Total time: 0.36 s.\n",
      "Proportion of matched words: 0.96\n",
      "Total time: 0.24 s.\n",
      "Proportion of matched words: 0.94\n",
      "Total time: 0.25 s.\n",
      "Proportion of matched words: 0.94\n",
      "Total time: 0.37 s.\n",
      "Proportion of matched words: 0.95\n",
      "Total time: 0.09 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.53 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.54 s.\n",
      "Proportion of matched words: 0.97\n",
      "Total time: 0.4 s.\n",
      "Proportion of matched words: 0.96\n",
      "Total time: 0.15 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.56 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.21 s.\n",
      "Proportion of matched words: 0.94\n",
      "Total time: 0.31 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.26 s.\n",
      "Proportion of matched words: 0.9\n",
      "Total time: 0.5 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.37 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.42 s.\n",
      "Proportion of matched words: 0.92\n",
      "Total time: 0.34 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.29 s.\n",
      "Proportion of matched words: 0.93\n",
      "Total time: 0.2 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.52 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.47 s.\n",
      "Proportion of matched words: 0.86\n",
      "Total time: 0.36 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.66 s.\n",
      "Proportion of matched words: 0.95\n",
      "Total time: 0.34 s.\n",
      "Proportion of matched words: 0.95\n",
      "Total time: 0.45 s.\n",
      "Proportion of matched words: 0.93\n",
      "Total time: 0.23 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.22 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.27 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.59 s.\n",
      "Proportion of matched words: 0.97\n",
      "Total time: 0.65 s.\n",
      "Proportion of matched words: 0.96\n",
      "Total time: 0.38 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.05 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.6 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.46 s.\n",
      "Proportion of matched words: 0.82\n",
      "Total time: 0.43 s.\n",
      "Proportion of matched words: 0.97\n",
      "Total time: 0.03 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.51 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.38 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.3 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.42 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.26 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.55 s.\n",
      "Proportion of matched words: 0.91\n",
      "Total time: 0.36 s.\n",
      "Proportion of matched words: 0.96\n",
      "Total time: 0.24 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.33 s.\n",
      "Proportion of matched words: 0.92\n",
      "Total time: 0.34 s.\n",
      "Proportion of matched words: 0.73\n",
      "Total time: 0.35 s.\n",
      "Proportion of matched words: 0.92\n",
      "Total time: 0.46 s.\n",
      "Proportion of matched words: 0.97\n",
      "Total time: 0.48 s.\n",
      "Proportion of matched words: 0.95\n",
      "Total time: 0.24 s.\n",
      "Proportion of matched words: 0.94\n",
      "Total time: 0.61 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.35 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.32 s.\n",
      "Proportion of matched words: 0.96\n",
      "Total time: 0.49 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.42 s.\n",
      "Proportion of matched words: 0.82\n",
      "Total time: 0.2 s.\n",
      "Proportion of matched words: 0.93\n",
      "Total time: 0.34 s.\n",
      "Proportion of matched words: 1.0\n",
      "Total time: 0.18 s.\n",
      "Proportion of matched words: 0.83\n"
     ]
    }
   ],
   "source": [
    "topics_embs = get_convex_topics_embeddings(topics2words[0], glove_vocab_normalised, glove_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look at the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbors(topic, topics):\n",
    "    \"\"\"Returns the most similar topics to the given one in the listo of topics\"\"\"\n",
    "    cos = torch.nn.CosineSimilarity(dim = -1)\n",
    "    ranks = cos(torch.tensor(topic), torch.tensor(topics))\n",
    "    mostSimilar = []\n",
    "    return ranks.numpy().argsort()[::-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the first topic \n",
    "nns_0 = nearest_neighbors(topics_embs[0], topics_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_most_similar(topic_id, topics_embs, topics2words, n=10):\n",
    "    \"\"\"Prints the words of the topic and its neareast neighbors.\"\"\"\n",
    "    nns = nearest_neighbors(topics_embs[topic_id], topics_embs)\n",
    "    print(\"-\"*10)\n",
    "    print(\"Topic \"+ str(topic_id))\n",
    "    print(\" \".join(item[0] for item in topics2words[0]))\n",
    "    print(\"-\"*10)\n",
    "    print(str(n)+\" most similar topics\")\n",
    "    for i in range(n):\n",
    "        print(\"Topic \"+str(i+1))\n",
    "        print(\" \".join(item[0] for item in topics2words[nns[i+1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Topic 0\n",
      "flow fluid transport pressur veloc measur air turbul channel water liquid bubbl heat_transfer particl viscos two regim ga simul convect capillari porou_medium surfac droplet permeabl\n",
      "----------\n",
      "10 most similar topics\n",
      "Topic 1\n",
      "simul transport atmospher event region convect wind precipit km surfac vertic trajectori air_mass associ day block local advect air flow weather temperatur variabl strong larg_scale mesoscal near_surfac moistur storm intens forecast winter valley system meteorolog anomali cyclon\n",
      "Topic 2\n",
      "diffus water mixtur solvent system liquid diffus_coeffici predict data rh coeffici organ compon calcul describ methanol experiment_data interact\n",
      "Topic 3\n",
      "sediment carbon oxygen sourc cr fraction isotop oc methan miner dissolv age deposit organ_carbon releas cycl concentr flux ocean marin water suggest valu organ_matter deriv pool format produc degrad indic signatur lake process potenti terrestri sedimentari transport river seawat\n",
      "Topic 4\n",
      "particl temperatur droplet activ water inp ice ice_nucleat freez size measur aerosol atmospher aerosol_particl cloud nucleat concentr observ soot particl_size investig\n",
      "Topic 5\n",
      "surfac structur shape polar field wave direct orient measur domain scatter imag phase local geometri along show mechan distribut contact acoust layer angl boundari insid densiti\n",
      "Topic 6\n",
      "wood pore materi poros dri composit structur cellulos adsorpt adsorb water mechan effect properti glass cell_wall swell layer explos sorption porou\n",
      "Topic 7\n",
      "ocean water region marin global southern_ocean coral phytoplankton ka sea north_atlant data depth record diatom age climat carbon pco product biomass temperatur co chang concentr surfac reconstruct flux biolog distribut th holocen basin si atlant shelf upwel control glacial tracer nutrient current\n",
      "Topic 8\n",
      "engin voltag oper current loss inject power thermal fuel switch electr ignit circuit convert combust low cycl temperatur heat lean control charg insul load\n",
      "Topic 9\n",
      "melt rock miner magma composit fluid crystal erupt age temperatur magmat earth sampl pb zircon form isotop volcan quartz process metal basalt trace_element garnet rich ree core condit deposit carbon data format hydrotherm olivin gpa abund metamorph ca chemic element system indic zone\n",
      "Topic 10\n",
      "magnet electron state observ spin energi interact system coupl effect field atom measur phase magnet_field excit materi temperatur quantum reson charg transit find band pul show order topolog strong experi regim depend transport report laser investig tunnel induc demonstr calcul\n"
     ]
    }
   ],
   "source": [
    "visualise_most_similar(0, topics_embs, topics2words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Documents embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
