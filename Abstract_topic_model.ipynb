{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling on abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import numpy as np\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, RegexpTokenizer,PunktSentenceTokenizer, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#abstracts = pd.read_csv(\"abstracts.csv\")\n",
    "abstracts = pd.read_csv(\"abstracts_eng.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The goal of this note is to introduce new clas...</td>\n",
       "      <td>188444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We will review a Lemma published by Ran Raz in...</td>\n",
       "      <td>188623.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>China’s growing influence in Europe has the po...</td>\n",
       "      <td>346708.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nowhere is China's Belt and Road Initiative (B...</td>\n",
       "      <td>346709.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Passenger transit modes typical of the urban s...</td>\n",
       "      <td>187461.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract        id\n",
       "0  The goal of this note is to introduce new clas...  188444.0\n",
       "1  We will review a Lemma published by Ran Raz in...  188623.0\n",
       "2  China’s growing influence in Europe has the po...  346708.0\n",
       "3  Nowhere is China's Belt and Road Initiative (B...  346709.0\n",
       "4  Passenger transit modes typical of the urban s...  187461.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_list = list(abstracts['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20494"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(abs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**21421** abstracts in total  \n",
    "**20494** abstracts in english"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Step 1 \n",
    "- tokenization \n",
    "- punctuation removal \n",
    "- lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3855522 tokens created\n"
     ]
    }
   ],
   "source": [
    "tokenised = []\n",
    "count = 0\n",
    "for abstract in abs_list:\n",
    "    raw = abstract\n",
    "    tokens = gensim.utils.simple_preprocess(str(raw), deacc=True)\n",
    "    tokenised.append(tokens)\n",
    "    count += len(tokens)\n",
    "print(str(count)+\" tokens created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71429"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = []\n",
    "for doc in tokenised: \n",
    "    c+=doc\n",
    "len(set(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have **83831** / **71429** (de/en) unique words in the beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Step 2 \n",
    "- removing stopwords \n",
    "- (removing other words based on different strategies - like word length thresholding) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "cleaned = [[word for word in doc if word not in stop_words] for doc in tokenised]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider extending the stopwords ...\n",
    "# stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider removing words with less than [x] characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71293"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = []\n",
    "for doc in cleaned: \n",
    "    c+=doc\n",
    "len(set(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after stopwords removal we have **83695** / **71293** terms (136 less)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Step 3 \n",
    "- stemming \n",
    "- lemmatizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "lemmatized = [[lemmatiser.lemmatize(word_stemmer.stem(word)) for word in doc] for doc in cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50948"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = []\n",
    "for doc in lemmatized: \n",
    "    c+=doc\n",
    "len(set(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And after stemming and lemmatization we have **61182** / **50948** terms (22,513 less)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we could also lemmatise keeping only noun, adjective, verb, adverb\n",
    "\n",
    "data_lemmatized = lemmatization(bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daniel: slightly different approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_clean = []\n",
    "for i in range(len(lemmatized)):\n",
    "    b=(' '.join(word for word in lemmatized[i]))\n",
    "    abstract_clean.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'last two decad electr distribut sector wit wave regulatori reform aim improv effici incent regul regul scheme use benchmark name measur compani effici reward accordingli reliabl effici estim crucial effect implement incent mechan main problem face regul choic among sever legitim benchmark model usual produc differ result brief overview benchmark methodolog paper summar method use regul practic sever oecd countri benchmark practic rel widespread repeat observ similar compani time name panel data allow better understand unobserv firm specif factor disentangl effici estim focus parametr cost frontier model paper present two altern approach could use improv reliabl benchmark method base recent empir evid draw recommend regulatori practic power distribut network'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_clean[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Build n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be done before lemmatization and stemming in a lot of tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-grams creation hyperparameters \n",
    "# leaving here the hyperparameters so that we can tune them properly\n",
    "# min_count (float, optional) – Ignore all words and bigrams with total collected count lower than this value.\n",
    "b_min_c = 5 \n",
    "t_min_c = 5\n",
    "# threshold (float, optional) – Represent a score threshold for forming the phrases (higher means fewer phrases)\n",
    "b_thre = 50\n",
    "t_thre = 5\n",
    "# scoring ({'default', 'npmi', function}, optional) –Specify how potential phrases are scored\n",
    "# for now we go with default storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(lemmatized, min_count=b_min_c, threshold=b_thre) \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two interesting results from the bigram model : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Case 1: no change --> no bigrams found \n",
    "lemmatized[0]==bigram_mod[lemmatized[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-17"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Case 2: some change but we actually lose vocabulary ...\n",
    "len(bigram_mod[lemmatized[110]])-len(lemmatized[110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['edit', 'volum', 'inquir', 'use', 'predict', 'intersect', 'polit', 'academia', 'reflect', 'upon', 'implic', 'futur', 'orient', 'polici', 'make', 'across', 'differ', 'field', 'first', 'part', 'book', 'discus', 'differ', 'academ', 'perspect', 'contribut', 'futur', 'orient', 'polici', 'make', 'second', 'part', 'discus', 'role', 'futur', 'knowledg', 'decis_make', 'across', 'differ', 'empir', 'issu', 'climat', 'health', 'financ', 'bio', 'nuclear_weapon', 'civil_war', 'crime', 'analys', 'predict', 'integr', 'public', 'polici', 'govern', 'return', 'govern', 'structur', 'influenc', 'make', 'knowledg', 'futur', 'volum', 'contribut', 'better_understand', 'complex', 'interact', 'feedback_loop', 'process', 'creat', 'knowledg', 'futur', 'applic', 'futur', 'knowledg', 'public', 'polici', 'govern', 'publish', 'websit', 'dieser', 'sammelband', 'untersucht', 'den', 'einsatz', 'von', 'vorhersagen', 'der', 'schnittstel', 'zwischen', 'politik_und', 'wissenschaft', 'und', 'reflektiert', 'uber_die', 'auswirkungen', 'verschiedenen', 'bereichen', 'im_ersten', 'teil', 'de', 'buch', 'werden', 'verschieden', 'akademisch', 'perspektiven', 'und', 'beitrag', 'zur', 'diskutiert', 'im', 'zweiten', 'teil', 'wird_die', 'roll', 'zukunftigen', 'wissen', 'bei_der', 'verschiedenen', 'empirischen', 'bereichen', 'wie', 'klima', 'gesundheit', 'finanzen', 'bio', 'und', 'atomwaffen', 'burgerkrieg', 'und', 'kriminalitat', 'untersucht', 'e', 'wird', 'analysiert', 'wie', 'vorhersagen', 'die', 'offentlich', 'politik_und', 'integriert', 'werden', 'und', 'wie', 'wiederum', 'die', 'da', 'generieren', 'von', 'wissen', 'uber_die', 'zukunft', 'beeinflussen', 'der', 'sammelband', 'tragt', 'zu_einem', 'besseren', 'verstandni', 'der', 'komplexen', 'und', 'zwischen_den', 'prozessen', 'der', 'schaffung', 'von', 'wissen', 'uber_die', 'zukunft', 'und_der', 'anwendung', 'dy', 'wissen', 'politik_und', 'verwaltung', 'bei']\n"
     ]
    }
   ],
   "source": [
    "# MOREOVER, we have german words inside!!\n",
    "print(bigram_mod[lemmatized[110]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = gensim.models.Phrases(bigram[lemmatized], min_count=t_min_c, threshold=t_thre)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['relev', 'nuclear_weapon', 'world', 'affair', 'increas_decreas', 'nuclear_power', 'modern', 'arsen', 'may', 'result', 'destabil', 'effect', 'nuclear', 'deterr', 'constel', 'time', 'discrep', 'import', 'arm', 'control', 'necessari', 'supplement', 'nuclear', 'deterr', 'one_hand', 'actual', 'limit', 'role', 'intern', 'affair', 'hand', 'constantli', 'grow', 'order_avoid', 'futur', 'nuclear_war', 'creat', 'strateg', 'stabil', 'renaiss', 'arm', 'control', 'urgent_need']\n"
     ]
    }
   ],
   "source": [
    "# Take a look at some of the trigrams\n",
    "print(trigram_mod[lemmatized[31]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrammed = make_bigrams(lemmatized)\n",
    "trigrammed = make_trigrams(bigrammed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56418"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = []\n",
    "for doc in bigrammed: \n",
    "    c+=doc\n",
    "len(set(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5470** bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69312"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = []\n",
    "for doc in trigrammed: \n",
    "    c+=doc\n",
    "len(set(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12894** trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After grouping words that occur commonly together we have 69312 / **69312** terms (17,741 more)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Analyse the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = trigrammed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = flatten(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', 11657),\n",
       " ('model', 10841),\n",
       " ('results', 9409),\n",
       " ('based', 9307),\n",
       " ('using', 8577),\n",
       " ('two', 7303),\n",
       " ('study', 7211),\n",
       " ('high', 7092),\n",
       " ('time', 7025),\n",
       " ('different', 6783)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(counts.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3.7 install tomotopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tomotopy as tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training example\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining hyperparameters\n",
    "tw = tp.TermWeight.ONE # term weighting scheme in TermWeight. The default value is TermWeight.ONE\n",
    "k = 30 # number of topics...\n",
    "min_cf=3 # minimum collection frequency of words. Words with a smaller collection frequency than min_cf are excluded from the model. The default value is 0, which means no words are excluded.\n",
    "min_df=0 # minimum document frequency of words. Words with a smaller document frequency than min_df are excluded from the model. The default value is 0, which means no words are excluded\n",
    "rm_top=5 # the number of top words to be removed. If you want to remove too common words from model, you can set this value to 1 or more. The default value is 0, which means no top words are removed.\n",
    "alpha = None # hyperparameter of Dirichlet distribution for document-topic\n",
    "eta = None # hyperparameter of Dirichlet distribution for topic-word\n",
    "seed = 41 # random seed\n",
    "model_burn_in = 100 \n",
    "train_updates = 1000\n",
    "train_iter = 10\n",
    "save_path = \"lda_model.bin\" #.bin format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the model\n",
    "model = tp.LDAModel(tw=tp.TermWeight.ONE, min_cf=min_cf, rm_top=rm_top, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding documents to the model \n",
    "for doc in cleaned: model.add_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num docs: 20494 , Vocab size: 41921 , Num words: 2047104\n",
      "Removed top words: ['use', 'model', 'result', 'studi', 'base']\n",
      "Iteration: 0\n",
      "Iteration: 100\n",
      "Iteration: 200\n",
      "Iteration: 300\n",
      "Iteration: 400\n",
      "Iteration: 500\n",
      "Iteration: 600\n",
      "Iteration: 700\n",
      "Iteration: 800\n",
      "Iteration: 900\n"
     ]
    }
   ],
   "source": [
    "# training**\n",
    "model.burn_in = model_burn_in\n",
    "# initialising \n",
    "model.train(iter=0)\n",
    "print('Num docs:', len(model.docs), ', Vocab size:', len(model.used_vocabs), ', Num words:', model.num_words)\n",
    "print('Removed top words:', model.removed_top_words)\n",
    "print('Training...', file=sys.stderr, flush=True)\n",
    "# actual training \n",
    "time = []\n",
    "LLs = []\n",
    "for i in range(0, train_updates, train_iter):\n",
    "    model.train(train_iter)\n",
    "    if i%100==0:print('Iteration: {}'.format(i))\n",
    "    time.append(i)\n",
    "    LLs.append(model.ll_per_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Iteration')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(time,LLs)\n",
    "plt.ylabel('Log-likelihood')\n",
    "plt.xlabel('Iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Basic Info>\n",
      "| LDAModel (current version: 0.9.1)\n",
      "| 20494 docs, 2047104 words\n",
      "| Total Vocabs: 68689, Used Vocabs: 41921\n",
      "| Entropy of words: -8.69615\n",
      "| Removed Vocabs: use model result studi base\n",
      "|\n",
      "<Training Info>\n",
      "| Iterations: 1000, Burn-in steps: 100\n",
      "| Optimization Interval: 10\n",
      "| Log-likelihood per word: -8.63168\n",
      "|\n",
      "<Initial Parameters>\n",
      "| tw: TermWeight.ONE\n",
      "| min_cf: 3 (minimum collection frequency of words)\n",
      "| min_df: 0 (minimum document frequency of words)\n",
      "| rm_top: 5 (the number of top words to be removed)\n",
      "| k: 30 (the number of topics between 1 ~ 32767)\n",
      "| alpha: 0.1 (hyperparameter of Dirichlet distribution for document-topic)\n",
      "| eta: 0.01 (hyperparameter of Dirichlet distribution for topic-word)\n",
      "| seed: 413981393 (random seed)\n",
      "| trained in version 0.9.1\n",
      "|\n",
      "<Parameters>\n",
      "| alpha (Dirichlet prior on the per-document topic distributions)\n",
      "|  [0.02115543 0.04057477 0.0473648  0.03881337 0.03060828 0.03408973\n",
      "|   0.04750634 0.02920154 0.03210319 0.03348985 0.14166176 0.02239393\n",
      "|   0.02050162 0.34959254 0.01789482 0.00670509 0.02380325 0.02882896\n",
      "|   0.03361249 0.05521702 0.02625764 0.04729076 0.01606809 0.05106092\n",
      "|   0.03988693 0.01632953 0.01218499 0.02354932 0.05027501 0.1976262 ]\n",
      "| eta (Dirichlet prior on the per-topic word distribution)\n",
      "|  0.01\n",
      "|\n",
      "<Topics>\n",
      "| #0 (30640) : survey activ data particip choic\n",
      "| #1 (58831) : countri polici effect firm product\n",
      "| #2 (70662) : reaction oxid activ water surfac\n",
      "| #3 (53094) : system optim energi design product\n",
      "| #4 (39046) : social group work relat student\n",
      "| #5 (62621) : chang climat region temperatur simul\n",
      "| #6 (69961) : electron magnet measur optic field\n",
      "| #7 (58081) : soil plant speci forest increa\n",
      "| #8 (64799) : gene speci popul genom sequenc\n",
      "| #9 (48734) : algorithm network problem graph show\n",
      "| #10 (149690) : develop research data system process\n",
      "| #11 (48981) : neuron brain activ function connect\n",
      "| #12 (45987) : emiss observ measur co ozon\n",
      "| #13 (280123) : differ effect chang observ howev\n",
      "| #14 (31187) : measur jet present data particl\n",
      "| #15 (13648) : der und die von de\n",
      "| #16 (44626) : bacteria strain bacteri product infect\n",
      "| #17 (38404) : imag seismic map earthquak measur\n",
      "| #18 (75413) : patient as measur group method\n",
      "| #19 (72355) : gener function theori problem approxim\n",
      "| #20 (54735) : sediment water lake river age\n",
      "| #21 (59434) : system control perform sensor design\n",
      "| #22 (35950) : search mass product event standard_model\n",
      "| #23 (70213) : materi structur flow mechan surfac\n",
      "| #24 (88276) : cell express activ induc human\n",
      "| #25 (36433) : particl aerosol cloud measur observ\n",
      "| #26 (17242) : mass observ galaxi detect imag\n",
      "| #27 (35137) : network urban simul citi traffic\n",
      "| #28 (97113) : protein function cell activ complex\n",
      "| #29 (195688) : method data approach measur estim\n",
      "|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    }
   ],
   "source": [
    "print('Saving...', file=sys.stderr, flush=True)\n",
    "model.save(save_path, full=True) # If full is True, the model with its all documents and state will be saved. If you want to train more after, use full model. If False, only topic parameters of the model will be saved. This model can be only used for inference of an unseen document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a better look at the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0\n",
      "\t\tsurvey\t0.013973632827401161\n",
      "\t\tactiv\t0.012106232345104218\n",
      "\t\tdata\t0.011494496837258339\n",
      "\t\tparticip\t0.009594899602234364\n",
      "\t\tchoic\t0.009111951105296612\n",
      "\t\ttime\t0.008822182193398476\n",
      "\t\tperson\t0.007405532523989677\n",
      "\t\thousehold\t0.007180156651884317\n",
      "\t\tbehavior\t0.007147959899157286\n",
      "\t\testim\t0.007147959899157286\n",
      "Topic #1\n",
      "\t\tcountri\t0.01404231321066618\n",
      "\t\tpolici\t0.012152024544775486\n",
      "\t\teffect\t0.011071859858930111\n",
      "\t\tfirm\t0.008573978208005428\n",
      "\t\tproduct\t0.008523344993591309\n",
      "\t\tfind\t0.008287059143185616\n",
      "\t\tmarket\t0.007881997153162956\n",
      "\t\tprice\t0.007240649312734604\n",
      "\t\teconom\t0.006886220537126064\n",
      "\t\tincrea\t0.006126729305833578\n",
      "Topic #2\n",
      "\t\treaction\t0.008497463539242744\n",
      "\t\toxid\t0.00690773269161582\n",
      "\t\tactiv\t0.006767048500478268\n",
      "\t\twater\t0.005782259628176689\n",
      "\t\tsurfac\t0.0055853016674518585\n",
      "\t\tformat\t0.005543096456676722\n",
      "\t\tchemic\t0.005332070402801037\n",
      "\t\tconcentr\t0.005303933285176754\n",
      "\t\tco\t0.005135112442076206\n",
      "\t\tprocess\t0.00483967550098896\n",
      "Topic #3\n",
      "\t\tsystem\t0.01928514428436756\n",
      "\t\toptim\t0.016986647620797157\n",
      "\t\tenergi\t0.015921488404273987\n",
      "\t\tdesign\t0.014090165495872498\n",
      "\t\tproduct\t0.01171691995114088\n",
      "\t\tprocess\t0.011530050076544285\n",
      "\t\toper\t0.010894693434238434\n",
      "\t\tcost\t0.009194178506731987\n",
      "\t\teffici\t0.008129021152853966\n",
      "\t\treduc\t0.007587098516523838\n",
      "Topic #4\n",
      "\t\tsocial\t0.011352024041116238\n",
      "\t\tgroup\t0.008640775457024574\n",
      "\t\twork\t0.008463403210043907\n",
      "\t\trelat\t0.0070190937258303165\n",
      "\t\tstudent\t0.006993754766881466\n",
      "\t\tarchitectur\t0.005574783310294151\n",
      "\t\tcooper\t0.005524105858057737\n",
      "\t\tlearn\t0.005296056624501944\n",
      "\t\tdesign\t0.005194701720029116\n",
      "\t\tconcept\t0.004865297582000494\n",
      "Topic #5\n",
      "\t\tchang\t0.01261115726083517\n",
      "\t\tclimat\t0.012230447493493557\n",
      "\t\tregion\t0.011421440169215202\n",
      "\t\ttemperatur\t0.011246948502957821\n",
      "\t\tsimul\t0.009803425520658493\n",
      "\t\tobserv\t0.008677160367369652\n",
      "\t\tincrea\t0.008328177034854889\n",
      "\t\tprecipit\t0.006662573199719191\n",
      "\t\twater\t0.006646709982305765\n",
      "\t\timpact\t0.006028057541698217\n",
      "Topic #6\n",
      "\t\telectron\t0.008482640609145164\n",
      "\t\tmagnet\t0.008127426728606224\n",
      "\t\tmeasur\t0.008127426728606224\n",
      "\t\toptic\t0.006365567911416292\n",
      "\t\tfield\t0.006266108248382807\n",
      "\t\tsystem\t0.006223482545465231\n",
      "\t\treson\t0.005726183764636517\n",
      "\t\tmateri\t0.005726183764636517\n",
      "\t\texperi\t0.005726183764636517\n",
      "\t\tdevic\t0.005655141081660986\n",
      "Topic #7\n",
      "\t\tsoil\t0.019025743007659912\n",
      "\t\tplant\t0.01878642849624157\n",
      "\t\tspeci\t0.014205248095095158\n",
      "\t\tforest\t0.010871926322579384\n",
      "\t\tincrea\t0.008615524508059025\n",
      "\t\teffect\t0.007162538357079029\n",
      "\t\tproduct\t0.006222371011972427\n",
      "\t\troot\t0.006205277051776648\n",
      "\t\tcrop\t0.006102713290601969\n",
      "\t\tdiffer\t0.005743740126490593\n",
      "Topic #8\n",
      "\t\tgene\t0.01599261909723282\n",
      "\t\tspeci\t0.014167975634336472\n",
      "\t\tpopul\t0.013109988532960415\n",
      "\t\tgenom\t0.012695994228124619\n",
      "\t\tsequenc\t0.00946070160716772\n",
      "\t\tinfect\t0.008080719038844109\n",
      "\t\tgenet\t0.007513392250984907\n",
      "\t\thost\t0.00673140212893486\n",
      "\t\tselect\t0.006225408520549536\n",
      "\t\tassoci\t0.006010744720697403\n",
      "Topic #9\n",
      "\t\talgorithm\t0.013020716607570648\n",
      "\t\tnetwork\t0.012980027124285698\n",
      "\t\tproblem\t0.012288312427699566\n",
      "\t\tgraph\t0.010050411336123943\n",
      "\t\tshow\t0.00876870471984148\n",
      "\t\tset\t0.008015956729650497\n",
      "\t\tnode\t0.006978384684771299\n",
      "\t\tcomput\t0.00632735900580883\n",
      "\t\tprotocol\t0.006144258193671703\n",
      "\t\tsecur\t0.005859434138983488\n",
      "Topic #10\n",
      "\t\tdevelop\t0.013643467798829079\n",
      "\t\tresearch\t0.010259264148771763\n",
      "\t\tdata\t0.008493882603943348\n",
      "\t\tsystem\t0.008453912101686\n",
      "\t\tprocess\t0.006888385396450758\n",
      "\t\tinform\t0.0065286471508443356\n",
      "\t\tprovid\t0.0062421890906989574\n",
      "\t\tapproach\t0.006075643468648195\n",
      "\t\ttechnolog\t0.005822494626045227\n",
      "\t\tinclud\t0.005742552690207958\n",
      "Topic #11\n",
      "\t\tneuron\t0.013299740850925446\n",
      "\t\tbrain\t0.011801771819591522\n",
      "\t\tactiv\t0.008886804804205894\n",
      "\t\tfunction\t0.007125678472220898\n",
      "\t\tconnect\t0.006376693490892649\n",
      "\t\ttask\t0.006376693490892649\n",
      "\t\trelat\t0.005708680488169193\n",
      "\t\tbehavior\t0.005688437260687351\n",
      "\t\tcontrol\t0.005627708975225687\n",
      "\t\tassoci\t0.0055872234515845776\n",
      "Topic #12\n",
      "\t\temiss\t0.014782719314098358\n",
      "\t\tobserv\t0.012649384327232838\n",
      "\t\tmeasur\t0.010709988884627819\n",
      "\t\tco\t0.010494500398635864\n",
      "\t\tozon\t0.008490458130836487\n",
      "\t\tatmosph\t0.008188774809241295\n",
      "\t\tsimul\t0.007951737381517887\n",
      "\t\tregion\t0.007046686485409737\n",
      "\t\tflux\t0.006917393300682306\n",
      "\t\tchang\t0.006895844358950853\n",
      "Topic #13\n",
      "\t\tdiffer\t0.013238685205578804\n",
      "\t\teffect\t0.010921742767095566\n",
      "\t\tchang\t0.009128786623477936\n",
      "\t\tobserv\t0.0077778310514986515\n",
      "\t\thowev\t0.006615795660763979\n",
      "\t\tincrea\t0.006533811800181866\n",
      "\t\tshow\t0.006412617862224579\n",
      "\t\tdynam\t0.006391230504959822\n",
      "\t\ttime\t0.005992003716528416\n",
      "\t\texperi\t0.0058957613073289394\n",
      "Topic #14\n",
      "\t\tmeasur\t0.035562947392463684\n",
      "\t\tjet\t0.011010810732841492\n",
      "\t\tpresent\t0.010694417171180248\n",
      "\t\tdata\t0.009049170650541782\n",
      "\t\tparticl\t0.008669499307870865\n",
      "\t\tcross_section\t0.008637859486043453\n",
      "\t\tevent\t0.008353105746209621\n",
      "\t\tgev\t0.008131629787385464\n",
      "\t\tfunction\t0.008099990896880627\n",
      "\t\tcompar\t0.008005072362720966\n",
      "Topic #15\n",
      "\t\tder\t0.036255236715078354\n",
      "\t\tund\t0.03270087018609047\n",
      "\t\tdie\t0.031065862625837326\n",
      "\t\tvon\t0.014929044060409069\n",
      "\t\tde\t0.014573607593774796\n",
      "\t\tein\t0.012085551396012306\n",
      "\t\tda\t0.010592718608677387\n",
      "\t\tzu\t0.010450543835759163\n",
      "\t\tim\t0.009028797969222069\n",
      "\t\tmit\t0.008389012888073921\n",
      "Topic #16\n",
      "\t\tbacteria\t0.012032577767968178\n",
      "\t\tstrain\t0.009945785626769066\n",
      "\t\tbacteri\t0.008169792592525482\n",
      "\t\tproduct\t0.006882196757942438\n",
      "\t\tinfect\t0.006704597733914852\n",
      "\t\tconcentr\t0.005949800368398428\n",
      "\t\tisol\t0.005927600432187319\n",
      "\t\tcell\t0.005727801006287336\n",
      "\t\tiron\t0.005705601070076227\n",
      "\t\tresist\t0.0056168013252317905\n",
      "Topic #17\n",
      "\t\timag\t0.02931261993944645\n",
      "\t\tseismic\t0.010663981549441814\n",
      "\t\tmap\t0.00891245249658823\n",
      "\t\tearthquak\t0.008242749609053135\n",
      "\t\tmeasur\t0.007624562829732895\n",
      "\t\tarea\t0.006594251375645399\n",
      "\t\tveloc\t0.006491219624876976\n",
      "\t\treconstruct\t0.005873032845556736\n",
      "\t\tdetect\t0.005847275257110596\n",
      "\t\tfault\t0.005641212686896324\n",
      "Topic #18\n",
      "\t\tpatient\t0.01943778246641159\n",
      "\t\tas\t0.010009071789681911\n",
      "\t\tmeasur\t0.009006858803331852\n",
      "\t\tgroup\t0.008189263753592968\n",
      "\t\tmethod\t0.007187051698565483\n",
      "\t\tcompar\t0.006831002421677113\n",
      "\t\tconclu\t0.006712319329380989\n",
      "\t\tperform\t0.0064353919588029385\n",
      "\t\ttest\t0.006079342681914568\n",
      "\t\tinterv\t0.005696919746696949\n",
      "Topic #19\n",
      "\t\tgener\t0.010388433001935482\n",
      "\t\tfunction\t0.008602085523307323\n",
      "\t\ttheori\t0.008189852349460125\n",
      "\t\tproblem\t0.007818841375410557\n",
      "\t\tapproxim\t0.007434089668095112\n",
      "\t\tsolut\t0.007406607270240784\n",
      "\t\tshow\t0.006911926902830601\n",
      "\t\tsystem\t0.00669206865131855\n",
      "\t\tcomput\t0.006293575745075941\n",
      "\t\tderiv\t0.005840118508785963\n",
      "Topic #20\n",
      "\t\tsediment\t0.009518946520984173\n",
      "\t\twater\t0.007760241627693176\n",
      "\t\tlake\t0.006364156026393175\n",
      "\t\triver\t0.0058383578434586525\n",
      "\t\tage\t0.00547573808580637\n",
      "\t\tisotop\t0.005004332400858402\n",
      "\t\tsampl\t0.004859284497797489\n",
      "\t\tcarbon\t0.004406009800732136\n",
      "\t\tsuggest\t0.004351616837084293\n",
      "\t\trecord\t0.004170306958258152\n",
      "Topic #21\n",
      "\t\tsystem\t0.021953877061605453\n",
      "\t\tcontrol\t0.011845814064145088\n",
      "\t\tperform\t0.010776531882584095\n",
      "\t\tsensor\t0.009690540842711926\n",
      "\t\tdesign\t0.008938701823353767\n",
      "\t\ttrain\t0.008270400576293468\n",
      "\t\trobot\t0.007602098397910595\n",
      "\t\ttask\t0.007084164768457413\n",
      "\t\tlearn\t0.006833551451563835\n",
      "\t\tdevic\t0.006733306217938662\n",
      "Topic #22\n",
      "\t\tsearch\t0.013253243640065193\n",
      "\t\tmass\t0.012675832025706768\n",
      "\t\tproduct\t0.012153412215411663\n",
      "\t\tevent\t0.011988437734544277\n",
      "\t\tstandard_model\t0.01196094136685133\n",
      "\t\tobserv\t0.009348842315375805\n",
      "\t\tlimit\t0.009266355074942112\n",
      "\t\tcorrespond_integr_lumino_fb\t0.008166523650288582\n",
      "\t\tdecay\t0.007369145750999451\n",
      "\t\tfinal_state\t0.006874220911413431\n",
      "Topic #23\n",
      "\t\tmateri\t0.012699729762971401\n",
      "\t\tstructur\t0.012614782899618149\n",
      "\t\tflow\t0.009032847359776497\n",
      "\t\tmechan\t0.009004532359540462\n",
      "\t\tsurfac\t0.008240008726716042\n",
      "\t\texperi\t0.006994117982685566\n",
      "\t\ttest\t0.0066118561662733555\n",
      "\t\tmeasur\t0.005776543170213699\n",
      "\t\tdeform\t0.005465070251375437\n",
      "\t\tload\t0.005238545127213001\n",
      "Topic #24\n",
      "\t\tcell\t0.02360905334353447\n",
      "\t\texpress\t0.008185448125004768\n",
      "\t\tactiv\t0.007835935801267624\n",
      "\t\tinduc\t0.007159462198615074\n",
      "\t\thuman\t0.0066972048953175545\n",
      "\t\ttissu\t0.006573184859007597\n",
      "\t\tmous\t0.006156025920063257\n",
      "\t\tincrea\t0.005208962131291628\n",
      "\t\ttreatment\t0.005073667503893375\n",
      "\t\tdevelop\t0.004825626965612173\n",
      "Topic #25\n",
      "\t\tparticl\t0.022604070603847504\n",
      "\t\taerosol\t0.018180999904870987\n",
      "\t\tcloud\t0.016471467912197113\n",
      "\t\tmeasur\t0.01565740630030632\n",
      "\t\tobserv\t0.009226311929523945\n",
      "\t\tconcentr\t0.008819281123578548\n",
      "\t\tsimul\t0.0070283436216413975\n",
      "\t\tatmosph\t0.006811260245740414\n",
      "\t\ttemperatur\t0.006322822533547878\n",
      "\t\twater\t0.00602433318272233\n",
      "Topic #26\n",
      "\t\tmass\t0.013249940238893032\n",
      "\t\tobserv\t0.013023455627262592\n",
      "\t\tgalaxi\t0.01155130285769701\n",
      "\t\tdetect\t0.008267269469797611\n",
      "\t\timag\t0.006738496012985706\n",
      "\t\torbit\t0.006172283552587032\n",
      "\t\tdisk\t0.006002419628202915\n",
      "\t\tstar\t0.005662692245095968\n",
      "\t\tplanet\t0.005322964861989021\n",
      "\t\tgroup\t0.005322964861989021\n",
      "Topic #27\n",
      "\t\tnetwork\t0.016762472689151764\n",
      "\t\turban\t0.015496870502829552\n",
      "\t\tsimul\t0.01515937689691782\n",
      "\t\tciti\t0.013162539340555668\n",
      "\t\ttraffic\t0.00987197458744049\n",
      "\t\tvehicl\t0.009646979160606861\n",
      "\t\tarea\t0.007593891583383083\n",
      "\t\tdemand\t0.007537642493844032\n",
      "\t\ttransport\t0.007481393404304981\n",
      "\t\tplan\t0.0074251447804272175\n",
      "Topic #28\n",
      "\t\tprotein\t0.021623728796839714\n",
      "\t\tfunction\t0.01056071650236845\n",
      "\t\tcell\t0.010345403105020523\n",
      "\t\tactiv\t0.008899726904928684\n",
      "\t\tcomplex\t0.007577086798846722\n",
      "\t\tregul\t0.007402785122394562\n",
      "\t\tinteract\t0.006387735716998577\n",
      "\t\tgene\t0.006254446692764759\n",
      "\t\tstructur\t0.006018626969307661\n",
      "\t\texpress\t0.00585457868874073\n",
      "Topic #29\n",
      "\t\tmethod\t0.02197272516787052\n",
      "\t\tdata\t0.02088148705661297\n",
      "\t\tapproach\t0.01326320506632328\n",
      "\t\tmeasur\t0.01066258642822504\n",
      "\t\testim\t0.00933168176561594\n",
      "\t\tperform\t0.009010428562760353\n",
      "\t\tanalysi\t0.00850050337612629\n",
      "\t\tpredict\t0.00819964800029993\n",
      "\t\tsampl\t0.007949784398078918\n",
      "\t\tdiffer\t0.007597936317324638\n"
     ]
    }
   ],
   "source": [
    "for k in range(model.k):\n",
    "    print('Topic #{}'.format(k))\n",
    "    for word, prob in model.get_topic_words(k):\n",
    "        print('\\t', word, prob, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning by optimizing log-likelihood  \n",
    "\n",
    "---\n",
    "\n",
    "Note: log-likelihood is generally not considered a good measure for topic model performance..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LDA(documents, k, min_cf=0, min_df=0, rm_top=0, alpha=0.1, eta=0.01, model_burn_in=100, \n",
    "              train_updates = 1000, train_iter = 10):\n",
    "    \n",
    "    # instantiate\n",
    "    model = tp.LDAModel(tw=tp.TermWeight.ONE, min_cf=min_cf, rm_top=rm_top, k=k)\n",
    "    \n",
    "    # add documents to model\n",
    "    for doc in documents: model.add_doc(doc)\n",
    "    \n",
    "    # training**\n",
    "    model.burn_in = model_burn_in\n",
    "    # initialising \n",
    "    model.train(iter=0)\n",
    "    print('Num docs:', len(model.docs), ', Vocab size:', len(model.used_vocabs), ', Num words:', model.num_words)\n",
    "    print('Removed top words:', model.removed_top_words)\n",
    "    print('Training...', file=sys.stderr, flush=True)\n",
    "    # actual training \n",
    "    time = []\n",
    "    LLs = []\n",
    "    for i in range(0, train_updates, train_iter):\n",
    "        model.train(train_iter)\n",
    "        if i%100==0:print('Iteration: {}'.format(i))\n",
    "        time.append(i)\n",
    "        LLs.append(model.ll_per_word)\n",
    "    \n",
    "    return model, LLs, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num docs: 20494 , Vocab size: 69312 , Num words: 2141113\n",
      "Removed top words: []\n",
      "Iteration: 0\n",
      "Iteration: 100\n",
      "Iteration: 200\n",
      "Iteration: 300\n",
      "Iteration: 400\n",
      "Iteration: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num docs: 20494 , Vocab size: 69312 , Num words: 2141113\n",
      "Removed top words: []\n",
      "Iteration: 0\n",
      "Iteration: 100\n",
      "Iteration: 200\n",
      "Iteration: 300\n",
      "Iteration: 400\n",
      "Iteration: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num docs: 20494 , Vocab size: 69312 , Num words: 2141113\n",
      "Removed top words: []\n",
      "Iteration: 0\n",
      "Iteration: 100\n",
      "Iteration: 200\n",
      "Iteration: 300\n",
      "Iteration: 400\n",
      "Iteration: 500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple loop for minimizing perplexity on the training set\n",
    "\n",
    "topics = [10,20,30]\n",
    "perplexity_score = np.array([])\n",
    "for k in topics:\n",
    "    print(\"Training for \"+str(k)+\" topics\")\n",
    "    model, LLs, time = train_LDA(cleaned, k = k, train_updates = 600)\n",
    "    perplexity_score = np.append(perplexity_score, model.perplexity)\n",
    "    print(\"Perplexity = \"+str(model.perplexity))\n",
    "\n",
    "topics[np.argmin(perplexity_score)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Number of topics')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(topics,perplexity_score)\n",
    "plt.ylabel('Perplexity')\n",
    "plt.xlabel('Number of topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20494\n"
     ]
    }
   ],
   "source": [
    "# split data in train and test set\n",
    "\n",
    "print(len(cleaned))\n",
    "train_size = int(0.8*len(cleaned))\n",
    "\n",
    "random.shuffle(cleaned)\n",
    "train_docs = cleaned[0:train_size]\n",
    "test_docs = cleaned[train_size:]\n",
    "\n",
    "assert len(train_docs) + len(test_docs) == len(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_LL(test_docs, model):\n",
    "    \n",
    "    # make a list of documents of type required by tp\n",
    "    test_set = []\n",
    "    for doc in test_docs:\n",
    "        test_set.append(model.make_doc(doc))\n",
    "    \n",
    "    # return topic distribution and log-likelihood of new documents\n",
    "    topic_dist, likelihood = model.infer(test_set)\n",
    "    \n",
    "    # use mean log-likelihood as performance measure\n",
    "    return np.mean(likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num docs: 16395 , Vocab size: 62919 , Num words: 1713454\n",
      "Removed top words: []\n",
      "Iteration: 0\n",
      "Iteration: 100\n",
      "Iteration: 200\n",
      "Iteration: 300\n",
      "Iteration: 400\n",
      "Iteration: 500\n",
      "Iteration: 600\n",
      "Iteration: 700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num docs: 16395 , Vocab size: 62919 , Num words: 1713454\n",
      "Removed top words: []\n",
      "Iteration: 0\n",
      "Iteration: 100\n",
      "Iteration: 200\n",
      "Iteration: 300\n",
      "Iteration: 400\n",
      "Iteration: 500\n",
      "Iteration: 600\n",
      "Iteration: 700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num docs: 16395 , Vocab size: 62919 , Num words: 1713454\n",
      "Removed top words: []\n",
      "Iteration: 0\n",
      "Iteration: 100\n",
      "Iteration: 200\n",
      "Iteration: 300\n",
      "Iteration: 400\n",
      "Iteration: 500\n",
      "Iteration: 600\n",
      "Iteration: 700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loop for maximizing mean likelihood of test set\n",
    "\n",
    "topics = [10,20,30]\n",
    "log_likelihoods = np.array([])\n",
    "for k in topics:\n",
    "    print(\"Training for \"+str(k)+\" topics\")\n",
    "    model, LLs, time = train_LDA(train_docs, k = k, train_updates = 800)\n",
    "    log_likelihoods = np.append(log_likelihoods, get_test_LL(test_docs, model))\n",
    "    print(\"Log likelihood = \"+str(get_test_LL(test_docs, model)))\n",
    "\n",
    "topics[np.argmax(log_likelihoods)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(topics,log_likelihoods)\n",
    "plt.ylabel('Likelihood')\n",
    "plt.xlabel('Number of topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-882.05810779 -882.89588814 -879.71187388]\n"
     ]
    }
   ],
   "source": [
    "print(likelihoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic coherence \n",
    "\n",
    "---\n",
    "\n",
    "Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_word = lambda x: x[0] # get_topic_words returns both the word and its probability in the topic\n",
    "topics = [[extract_word(tw) for tw in model.get_topic_words(k, 20)] for k in range(1,num_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['infect',\n",
       " 'strain',\n",
       " 'bacteria',\n",
       " 'host',\n",
       " 'resist',\n",
       " 'bacteri',\n",
       " 'pathogen',\n",
       " 'cell',\n",
       " 'use',\n",
       " 'viru',\n",
       " 'vaccin',\n",
       " 'isol',\n",
       " 'antibiot',\n",
       " 'product',\n",
       " 'iron',\n",
       " 'viral',\n",
       " 'microbiota',\n",
       " 'specif',\n",
       " 'diseas',\n",
       " 'phage']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(cleaned)\n",
    "BoW_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in cleaned] # bag of words corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 2),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 1),\n",
       " (13, 1),\n",
       " (14, 1),\n",
       " (15, 1),\n",
       " (16, 2),\n",
       " (17, 1),\n",
       " (18, 1),\n",
       " (19, 1),\n",
       " (20, 1)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BoW_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the coherence preprocessing operations into two functions\n",
    "extract_word = lambda x: x[0] # get_topic_words returns both the word and its probability in the topic\n",
    "\n",
    "def get_topics(model, num_topics):\n",
    "    return [[extract_word(tw) for tw in model.get_topic_words(k, 20)] for k in range(1,num_topics)]\n",
    "\n",
    "def get_corpus(dictionary, texts):\n",
    "    return [dictionary.doc2bow(doc, allow_update=True) for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49646788489478966"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# topics (list of list of str, optional) – List of tokenized topics\n",
    "# texts (list of list of str, optional) – Tokenized texts, needed for coherence models that use sliding window based (i.e. coherence=`c_something`) probability estimator .\n",
    "# corpus (iterable of list of (int, number), optional) – Corpus in BoW format.\n",
    "# dictionary (Dictionary, optional) – Gensim dictionary mapping of id word to create corpus. If model.id2word is present, this is not needed. If both are provided, passed dictionary will be used.\n",
    "# window_size (int, optional) – Is the size of the window to be used for coherence measures using boolean sliding window as their probability estimator. For ‘u_mass’ this doesn’t matter. If None - the default window sizes are used which are: ‘c_v’ - 110, ‘c_uci’ - 10, ‘c_npmi’ - 10.\n",
    "# coherence ({'u_mass', 'c_v', 'c_uci', 'c_npmi'}, optional) – Coherence measure to be used. Fastest method - ‘u_mass’, ‘c_uci’ also known as c_pmi. For ‘u_mass’ corpus should be provided, if texts is provided, it will be converted to corpus using the dictionary. For ‘c_v’, ‘c_uci’ and ‘c_npmi’ texts should be provided (corpus isn’t needed)\n",
    "# topn (int, optional) – Integer corresponding to the number of top words to be extracted from each topic.\n",
    "cm = CoherenceModel(topics=topics, corpus=BoW_corpus, dictionary=dictionary, texts=cleaned, coherence='c_v')\n",
    "coherence = cm.get_coherence()  # get coherence value\n",
    "coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: there are different types of coherence measures, we need to decide which to use!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Complete grid search \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20494\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data in train, test and validation set\n",
    "print(len(cleaned))\n",
    "train_size = int(0.6*len(cleaned)) #60% for training\n",
    "test_size = int(0.2*len(cleaned)) #20% for testing and 10% for validating\n",
    "\n",
    "random.shuffle(cleaned)\n",
    "\n",
    "train_docs = cleaned[0:train_size]\n",
    "test_docs = cleaned[train_size:train_size+test_size]\n",
    "val_docs = cleaned[train_size+test_size:]\n",
    "\n",
    "len(train_docs) + len(test_docs) + len(val_docs) == len(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10 topics -----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num docs: 12296 , Vocab size: 56507 , Num words: 1282721\n",
      "Removed top words: []\n",
      "Iteration: 0\n",
      "Iteration: 100\n",
      "Iteration: 200\n",
      "Iteration: 300\n",
      "Iteration: 400\n",
      "Log likelihood = -879.3397307918385\n",
      "Perplexity = 6832.771771754188\n",
      "Coherence = -1.9234681155732096\n",
      "Time elapsed: 63.8 s\n",
      "Training for 20 topics -----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num docs: 12296 , Vocab size: 56507 , Num words: 1282721\n",
      "Removed top words: []\n",
      "Iteration: 0\n",
      "Iteration: 100\n",
      "Iteration: 200\n",
      "Iteration: 300\n",
      "Iteration: 400\n",
      "Log likelihood = -883.6947845289072\n",
      "Perplexity = 6884.56302076371\n",
      "Coherence = -2.3103442700798644\n",
      "Time elapsed: 104.3 s\n",
      "Training for 30 topics -----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num docs: 12296 , Vocab size: 56507 , Num words: 1282721\n",
      "Removed top words: []\n"
     ]
    }
   ],
   "source": [
    "# Grid search of best topic number (this needs to run for a while)\n",
    "# We collect LL, perplexity and coherence scores, saving them in variables \n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "dictionary = Dictionary(cleaned) # this we need later for gensim \n",
    "ks = [10,20,30,50,70,100,1000] # we have 16k documents, how many topics should we identify?\n",
    "log_likelihoods = []\n",
    "perplexities = []\n",
    "coherences = []\n",
    "for k in ks:\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    print(\"Training for \"+str(k)+\" topics -----------------------------\")\n",
    "    model, LLs, _ = train_LDA(train_docs, k, min_cf=0, \n",
    "                                 min_df=0, rm_top=0, alpha=0.1, \n",
    "                                 eta=0.01, model_burn_in=100, \n",
    "                                 train_updates = 500, train_iter = 10)\n",
    "    \n",
    "    # LL\n",
    "    log_likelihoods += [get_test_LL(test_docs, model)]\n",
    "    print(\"Log likelihood = \"+str(get_test_LL(test_docs, model)))\n",
    "    \n",
    "    ## PP\n",
    "    # TODO: obtain perplexity on the test set\n",
    "    perplexities += [model.perplexity]\n",
    "    print(\"Perplexity = \"+str(model.perplexity))\n",
    "    \n",
    "    ## CHR\n",
    "    topics = get_topics(model, k)\n",
    "    corpus = get_corpus(dictionary, test_docs)\n",
    "    cm = CoherenceModel(topics=topics, corpus=corpus, dictionary=dictionary, texts=train_docs, coherence='u_mass')\n",
    "    coherence = cm.get_coherence()  # get coherence value\n",
    "    coherences += [coherence]\n",
    "    print(\"Coherence = \"+str(coherence))\n",
    "    \n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"Time elapsed: \"+ str(round(end - start,1))+\" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the results\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "axs[0].plot(topics,log_likelihoods)\n",
    "axs[1].plot(topics,perplexities)\n",
    "axs[2].plot(topics,coherences)\n",
    "axs[0].set_title(\"Test log-likelihood\")\n",
    "axs[1].set_title(\"Train perplexity\")\n",
    "axs[2].set_title(\"Train coherence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the validation set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nice visualisation \n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3.7 install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: you need to have trained a model to use the following code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_term_dists = np.stack([model.get_topic_word_dist(k) for k in range(model.k)])\n",
    "doc_topic_dists = np.stack([doc.get_topic_dist() for doc in model.docs])\n",
    "doc_lengths = np.array([len(doc.words) for doc in model.docs])\n",
    "vocab = list(model.used_vocabs)\n",
    "term_frequency = model.used_vocab_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesn't work ...\n",
    "pyLDAvis.prepare(\n",
    "    topic_term_dists, \n",
    "    doc_topic_dists, \n",
    "    doc_lengths, \n",
    "    vocab, \n",
    "    term_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next up\n",
    "\n",
    "- experimenting with CTM\n",
    "- experimenting with Pachinko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
