{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, RegexpTokenizer,PunktSentenceTokenizer, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "abstracts = pd.read_csv(\"abstracts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>The goal of this note is to introduce new clas...</td>\n",
       "      <td>188444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>We will review a Lemma published by Ran Raz in...</td>\n",
       "      <td>188623.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>China’s growing influence in Europe has the po...</td>\n",
       "      <td>346708.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Nowhere is China's Belt and Road Initiative (B...</td>\n",
       "      <td>346709.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Passenger transit modes typical of the urban s...</td>\n",
       "      <td>187461.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract        id\n",
       "0  The goal of this note is to introduce new clas...  188444.0\n",
       "1  We will review a Lemma published by Ran Raz in...  188623.0\n",
       "2  China’s growing influence in Europe has the po...  346708.0\n",
       "3  Nowhere is China's Belt and Road Initiative (B...  346709.0\n",
       "4  Passenger transit modes typical of the urban s...  187461.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_list = list(abstracts['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised = []\n",
    "for abstract in abs_list:\n",
    "        raw = abstract\n",
    "        \n",
    "        tokens = gensim.utils.simple_preprocess(str(raw), deacc=True)\n",
    "        \n",
    "        tokenised.append(tokens)\n",
    "\n",
    "#         #Make everything lower case\n",
    "#         raw = raw.lower()\n",
    "#         #print(raw)\n",
    "\n",
    "#         #Tokenise words\n",
    "#         tokens=[]\n",
    "#         for words in raw:\n",
    "#             tokens.append(nltk.word_tokenize(sentence))\n",
    "#         #print('Tokenised sentences',tokens)\n",
    "\n",
    "#         #Remove Stop Words. Remember add stopwords that may be relevant\n",
    "#         int_words=[]\n",
    "#         for j in range(0,len(tokens)):\n",
    "#             int_words.append([ i for i in tokens[j] if not i in stop_words])\n",
    "#         #print ('Removed stop words',words)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "#stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "cleaned = [[word for word in doc if word not in stop_words] for doc in tokenised]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "corpus = [[lemmatiser.lemmatize(word_stemmer.stem(word)) for word in doc] for doc in cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = flatten(cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq = [wordlist.count(p) for p in wordlist]\n",
    "dictionary_count=dict(list(zip(wordlist,wordfreq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(tokenised, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tomotopy as tp\n",
    "\n",
    "def lda_example(input_file, save_path):\n",
    "    mdl = tp.LDAModel(tw=tp.TermWeight.ONE, min_cf=3, rm_top=5, k=20)\n",
    "    for n, line in enumerate(open(input_file, encoding='utf-8')):\n",
    "        ch = line.strip().split()\n",
    "        mdl.add_doc(ch)\n",
    "    mdl.burn_in = 100\n",
    "    mdl.train(0)\n",
    "    print('Num docs:', len(mdl.docs), ', Vocab size:', len(mdl.used_vocabs), ', Num words:', mdl.num_words)\n",
    "    print('Removed top words:', mdl.removed_top_words)\n",
    "    print('Training...', file=sys.stderr, flush=True)\n",
    "    for i in range(0, 1000, 10):\n",
    "        mdl.train(10)\n",
    "        print('Iteration: {}\\tLog-likelihood: {}'.format(i, mdl.ll_per_word))\n",
    "    \n",
    "    mdl.summary()\n",
    "    print('Saving...', file=sys.stderr, flush=True)\n",
    "    mdl.save(save_path, True)\n",
    "\n",
    "    for k in range(mdl.k):\n",
    "        print('Topic #{}'.format(k))\n",
    "        for word, prob in mdl.get_topic_words(k):\n",
    "            print('\\t', word, prob, sep='\\t')\n",
    "\n",
    "# You can get the sample data file 'enwiki-stemmed-1000.txt'\n",
    "# at https://drive.google.com/file/d/18OpNijd4iwPyYZ2O7pQoPyeTAKEXa71J/view?usp=sharing\n",
    "\n",
    "print('Running LDA')\n",
    "lda_example('enwiki-stemmed-1000.txt', 'test.lda.bin')import sys\n",
    "import tomotopy as tp\n",
    "\n",
    "def lda_example(input_file, save_path):\n",
    "    mdl = tp.LDAModel(tw=tp.TermWeight.ONE, min_cf=3, rm_top=5, k=20)\n",
    "    for n, line in enumerate(open(input_file, encoding='utf-8')):\n",
    "        ch = line.strip().split()\n",
    "        mdl.add_doc(ch)\n",
    "    mdl.burn_in = 100\n",
    "    mdl.train(0)\n",
    "    print('Num docs:', len(mdl.docs), ', Vocab size:', len(mdl.used_vocabs), ', Num words:', mdl.num_words)\n",
    "    print('Removed top words:', mdl.removed_top_words)\n",
    "    print('Training...', file=sys.stderr, flush=True)\n",
    "    for i in range(0, 1000, 10):\n",
    "        mdl.train(10)\n",
    "        print('Iteration: {}\\tLog-likelihood: {}'.format(i, mdl.ll_per_word))\n",
    "    \n",
    "    mdl.summary()\n",
    "    print('Saving...', file=sys.stderr, flush=True)\n",
    "    mdl.save(save_path, True)\n",
    "\n",
    "    for k in range(mdl.k):\n",
    "        print('Topic #{}'.format(k))\n",
    "        for word, prob in mdl.get_topic_words(k):\n",
    "            print('\\t', word, prob, sep='\\t')\n",
    "\n",
    "# You can get the sample data file 'enwiki-stemmed-1000.txt'\n",
    "# at https://drive.google.com/file/d/18OpNijd4iwPyYZ2O7pQoPyeTAKEXa71J/view?usp=sharing\n",
    "\n",
    "print('Running LDA')\n",
    "lda_example('enwiki-stemmed-1000.txt', 'test.lda.bin')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
