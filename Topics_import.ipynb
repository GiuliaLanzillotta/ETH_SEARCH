{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy as tp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import nltk\n",
    "from nltk import word_tokenize, RegexpTokenizer,PunktSentenceTokenizer, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the model and the abstracts file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tp.LDAModel.load(\"lda_model150.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"abstracts_eng.csv\")\n",
    "collection = list(data['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The goal of this note is to introduce new clas...</td>\n",
       "      <td>188444.0</td>\n",
       "      <td>Asymptotic versions for operators and operator...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We will review a Lemma published by Ran Raz in...</td>\n",
       "      <td>188623.0</td>\n",
       "      <td>Some remarks on a lemma of Ran Raz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>China’s growing influence in Europe has the po...</td>\n",
       "      <td>346708.0</td>\n",
       "      <td>China as a Stress Test for Europe’s Coherence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract        id  \\\n",
       "0  The goal of this note is to introduce new clas...  188444.0   \n",
       "1  We will review a Lemma published by Ran Raz in...  188623.0   \n",
       "2  China’s growing influence in Europe has the po...  346708.0   \n",
       "\n",
       "                                               title  \n",
       "0  Asymptotic versions for operators and operator...  \n",
       "1                 Some remarks on a lemma of Ran Raz  \n",
       "2      China as a Stress Test for Europe’s Coherence  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example collection\n",
    "toy_collection = ['hello hi I am a Data Science STUDENT here with Giulia and Andreas!U.S.A, United States of AMerica dixterochlomaterine hreoihso my dog ran away yesterday?! I will fly, to space tomorrow...',\n",
    "                  'space tomorrow space tomorrow if the dog ran away fly to space',\n",
    "                  ' fly to space  with my dog ran away']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pre-process the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-processing comprises 2 steps: \n",
    "- tokenisation, stopwords-removal, stemming (optional), lemmatizing(optional)\n",
    "- ngrams modeling (bigrams & trigrams supported)\n",
    "\n",
    "Note: the first step is only dependent on a single input document, while ngrams modeling depends on the whole collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "word_stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "def normalisation(document, stemming = True, lemmatising = True, min_word_len = 3):\n",
    "    tokens = gensim.utils.simple_preprocess(str(document), deacc=True, max_len = sys.maxsize)\n",
    "    cleaned = [word for word in tokens if word not in stop_words]\n",
    "    if stemming:\n",
    "        cleaned = [word_stemmer.stem(word) for word in cleaned]\n",
    "    if lemmatising:\n",
    "        cleaned = [lemmatiser.lemmatize(word) for word in cleaned]\n",
    "    cleaned = [word for word in cleaned if (min_word_len<=len(word))]\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "def ngram(cleaned_docs, do_trigram = True, min_count_bigram = 5, threshold_bigram = 50, min_count_trigram = 5, threshold_trigram=50):\n",
    "    #Bigrams\n",
    "    bigram = gensim.models.Phrases(cleaned_docs, min_count= min_count_bigram, threshold=threshold_bigram) \n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    output = [bigram_mod[doc] for doc in cleaned_docs]\n",
    "    #Trigrams\n",
    "    if do_trigram:\n",
    "        trigram = gensim.models.Phrases(output, min_count= min_count_trigram, threshold=threshold_trigram) \n",
    "        trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "        output = [trigram_mod[doc] for doc in output]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try out on toy collection\n",
    "toy_normalised = [normalisation(text, lemmatising = True, stemming = True, min_word_len = -1) for text in toy_collection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on true collection\n",
    "normalised = [normalisation(doc) for doc in collection]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract topics for each document \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_top_topics(document, model, min_score=0.8):\n",
    "    \"\"\" \n",
    "    Extracting top n topics for each document. \n",
    "    Selects the n most likely topics whose p(topic|document) sum to min_score.\n",
    "    \"\"\"\n",
    "    # inserting the document in the model\n",
    "    new_doc = model.make_doc(document)\n",
    "    _,_ = model.infer(new_doc)\n",
    "    # ordering from most probable topic to least one \n",
    "    dist = new_doc.get_topic_dist()\n",
    "    indices = np.flip(np.argsort(dist))\n",
    "    score = 0\n",
    "    indices_kept = []\n",
    "    for index in indices:\n",
    "        if score > min_score: break\n",
    "        score += dist[index]\n",
    "        indices_kept.append(index)\n",
    "    return indices_kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[61, 41, 6]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying on toy collection\n",
    "get_top_topics(toy_normalised[0], model, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting topics for the original collection\n",
    "docs2topics = [get_top_topics(doc, model) for doc in normalised]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enriching original dataframe with the topics list x document \n",
    "# Note: the following command will only work if the order of documents is the same for the collection and the docs-topics list\n",
    "enriched = data\n",
    "enriched[\"topics\"] = docs2topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploding the dataframe (one row for each document-topic pair)\n",
    "enriched=enriched.explode(\"topics\")\n",
    "enriched.columns = [\"abstract\",\"publication_id\",\"publication_title\",\"topic_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to .csv \n",
    "file_name=\"abstract+topic.csv\"\n",
    "enriched.to_csv(file_name,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Extract words for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(topic, model, min_score=0.8):\n",
    "    \"\"\"\n",
    "    Extracting top n words for each document. \n",
    "    Selects the n most likely words whose p(word|topic) sum to min_score.\n",
    "    \"\"\"\n",
    "    dist = model.get_topic_word_dist(topic)\n",
    "    indices = np.flip(np.argsort(dist))\n",
    "    score = 0\n",
    "    word_kept = []\n",
    "    for index in indices:\n",
    "        if score > min_score: break\n",
    "        score += dist[index]\n",
    "        word_kept.append(model.used_vocabs[index])\n",
    "    return word_kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tc',\n",
       " 'spectrum',\n",
       " 'peak',\n",
       " 'ce',\n",
       " 'ab',\n",
       " 'spectral',\n",
       " 'band',\n",
       " 'deriv',\n",
       " 'sa',\n",
       " 'transit',\n",
       " 'experi',\n",
       " 'signal',\n",
       " 'inclus',\n",
       " 'cen',\n",
       " 'atc',\n",
       " 'pin',\n",
       " 'nm',\n",
       " 'specif_heat',\n",
       " 'temperatur',\n",
       " 'irradi',\n",
       " 'muon_spin',\n",
       " 'ihm',\n",
       " 'baryon',\n",
       " 'compound',\n",
       " 'mass',\n",
       " 'reconstruct',\n",
       " 'μsr',\n",
       " 'fit',\n",
       " 'co',\n",
       " 'temperatur_depend',\n",
       " 'imprecis',\n",
       " 'ft',\n",
       " 'eno',\n",
       " 'cef',\n",
       " 'elast',\n",
       " 'seri',\n",
       " 'heat_rate',\n",
       " 'nu',\n",
       " 'site_specif',\n",
       " 'ga',\n",
       " 'compon',\n",
       " 'myo_ip',\n",
       " 'iaa',\n",
       " 'chlamydia',\n",
       " 'critic_temperatur',\n",
       " 'amplif',\n",
       " 'zero',\n",
       " 'background',\n",
       " 'channel',\n",
       " 'crossov',\n",
       " 'epr',\n",
       " 'gamma',\n",
       " 'axi',\n",
       " 'liquid_water',\n",
       " 'superfluid_densiti',\n",
       " 'cross_section',\n",
       " 'ion',\n",
       " 'ceo_nm',\n",
       " 'rotat_relax',\n",
       " 'broad',\n",
       " 'employ',\n",
       " 'base_superconductor',\n",
       " 'famili',\n",
       " 'lat',\n",
       " 'one_nucleu',\n",
       " 'high_resolut',\n",
       " 'iodin_intak',\n",
       " 'axl',\n",
       " 'epidot',\n",
       " 'anthropogen_ce',\n",
       " 'mu_sr',\n",
       " 'ensembl',\n",
       " 'fd',\n",
       " 'superconduct_gap',\n",
       " 'hyscor',\n",
       " 'soc',\n",
       " 'hadron',\n",
       " 'ptfe',\n",
       " 'amorph_ice',\n",
       " 'calcit_wedg',\n",
       " 'site_amplif',\n",
       " 'sewag_sludg',\n",
       " 'anisotrop',\n",
       " 'wavenumb',\n",
       " 'bedrock',\n",
       " 'turn',\n",
       " 'penetr_depth',\n",
       " 'sdf',\n",
       " 'maldi_tof_mass',\n",
       " 'σh',\n",
       " 'pseudo',\n",
       " 'transit_temperatur',\n",
       " 'singular',\n",
       " 'aux_iaa',\n",
       " 'thyroid_cancer',\n",
       " 'pariti_doubl',\n",
       " 'superconduct_transit',\n",
       " 'evid',\n",
       " 'gaussian',\n",
       " 'heavier',\n",
       " 'beta',\n",
       " 'host',\n",
       " 'underli',\n",
       " 'step',\n",
       " 'nozzl_exit',\n",
       " 'sia',\n",
       " 'uh',\n",
       " 'pedv',\n",
       " 'transvers_field',\n",
       " 'ain',\n",
       " 'method_appli',\n",
       " 'kbar',\n",
       " 'hyperfin_coupl',\n",
       " 'lorentzian',\n",
       " 'reson',\n",
       " 'deconvolut',\n",
       " 'auxin',\n",
       " 'zip',\n",
       " 'tent',\n",
       " 'restor',\n",
       " 'tf',\n",
       " 'hr',\n",
       " 'ir',\n",
       " 'isotrop',\n",
       " 'slight',\n",
       " 'spo',\n",
       " 'spectromet',\n",
       " 'lineshap',\n",
       " 'slc',\n",
       " 'quark',\n",
       " 'aberr',\n",
       " 'nucleus',\n",
       " 'scatter_cross_section',\n",
       " 'nbc',\n",
       " 'superconduct_state',\n",
       " 'ndfeaso',\n",
       " 'lc_hrm',\n",
       " 'bex',\n",
       " 'peak_height',\n",
       " 'incl',\n",
       " 'pariti',\n",
       " 'electron_scatter',\n",
       " 'detect_nmr',\n",
       " 'scenario',\n",
       " 'fast_neutron',\n",
       " 'phenomenolog',\n",
       " 'occult',\n",
       " 'nf',\n",
       " 'swiss_hazard',\n",
       " 'smart',\n",
       " 'raman_spectrum',\n",
       " 'elig',\n",
       " 'tem',\n",
       " 'eseem',\n",
       " 'ent',\n",
       " 'scftir',\n",
       " 'chiral_symmetri',\n",
       " 'resolv',\n",
       " 'qsl',\n",
       " 'heavi_ion',\n",
       " 'rebe',\n",
       " 'upper',\n",
       " 'delafossit',\n",
       " 'multigap',\n",
       " 'novel_method',\n",
       " 'thyroid',\n",
       " 'deshler',\n",
       " 'zz',\n",
       " 'refeaso',\n",
       " 'within_framework',\n",
       " 'neg_pariti',\n",
       " 'elabor',\n",
       " 'cra',\n",
       " 'evidenc',\n",
       " 'groundstat',\n",
       " 'lett',\n",
       " 'antigorit',\n",
       " 'noesi',\n",
       " 'previous_report',\n",
       " 'hard',\n",
       " 'thfeasn',\n",
       " 'jc',\n",
       " 'bodi',\n",
       " 'indirect',\n",
       " 'kik',\n",
       " 'quark_gluon_plasma',\n",
       " 'collat',\n",
       " 'dual',\n",
       " 'pcnc',\n",
       " 'anaplast',\n",
       " 'unperturb',\n",
       " 'deconfin',\n",
       " 'fastsum',\n",
       " 'jd',\n",
       " 'ww',\n",
       " 'deuterium',\n",
       " 'sizabl',\n",
       " 'encas',\n",
       " 'spectral_rang',\n",
       " 'uwb',\n",
       " 'vimf',\n",
       " 'ce_ce',\n",
       " 'pwd',\n",
       " 'tac',\n",
       " 'jph',\n",
       " 'cbl',\n",
       " 'degener',\n",
       " 'lyra',\n",
       " 'non_uniform',\n",
       " 'noe',\n",
       " 'ξab',\n",
       " 'depend_upon',\n",
       " 'temperatur_tc',\n",
       " 'qec',\n",
       " 'srpta',\n",
       " 'emerg',\n",
       " 'current_avail',\n",
       " 'cheesi',\n",
       " 'intrins',\n",
       " 'hta',\n",
       " 'quantif',\n",
       " 'posit_neg',\n",
       " 'equat_state',\n",
       " 'sourc_uncertainti',\n",
       " 'tupl',\n",
       " 'delta',\n",
       " 'kovilakam',\n",
       " 'cncbl',\n",
       " 'follicular',\n",
       " 'lizardit',\n",
       " 'need_improv',\n",
       " 'qqtof',\n",
       " 'nto',\n",
       " 'opc',\n",
       " 'nontarget',\n",
       " 'continuum',\n",
       " 'japanes',\n",
       " 'salicyl',\n",
       " 'mix',\n",
       " 'metadata',\n",
       " 'provid_meaning',\n",
       " 'disc',\n",
       " 'tighten',\n",
       " 'endocrin',\n",
       " 'full_spectrum',\n",
       " 'quark_mass',\n",
       " 'fourier',\n",
       " 'debat_regard',\n",
       " 'reticul',\n",
       " 'wedg',\n",
       " 'superimpos',\n",
       " 'hydrolys',\n",
       " 'fourier_domain',\n",
       " 'zf',\n",
       " 'thu_facilit',\n",
       " 'volt',\n",
       " 'empir_analysi',\n",
       " 'london',\n",
       " 'torqu',\n",
       " 'tropic_cyclon',\n",
       " 'antimicrobi_resist',\n",
       " 'homologu',\n",
       " 'critic_point',\n",
       " 'δs',\n",
       " 'uniform',\n",
       " 'controversi',\n",
       " 'liquid_ice',\n",
       " 'er',\n",
       " 'molecular_fingerprint',\n",
       " 'sever_hour',\n",
       " 'scale_linearli',\n",
       " 'densest',\n",
       " 'session',\n",
       " 'hyperon',\n",
       " 'occupi',\n",
       " 'heat_capac',\n",
       " 'ni',\n",
       " 'pre_process',\n",
       " 'present',\n",
       " 'mod',\n",
       " 'yba_cu',\n",
       " 'isospin',\n",
       " 'wt',\n",
       " 'devoid',\n",
       " 'cobalamin',\n",
       " 'nucleon',\n",
       " 'repres_swiss',\n",
       " 'doppler',\n",
       " 'clear_cut',\n",
       " 'macroscop',\n",
       " 'sampl',\n",
       " 'serv_proxi',\n",
       " 'collis_energi',\n",
       " 'provid_compel_evid',\n",
       " 'particular_interest',\n",
       " 'ptc',\n",
       " 'current_practic',\n",
       " 'half_live',\n",
       " 'improv_qualiti',\n",
       " 'remain_elus']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying it out\n",
    "get_top_words(2, model, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics2words = [get_top_words(i) for i in range(num_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new topic dataframe\n",
    "topics_df = pd.DataFrame(\"TopicID\":range(num_topics),\"TopicWords\":topics2words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to .csv \n",
    "file_name=\"topics.csv\"\n",
    "topics_df.to_csv(file_name,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to Neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic nodes: \n",
    "- TopicID (long)\n",
    "- Words (list(str)) \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "        #Defining the topic nodes\n",
    "        CREATE CONSTRAINT ON (c:Topic) ASSERT c.ID IS UNIQUE;\n",
    "        \n",
    "        #Loading the topic nodes \n",
    "        LOAD CSV WITH HEADERS FROM \"file:///topics.csv\" AS line\n",
    "        WITH line where line.TopicID IS NOT NULL\n",
    "        MERGE (t: Topic {ID: line.TopicID})\n",
    "        SET t.words= line.TopicWords);\n",
    "        \n",
    "        #Loading document<->topic relationships\n",
    "        LOAD CSV WITH HEADERS FROM \"file:///abstract+topic.csv\" AS line\n",
    "        MATCH (p:Publication {id:line.publication_id}),\n",
    "               (t:Topic {code:line.topic_id})\n",
    "        MERGE (p)-[:IS_ABOUT]->(t)\n",
    "        MERGE (t)-[:IS_IN]->(p);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
