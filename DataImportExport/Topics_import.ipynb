{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy as tp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import nltk\n",
    "import pickle\n",
    "import random\n",
    "from nltk import word_tokenize, RegexpTokenizer,PunktSentenceTokenizer, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the model and the abstracts file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "names = ['LDAbatch1.bin','LDAbatch2.bin','LDAbatch3.bin','LDAbatch4.bin']\n",
    "for name in names:\n",
    "    models.append(tp.LDAModel.load(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised = pickle.load(open( \"collection_cleaned.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important: Shuffle with same seed as in Streaming LDA notebook to align collection with models\n",
    "SEED = 11\n",
    "random.seed(SEED)\n",
    "random.shuffle(normalised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract topics for each document \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_top_topics(document, model, min_score=0.8):\n",
    "    \"\"\" \n",
    "    Extracting top n topics for each document. \n",
    "    Selects the n most likely topics whose p(topic|document) sum to min_score.\n",
    "    \"\"\"\n",
    "    # inserting the document in the model\n",
    "    new_doc = model.make_doc(document)\n",
    "    _,_ = model.infer(new_doc)\n",
    "    # ordering from most probable topic to least one \n",
    "    dist = new_doc.get_topic_dist()\n",
    "    indices = np.flip(np.argsort(dist))\n",
    "    score = 0\n",
    "    indices_kept = []\n",
    "    probs_kept = []\n",
    "    for index in indices:\n",
    "        if score > min_score: break\n",
    "        score += dist[index]\n",
    "        indices_kept.append(index)\n",
    "        probs_kept.append(dist[index])\n",
    "    return list(zip(indices_kept, probs_kept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs2topics = []\n",
    "num_batches = 4\n",
    "batches = np.array_split(normalised, num_batches)\n",
    "\n",
    "for i,model in enumerate(models):\n",
    "    print(\"Working on batch \"+str(i))\n",
    "    batch = batches[i].tolist()\n",
    "    docs2topics.append([get_top_topics(doc, model) for doc in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs2topics_flat = [item for sublist in docs2topics for item in sublist]\n",
    "len(docs2topics_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_indices(batches):\n",
    "    res = []\n",
    "    for i,batch in enumerate(batches):\n",
    "        res += [i]*len(batch)\n",
    "    return res\n",
    "batch_indices = get_batch_indices(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_indices[1000])\n",
    "print(batch_indices[6000])\n",
    "print(batch_indices[11000])\n",
    "print(batch_indices[16000])\n",
    "print(len(batch_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"abstracts_eng.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before merging the two files we need to match them \n",
    "# Here we're going to build the list of indices of the \"normalised\" collection with respect to \n",
    "# the \"data\" collection \n",
    "random.seed(SEED)\n",
    "indices = list(range(0,len(normalised)))\n",
    "random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the output from the two cells below correspond to the same publication!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[indices[0]][\"abstract\"][0:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(normalised[0][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched = data.iloc[indices]\n",
    "enriched[\"topics\"] = docs2topics_flat\n",
    "enriched[\"batchID\"] = batch_indices\n",
    "enriched.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploding the dataframe (to get one row for each document-topic pair)\n",
    "enriched=enriched.explode(\"topics\")\n",
    "enriched.columns = [\"abstract\",\"publication_id\",\"publication_title\",\"topic\",\"batch_id\"]\n",
    "enriched.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate topic id and probability\n",
    "\n",
    "enriched[[\"topic_id\",\"topic_probability\"]] = pd.DataFrame(enriched[\"topic\"].tolist(), \n",
    "                                                          index=enriched.index)\n",
    "enriched.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched = enriched.drop([\"topic\"], axis = 1)\n",
    "enriched.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# issue: quotes need to be escaped in neo4j\n",
    "# the following function helps us avoid errors when importing\n",
    "\n",
    "def add_quote(abstract):\n",
    "    if isinstance(abstract, str):\n",
    "        abstract = abstract.replace('\\\\\"', '\"\"')\n",
    "        return abstract.replace('\"', '\"\"')\n",
    "\n",
    "enriched[\"abstract\"] = enriched[\"abstract\"].apply(add_quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now export the document-topic pairs to csv \n",
    "file_name=\"abstract+topic.csv\"\n",
    "enriched.to_csv(file_name,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Extract words for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(topic, model, min_score=0.8):\n",
    "    \"\"\"\n",
    "    Extracting top n words for each document. \n",
    "    Selects the n most likely words whose p(word|topic) sum to min_score.\n",
    "    \"\"\"\n",
    "    dist = model.get_topic_word_dist(topic)\n",
    "    indices = np.flip(np.argsort(dist))\n",
    "    score = 0\n",
    "    word_kept = []\n",
    "    word_prob_kept = []\n",
    "    for index in indices:\n",
    "        if score > min_score: break\n",
    "        score += dist[index]\n",
    "        word_kept.append(model.used_vocabs[index])\n",
    "        word_prob_kept.append(dist[index])\n",
    "    return list(zip(word_kept, word_prob_kept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 125\n",
    "topics2words = []\n",
    "for i,model in enumerate(models): # note: preserving the order is extremely important here \n",
    "    print(\"Working on batch \"+str(i))\n",
    "    topics2words += [get_top_words(i, model, min_score=0.25) for i in range(num_topics)]\n",
    "print(len(topics2words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_ids = list(range(num_topics))*4\n",
    "len(topic_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_batch_indices = [0]*125+[1]*125+[2]*125+[3]*125\n",
    "len(topics_batch_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new topic dataframe\n",
    "topics_df = pd.DataFrame({\"TopicID\":topic_ids,\"BatchID\":topics_batch_indices,\"TopicWords\":topics2words})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topics_exploded = topics_df.explode(\"TopicWords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topics_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topics_exploded[[\"word\",\"word_probability\"]] = pd.DataFrame(topics_exploded[\"TopicWords\"].tolist(), \n",
    "                                                          index=topics_exploded.index)\n",
    "topics_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_exploded = topics_exploded.drop([\"TopicWords\"], axis = 1)\n",
    "topics_exploded.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now export the list of topic words to csv \n",
    "file_name=\"topics.csv\"\n",
    "topics_df.to_csv(file_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... and the word-topic pairs\n",
    "file_name = \"words.csv\"\n",
    "topics_exploded.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to Neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic nodes: \n",
    "- TopicID (long)\n",
    "- Words (list(str)) \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "        #Adding abstracts to existing publications [SHOULD HAVE BEEN DONE WITH METADATA]\n",
    "        #LOAD CSV WITH HEADERS FROM \"file:///abstract+topic.csv\" AS line\n",
    "        #WITH line WHERE line.publication_id IS NOT NULL\n",
    "        #MATCH (publication:Publication {title: line.publication_title})\n",
    "        #SET publication.abstract = line.abstract;\n",
    "        \n",
    "        #Defining the topic nodes\n",
    "        CREATE CONSTRAINT ON (t:Topic) ASSERT (t.ID, t.batchID) IS NODE KEY;\n",
    "        \n",
    "        #Loading the topic nodes from CSV\n",
    "        LOAD CSV WITH HEADERS FROM \"file:///topics.csv\" AS line\n",
    "        WITH line where line.TopicID IS NOT NULL\n",
    "        MERGE (t: Topic {ID: line.TopicID, batchID: line.BatchID})\n",
    "        SET t.words= line.TopicWords;\n",
    "\n",
    "        #Loading document<->topic relationships\n",
    "        LOAD CSV WITH HEADERS FROM \"file:///abstract+topic.csv\" AS line\n",
    "        MATCH (p:Publication {title: line.publication_title}),\n",
    "               (t:Topic {ID:line.topic_id, batchID: line.batch_id})\n",
    "        MERGE (p)-[r:IS_ABOUT {weight: round(1000 * toFloat(line.topic_probability)) / 1000}]->(t);\n",
    "        \n",
    "        #Constraint on word nodes being unique\n",
    "        CREATE CONSTRAINT ON (c:Word) ASSERT c.name IS UNIQUE;\n",
    "        \n",
    "        #Loading the word nodes from CSV\n",
    "        LOAD CSV WITH HEADERS FROM \"file:///words.csv\" AS line\n",
    "        WITH line where line.word IS NOT NULL\n",
    "        MERGE (w: Word{name: line.word});\n",
    "        \n",
    "        #Loading word<->topic relationships\n",
    "        LOAD CSV WITH HEADERS FROM \"file:///words.csv\" AS line\n",
    "        MATCH (t: Topic {ID: line.TopicID, batchID: line.BatchID}),\n",
    "                (w: Word {name: line.word})\n",
    "        MERGE (w)-[r:IS_IN {weight: round(1000 * toFloat(line.word_probability)) / 1000}]->(t);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the docs2topics and topics2words data extracted before to get nice visualisations of our collection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df = pd.read_csv(\"topics.csv\")\n",
    "topic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_topic(ls):\n",
    "    return ls.strip(\"][\").replace(\"'\",\"\").split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics2words = list(topic_df[\"TopicWords\"].apply(process_topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic sparsity measure of our documents\n",
    "docs2num_topics = [len(ts) for ts in docs2topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "h = plt.hist(docs2num_topics, bins=15, range=(0,10))\n",
    "plt.show(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = plt.hist(docs2num_topics, bins=10, range=(0,10), density=True, cumulative=True)\n",
    "plt.show(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word sparsity measure of our topics\n",
    "topics2num_words = [len(ts) for ts in topics2words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(topics2num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "h2 = plt.hist(topics2num_words, density = True)\n",
    "plt.show(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3 = plt.hist(topics2num_words, density=True, cumulative=True)\n",
    "plt.show(h3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
