{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction and insertion\n",
    "\n",
    "This notebook contains only the **final, necessary** code for the data extraction (cleaning/filtering/merging) and data import (into the graph database neo4j)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import numpy as np\n",
    "import re\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all data files, stratified by publication type\n",
    "\n",
    "zip_file = ZipFile('RC export 2020-10-12.zip')\n",
    "files = {text_file.filename: pd.read_csv(zip_file.open(text_file.filename))\n",
    "       for text_file in zip_file.infolist()\n",
    "       if text_file.filename.endswith('.csv')}\n",
    "\n",
    "ed = files[\"educational-2020-10-12.csv\"]\n",
    "books = files[\"books-2020-10-12.csv\"]\n",
    "conf = files[\"conference-2020-10-12.csv\"]\n",
    "journ = files[\"journal-2020-10-12.csv\"]\n",
    "oth = files[\"other-2020-10-12.csv\"]\n",
    "pap = files[\"papers-2020-10-12.csv\"]\n",
    "pat = files[\"patents-2020-10-12.csv\"]\n",
    "pres = files[\"presentations-2020-10-12.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we restrict ourselves to only book chapters, conference papers, journal papers and other papers\n",
    "\n",
    "research_data_df = pd.concat([books, conf, journ, pap], ignore_index=True)\n",
    "print(research_data_df.shape)\n",
    "print(\"There are \", research_data_df.shape[0], \"documents, \", \n",
    "      sum(research_data_df[\"dc.description.abstract\"].notna()), \" of which have abstracts\")\n",
    "research_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the columns\n",
    "set(research_data_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's remove all irrelevant columns\n",
    "\n",
    "rd_f = research_data_df[[\"id\",\"dc.contributor.author\",\"dc.title\",\"dc.date.issued\",\"dc.type\",\"dc.description.abstract\",\n",
    "                 \"dc.language.iso\", \"ethz.journal.title\"]]\n",
    "rd_f = rd_f.rename({\"dc.contributor.author\": \"author\", \"dc.date.issued\": \"publication date\",\n",
    "                       \"dc.title\": \"title\", \"dc.type\": \"publication type\", \"dc.description.abstract\": \"abstract\",\n",
    "                       \"dc.language.iso\": \"language\", \"ethz.journal.title\": \"journal\"}, \n",
    "              axis = 1)\n",
    "rd_f.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper functions for cleaning\n",
    "\n",
    "def separate_names(names):\n",
    "    \"\"\" Separes a string of names of the form name1||name2||name3||... into a list of names.\"\"\"\n",
    "    if isinstance(names, str):\n",
    "        return names.split(\"||\")\n",
    "\n",
    "def date_to_year(date):\n",
    "    \"\"\"Get the dates into a single format (YYYY)\"\"\"\n",
    "    if isinstance(date, str) and len(re.findall(\"[\\d]{4}\",date))>0:\n",
    "        return re.findall(\"[\\d]{4}\",date)[0]\n",
    "\n",
    "def date_to_year_and_month(date):\n",
    "    \"\"\"Get dates into format YYYY-MM\"\"\"\n",
    "    # if needed later\n",
    "    if isinstance(date, str):\n",
    "        if len(re.findall(\"[\\d]{4}[-][\\d]{2}\",date)) > 0:\n",
    "            return re.findall(\"[\\d]{4}[-][\\d]{2}\",date)[0]\n",
    "        else:\n",
    "            return str(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_array = rd_f[\"author\"].apply(separate_names)\n",
    "rd_f[\"author\"] = author_array\n",
    "# Date format\n",
    "date_format = date_to_year\n",
    "date_array = rd_f[\"publication date\"].apply(date_format)\n",
    "rd_f[\"publication date\"] = date_array\n",
    "\n",
    "rd_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a little detour: check how many papers were published each year\n",
    "\n",
    "rd_f[[\"id\",\"publication date\"]].groupby(\"publication date\").count().sort_values(\"publication date\", ascending = False).rename({'id':'count'}, axis = 1).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now explode on author field\n",
    "\n",
    "rd_fe = rd_f.explode(\"author\")\n",
    "print(\"Shape changed from \", rd_f.shape, \" to \", rd_fe.shape)\n",
    "rd_fe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Merge with department and organisation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Organisation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = pd.read_excel(\"ETH Professor list.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create professor column and keep only the relevant columns\n",
    "\n",
    "pl[\"Professor\"] = pl[\"Name\"] + \", \" + pl[\"First name\"]\n",
    "pl = pl[[\"Professor\", \"Org. unit code\", \"Organisation\"]]\n",
    "pl.columns = [\"professor\", \"organisation unit code\", \"organisation\"]\n",
    "print(pl.shape)\n",
    "pl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with exploded research collection\n",
    "\n",
    "rd_m = rd_fe.merge(pl,how=\"outer\",right_on=\"professor\",left_on=\"author\")\n",
    "print(\"Shape of research collection changed from \", rd_fe.shape, \" to \", rd_m.shape, \". Diff: \", \n",
    "      rd_m.shape[0]-rd_fe.shape[0])\n",
    "rd_m.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Leitzahl data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leitzahl_mapping = pd.read_csv(\"cost_centre_dept_mapping.csv\", encoding = \"ISO-8859-1\")\n",
    "leitzahl_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(leitzahl_mapping[\"NAME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: we have no department-specific code/leitzahl\n",
    "ln = leitzahl_mapping[[\"NAME\",\"DEPT\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map department code to department names\n",
    "dep_map = {'AGRL': 'Agricultural Sciences',\n",
    " 'ANBI': 'Applied Biosciences',\n",
    " 'ARCH': 'Architecture',\n",
    " 'BAUG': 'Civil, Environmental and Geomatic Engineering',\n",
    " 'BIOL': 'Biology',\n",
    " 'BSSE': 'Biosystems Science and Engineering',\n",
    " 'CHAB': 'Chemistry and Applied Biosciences',\n",
    " 'ERDW': 'Earth Sciences',\n",
    " 'GESS': 'Humanities, Social and Political Sciences',\n",
    " 'HEST': 'Health Sciences and Technology',\n",
    " 'INFK': 'Computer Science',\n",
    " 'ITET': 'Information Technology and Electrical Engineering',\n",
    " 'MATH': 'Mathematics',\n",
    " 'MATL': 'Materials',\n",
    " 'MAVT': 'Mechanical and Process Engineering',\n",
    " 'MTEC': 'Management, Technology and Economics',\n",
    " 'PHYS': 'Physics',\n",
    " 'USYS': 'Environmental Systems Science'}\n",
    "\n",
    "def transform_dep_code(code):\n",
    "    return dep_map[code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dep_code(code):\n",
    "    return dep_map[code]\n",
    "\n",
    "transform_dep_code('MATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln[\"DEPT\"] = ln[\"DEPT\"].apply(transform_dep_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now try with deleting parentheses after the names\n",
    "\n",
    "def delete_parenthesis(name):\n",
    "    if isinstance(name, str):\n",
    "        return re.split('(\\s\\([a-zA-Z.]+\\))', name)[0]\n",
    "\n",
    "print(delete_parenthesis('Schlunegger (ehem.)'))\n",
    "print(delete_parenthesis('Fontana, M. (em.)'))\n",
    "print(delete_parenthesis('Smith, Roy (Tit.)'))\n",
    "print(delete_parenthesis('Baccini, Peter (em.)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to leitzahl names, merge and check coverage\n",
    "\n",
    "ln[\"NAME\"] = ln[\"NAME\"].apply(delete_parenthesis)\n",
    "rd_m2 = rd_m.merge(ln, how = \"left\", left_on = \"author\", right_on = \"NAME\")\n",
    "print(\"Number of entries with complete author and department: \", \n",
    "      sum((rd_m2[\"author\"].notna() & rd_m2[\"DEPT\"].notna())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Final touches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop professor and name columns\n",
    "\n",
    "rd_final = rd_m2.drop(columns = [\"professor\", \"NAME\"])\n",
    "\n",
    "# rename for ease of neo4j import \n",
    "\n",
    "rd_final = rd_final.rename({\"publication date\": \"publication_date\", \"publication type\": \"publication_type\",\n",
    "                           \"organisation unit code\": \"organisation_unit_code\", \"DEPT\": \"department\"},\n",
    "                          axis = 1)\n",
    "rd_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# issue: quotes need to be escaped in neo4j\n",
    "# the following function helps us avoid errors when importing\n",
    "\n",
    "def add_quote(abstract):\n",
    "    if isinstance(abstract, str):\n",
    "        abstract = abstract.replace('\\\\\"', '\"\"')\n",
    "        return abstract.replace('\"', '\"\"')\n",
    "\n",
    "rd_final[\"abstract\"] = rd_final[\"abstract\"].apply(add_quote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import/export :):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd_final.to_csv(\"graph_data_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neo4j import details\n",
    "\n",
    "The following nodes will be created: \n",
    "- **person** [name, gender]\n",
    "- **publication** [id, title, date, type, abstract, journal]\n",
    "- **organisation** [name, code]\n",
    "- **department** [name, code]\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "\n",
    "**The commands**\n",
    "\n",
    "    \n",
    "To load the csv you first have to <u>copy it into your Neo4j base directory</u>. More info [here](https://neo4j.com/docs/cypher-manual/current/clauses/load-csv/#load-csv-import-data-from-a-csv-file). For instance, do the following: \n",
    "\n",
    "```cp .\\metadata_final.csv C:/Users/Giulia/.Neo4jDesktop/neo4jDatabases/database-befe90d3-7991-457e-9671-62c55c830654/installation-3.5.12/import```\n",
    "\n",
    "<u>Constraints first</u>\n",
    "\n",
    "The constraints are here to make sure we don't create duplicate nodes.\n",
    "\n",
    "    CREATE CONSTRAINT ON (c:Person) ASSERT c.name IS UNIQUE;\n",
    "    CREATE CONSTRAINT ON (c:Organisation) ASSERT c.code IS UNIQUE;\n",
    "    CREATE CONSTRAINT ON (c:Publication) ASSERT c.title IS UNIQUE;\n",
    "    CREATE CONSTRAINT ON (c:Department) ASSERT c.name IS UNIQUE; # or c.code\n",
    "    CREATE INDEX ON :Publication(id)\n",
    "\n",
    " \n",
    "Now we'll <u>load the data</u> in a very lightweight manner: \n",
    "\n",
    "1) person nodes <br>\n",
    "```\n",
    "    LOAD CSV WITH HEADERS FROM \"file:///graph_data_final.csv\" AS line\n",
    "    WITH line WHERE line.author IS NOT NULL\n",
    "    MERGE (person:Person {name: line.author})\n",
    "```\n",
    "\n",
    "2) publication nodes (this might take a while) <br>\n",
    "       \n",
    "        LOAD CSV WITH HEADERS FROM \"file:///graph_data_final.csv\" AS line\n",
    "        WITH line where line.id IS NOT NULL\n",
    "        MERGE (publication: Publication {title: line.title})\n",
    "        SET publication.id=line.id,             \n",
    "            publication.type=line.publication_type, \n",
    "            publication.date=date(line.publication_date),\n",
    "            publication.abstract=line.abstract,\n",
    "            publication.journal=line.journal;\n",
    "\n",
    "        \n",
    "3) organisation nodes <br> \n",
    "    \n",
    "        LOAD CSV WITH HEADERS FROM \"file:///graph_data_final.csv\" AS line\n",
    "        WITH line where line.organisation_unit_code IS NOT NULL\n",
    "        MERGE (organisation:Organisation {code:line.organisation_unit_code})\n",
    "        SET organisation.name=line.organisation;\n",
    "                                          \n",
    "            \n",
    "4) department nodes <br> \n",
    "    \n",
    "        LOAD CSV WITH HEADERS FROM \"file:///graph_data_final.csv\" AS line\n",
    "        WITH line where line.department IS NOT NULL\n",
    "        MERGE (department:Department {name:line.department})\n",
    "        SET department.name=line.department;\n",
    "                                                                \n",
    "            \n",
    "        \n",
    "5) finally all the edges <br> \n",
    "        \n",
    "        LOAD CSV WITH HEADERS FROM \"file:///graph_data_final.csv\" AS line\n",
    "        MATCH (person:Person {name:line.author}), \n",
    "               (publication:Publication {id:line.id})\n",
    "        MERGE (person)-[:PUBLISHED]->(publication)\n",
    "        \n",
    "        \n",
    "        LOAD CSV WITH HEADERS FROM \"file:///graph_data_final.csv\" AS line\n",
    "        MATCH (person:Person {name:line.author}),\n",
    "               (organisation:Organisation {code:line.organisation_unit_code})\n",
    "        MERGE (person)-[:BELONGS_TO]->(organisation)\n",
    "        \n",
    "        \n",
    "        LOAD CSV WITH HEADERS FROM \"file:///graph_data_final.csv\" AS line\n",
    "        MATCH (person:Person {name:line.author}),\n",
    "               (department:Department {name:line.department})\n",
    "        MERGE (person)-[:WORKS_IN]->(department)\n",
    "        \n",
    "        Alternative without leitzahl:\n",
    "        LOAD CSV WITH HEADERS FROM \"file:///graph_data_final.csv\" AS line\n",
    "        MATCH (person:Person {name:line.author}),\n",
    "               (department:Department {name:line.department})\n",
    "        MERGE (person)-[:WORKS_IN]->(department)\n",
    "        \n",
    "        \n",
    "Note: in case you did something wrong and you want to erase the network here's the query: \n",
    "\n",
    "        MATCH (n)\n",
    "        DETACH DELETE n;\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export for topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only english publications\n",
    "\n",
    "rd_full_en = rd_m2[rd_m2[\"language\"] == \"en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates (keeping only the first)\n",
    "\n",
    "abstracts_data = rd_full_en.drop_duplicates(subset = [\"id\"], keep = \"first\")\n",
    "print(abstracts_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the entries that have an abstract\n",
    "\n",
    "print(\"Number of abstracts: \", sum(abstracts_data[\"abstract\"].notna()))\n",
    "abstracts_only = abstracts_data[abstracts_data[\"abstract\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export only abstract and id\n",
    "\n",
    "abstracts_only[[\"abstract\",\"id\",\"title\"]].to_csv(\"abstracts_eng.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for estimating storage requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_storage_upper_bound(nrpubs, nrdeps, nrorgs, nrpeople):\n",
    "    \"\"\"Gives a rough upper bound of the storage required for a graph (GB) with the given input parameter values\"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Record size per node: 15B\n",
    "    # Record size per edge: 34B\n",
    "    # Record size per attribute: 41B\n",
    "    # Record size per string or array attribute: 128B\n",
    "    # https://neo4j.com/developer/kb/understanding-data-on-disk/\n",
    "    \n",
    "    nrtopics = np.log(nrpubs) # assume that the number of topics grows logarithmically with the number of publications\n",
    "    \n",
    "    # for each type on node, multiply the number of nodes with the storage required for the node annd its attributes\n",
    "    node_storage = nrpubs*(15+2*41+4*128) + nrdeps*(15+41+128) + nrorgs*(15+41+128) + nrpeople*(15+41+128) + nrtopics*(15+41+128)\n",
    "    \n",
    "    dep_people_edges = nrdeps*40 # assume max 40 professors per department on average\n",
    "    org_people_edges = nrorgs*5 # assume max 5 professors per organisation on average\n",
    "    pub_people_edges = nrpubs*10 # assume max 10 authors per publication on average\n",
    "    pub_topic_edges = nrpubs*20 # assume max 10 topics per publication on average\n",
    "    \n",
    "    # for each type on edge, multiply the number of nodes with the storage required for the node annd its attributes\n",
    "    edge_storage = dep_people_edges*34 + org_people_edges*34 + pub_people_edges*34 + pub_topic_edges*(34+128)\n",
    "    \n",
    "    # storage required for indices\n",
    "    # following neo4j heuristics: average property value size * (1/3)\n",
    "    # we have four indices, one for each node\n",
    "    avg_prop_size = (6*41+9*128)/15\n",
    "    index_storage = avg_prop_size*(nrpubs + nrdeps + nrorgs + nrpeople)*(1/3)\n",
    "    \n",
    "    # add and return in GB\n",
    "    return (node_storage + edge_storage + index_storage)/10**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_storage_upper_bound(170000, 16, 400, 10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
