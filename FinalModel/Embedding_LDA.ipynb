{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy as tp\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "import nltk\n",
    "from nltk import word_tokenize, RegexpTokenizer,PunktSentenceTokenizer, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics embeddings pipeline \n",
    "Here we implement the embedding pipeline for the topics extracted by the collection. <br>\n",
    "The results are the following: \n",
    "1. Topic embeddings \n",
    "2. Document embeddings \n",
    "3. Author embeddings\n",
    "4. Query embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading topic model(s) and collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = pickle.load(open( \"collection_cleaned_fullwords.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 4\n",
    "batches = np.array_split(collection, num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_indices(batches):\n",
    "    res = []\n",
    "    for i,batch in enumerate(batches):\n",
    "        res += [i]*len(batch)\n",
    "    return res\n",
    "batch_indices = get_batch_indices(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of docs per batch\n",
    "for b in batches: print(len(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch training models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics_per_batch = 125\n",
    "models = []\n",
    "model_names = ['LDA1batch1.bin','LDA1batch2.bin','LDA1batch3.bin','LDA1batch4.bin']\n",
    "for name in model_names:\n",
    "    models.append(tp.LDAModel.load(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract the topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_topics(document, model, min_score=0.8):\n",
    "    \"\"\" \n",
    "    Extracting top n topics for each document. \n",
    "    Selects the n most likely topics whose p(topic|document) sum to min_score.\n",
    "    \"\"\"\n",
    "    # inserting the document in the model\n",
    "    new_doc = model.make_doc(document)\n",
    "    _,_ = model.infer(new_doc)\n",
    "    # ordering from most probable topic to least one \n",
    "    dist = new_doc.get_topic_dist()\n",
    "    indices = np.flip(np.argsort(dist))\n",
    "    score = 0\n",
    "    indices_kept = []\n",
    "    probs_kept = []\n",
    "    for index in indices:\n",
    "        if score > min_score: break\n",
    "        score += dist[index]\n",
    "        indices_kept.append(index)\n",
    "        probs_kept.append(dist[index])\n",
    "    return list(zip(indices_kept, probs_kept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting topics for the original collection\n",
    "coll2topics = []\n",
    "# this takes a super long time\n",
    "for model, batch in zip(models,batches):\n",
    "    coll2topics += [get_top_topics(doc, model) for doc in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coll2topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(topic, model, min_score=0.8):\n",
    "    \"\"\"\n",
    "    Extracting top n words for each document. \n",
    "    Selects the n most likely words whose p(word|topic) sum to min_score.\n",
    "    \"\"\"\n",
    "    dist = model.get_topic_word_dist(topic)\n",
    "    indices = np.flip(np.argsort(dist))\n",
    "    score = 0\n",
    "    word_kept = []\n",
    "    word_prob_kept = []\n",
    "    for index in indices:\n",
    "        if score > min_score: break\n",
    "        score += dist[index]\n",
    "        word_kept.append(model.used_vocabs[index])\n",
    "        word_prob_kept.append(dist[index])\n",
    "    return list(zip(word_kept, word_prob_kept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_prob = 0.5 # extracting only top min_prob% of the words\n",
    "topics2words = []\n",
    "topics2words_batched = [] # saving both topics and their batches \n",
    "for model in models: \n",
    "    twt = [get_top_words(i, model, min_score=min_prob) for i in range(num_topics_per_batch)]\n",
    "    topics2words += twt\n",
    "    topics2words_batched += [twt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(topics2words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(topics2words_batched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embedding words in topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Glove Embeddings to embed the words in the topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import numpy as np \n",
    "import os \n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading from binary the glove vocabulary and embedding \n",
    "glove_vocab_path = \"glove_vocab\"\n",
    "glove_embedding_path = \"glove_embedding\"\n",
    "with open(glove_vocab_path, \"rb\") as fp:  \n",
    "    glove_vocab = pickle.load(fp)\n",
    "with open(glove_embedding_path, \"rb\") as fp: \n",
    "    glove_embedding = pickle.load(fp)\n",
    "assert len(glove_vocab)==len(glove_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_embeddings_topic(topic, vocab, embedding, silence=False):\n",
    "    \"\"\" Topic is represented as a list of tuples (word, word weight)\"\"\"\n",
    "    matched = 0 \n",
    "    total = 0\n",
    "    topic_embeddings = []\n",
    "    topic_weights = []\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for item in topic: \n",
    "        word, weight = item\n",
    "        total+=1\n",
    "        #check if the word appears in vocabulary \n",
    "        if word in vocab.values(): \n",
    "            matched+=1\n",
    "            emb = embedding[list(vocab.values()).index(word)]\n",
    "            topic_embeddings += [emb]\n",
    "            topic_weights += [weight]\n",
    "    \n",
    "    end = time.time()\n",
    "    if not silence: \n",
    "        print(\"Total time: \"+str(round(end-start,2))+\" s.\")\n",
    "        print(\"Proportion of matched words: \"+str(round(matched/total,2)))\n",
    "    return topic_embeddings, topic_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_embeddings, topic_weights = get_list_embeddings_topic(topics2words[0], glove_vocab, glove_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding nodes - baseline\n",
    "\n",
    "## 4. Embedding topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_convex_combination(weights, embeddings):\n",
    "    \"\"\" Obtain an embedding as convex combination of embedding vectors according \n",
    "    to the weights provided.\"\"\"\n",
    "    weight_vec = np.asarray(weights)\n",
    "    emb_vec = np.asarray(embeddings)\n",
    "    normalized_weights = weight_vec / np.sum(np.absolute(weight_vec))\n",
    "    return normalized_weights.transpose().dot(emb_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_emb = get_convex_combination(topic_weights, topic_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_convex_topics_embeddings(topics_collection, vocab, embedding):\n",
    "    \"\"\" Runs the above function over a whole collection of topics\"\"\"\n",
    "    topics_embs = []\n",
    "    for topic2word in topics_collection:\n",
    "        # extract the emebddings for the words in the topic \n",
    "        topic_embeddings, topic_weights = get_list_embeddings_topic(topic2word, vocab, embedding, silence=True)\n",
    "        # combine the embeddings with a convex sum \n",
    "        topic_emb = get_convex_combination(topic_weights, topic_embeddings)\n",
    "        topics_embs += [topic_emb]\n",
    "    return topics_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these two also take a while\n",
    "topics_embs = get_convex_topics_embeddings(topics2words, glove_vocab, glove_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_embs_batched = [get_convex_topics_embeddings(t2w, glove_vocab, glove_embedding) for t2w in topics2words_batched]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look at the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbors(this, others):\n",
    "    \"\"\"Returns the most similar vectors among others to the given one \"\"\"\n",
    "    cos = torch.nn.CosineSimilarity(dim = -1)\n",
    "    ranks = cos(torch.tensor(this), torch.tensor(others))\n",
    "    mostSimilar = []\n",
    "    return ranks.numpy().argsort()[::-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the first topic \n",
    "nns_100 = nearest_neighbors(topics_embs[100], topics_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_most_similar_topics(topic_id, batch_id, topics_embs, topics2words,\n",
    "                                  topics_embs_batched, topics2words_batched, n=10):\n",
    "    \"\"\"Prints the words of the topic and its neareast neighbors.\"\"\"\n",
    "    nns = nearest_neighbors(topics_embs_batched[batch_id][topic_id], topics_embs)\n",
    "    print(\"-\"*10)\n",
    "    print(\"Topic \"+ str(topic_id))\n",
    "    print(\" \".join(item[0] for item in topics2words_batched[batch_id][topic_id]))\n",
    "    print(\"-\"*10)\n",
    "    print(str(n)+\" most similar topics\")\n",
    "    for i in range(n):\n",
    "        print(\"Topic \"+str(nns[i+1]))\n",
    "        print(\" \".join(item[0] for item in topics2words[nns[i+1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_most_similar_topics(11, 1, topics_embs, topics2words,topics_embs_batched, topics2words_batched, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Documents embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_data = pd.read_csv(\"abstracts_eng.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the ordering by visual inspection \n",
    "n = np.random.randint(len(collection))\n",
    "print(abstracts_data[\"abstract\"][n][0:500]) \n",
    "print(\"\")\n",
    "print(\" \".join(collection[n][0:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched = abstracts_data\n",
    "enriched[\"topics\"] = coll2topics\n",
    "enriched[\"batchID\"] = batch_indices\n",
    "#enriched=enriched.explode(\"topics\")\n",
    "enriched.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs2topics = enriched[[\"topics\",\"batchID\"]].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_embeddings_document(doc, topics_embedding, silence=False):\n",
    "    \"\"\" Returns the list of topic embeddings and topic weights for the given document.\n",
    "    The document is represented as the tuple ([(topic_id, topic_weight)], batch_id)\"\"\"\n",
    "    doc_embeddings = []\n",
    "    doc_weights = []\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    topics, batch_id = doc\n",
    "    \n",
    "    for topic in topics: \n",
    "        topic_id, weight = topic\n",
    "        emb = topics_embedding[batch_id][topic_id]\n",
    "        doc_embeddings += [emb]\n",
    "        doc_weights += [weight]\n",
    "    \n",
    "    end = time.time()\n",
    "    if not silence: \n",
    "        print(\"Total time: \"+str(round(end-start,2))+\" s.\")\n",
    "    return doc_embeddings, doc_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc0_embs = get_list_embeddings_document(docs2topics[0], topics_embs_batched)\n",
    "len(doc0_embs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally documents embeddings \n",
    "def get_convex_docs_embeddings(documents_collection, topics_embedding):\n",
    "    \"\"\" Runs the above function over a whole collection of documents\"\"\"\n",
    "    docs_embs = []\n",
    "    for doc2topics in documents_collection:\n",
    "        # extract the emebddings for the words in the topic \n",
    "        doc_embeddings, doc_weights = get_list_embeddings_document(doc2topics, topics_embedding, silence=True)\n",
    "        # combine the embeddings with a convex sum \n",
    "        doc_emb = get_convex_combination(doc_embeddings, doc_weights)\n",
    "        docs_embs += [doc_emb]\n",
    "    return docs_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_embeddings = get_convex_docs_embeddings(docs2topics, topics_embs_batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_most_similar_docs(doc_id, docs_embeddings, docs_collection, n=10):\n",
    "    \"\"\"Prints the words of the topic and its neareast neighbors.\"\"\"\n",
    "    nns = nearest_neighbors(docs_embeddings[doc_id], docs_embeddings)\n",
    "    print(\"Document \"+ str(doc_id))\n",
    "    print((docs_collection[doc_id]).partition(\"||\")[0])\n",
    "    print(\"-\"*10)\n",
    "    print(str(n)+\" most similar documents in the collection\")\n",
    "    for i in range(n):\n",
    "        print(\"\")\n",
    "        print(\"Document \"+str(nns[i+1]))\n",
    "        print((docs_collection[nns[i+1]]).partition(\"||\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_most_similar_docs(5, documents_embeddings, abstracts_data[\"abstract\"], n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Authors embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the graph data \n",
    "graph_data = pd.read_csv(\"graph_data_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data = pd.merge(abstracts_data.reset_index(level=0, inplace=False)[[\"index\",\"title\"]], #adding column index, dropping everything else\n",
    "                      graph_data, on=[\"title\"], how=\"inner\")\n",
    "graph_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first use the metadata data to compute a scoring author-publication that we will then use to combine the publications embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df = graph_data.groupby([\"author\"]).size().reset_index(name='counts')\n",
    "graph_data_prime = pd.merge(counts_df, graph_data, on=[\"author\"], how=\"inner\")\n",
    "graph_data_prime = graph_data_prime[[\"index\",\"counts\",\"title\",\"author\",\"publication_date\",\"publication_type\"]]\n",
    "graph_data_prime.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df2 = graph_data_prime.groupby([\"title\"]).size().reset_index(name ='number_of_authors')\n",
    "graph_data_prime = pd.merge(counts_df2, graph_data_prime, on=[\"title\"], how=\"inner\")\n",
    "graph_data_prime = graph_data_prime[[\"index\",\"counts\",\"title\",\"author\",\"publication_date\",\"publication_type\",\"number_of_authors\"]]\n",
    "graph_data_prime.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha(x, t=20): \n",
    "    \"\"\" Hard thresholding function for number of publications\"\"\"\n",
    "    if x>t: return t\n",
    "    return x\n",
    "\n",
    "# totally arbitrary- can be changed\n",
    "publication_types_scores = {\n",
    "    \"Conference Paper\":10,\n",
    "    \"Journal Article\":9,\n",
    "    \"Book Chapter\":8,\n",
    "    \"Review Article\":7}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_publication(date, pub_type, number_of_authors, gamma = 0.5):\n",
    "    \"\"\" Scoring function for author-publication pair\"\"\"\n",
    "    #todo/future-work: add \"is_professor\" qualifier to the scoring \n",
    "    # todo: rank of a journal\n",
    "    # todo: citations - first author\n",
    "    date_weight = np.exp(date - (avg_date))\n",
    "    author_weight = (1/number_of_authors)**gamma\n",
    "    pub_type_weight = publication_types_scores.get(pub_type,1)\n",
    "    return pub_type_weight*date_weight*author_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date = max(graph_data_prime[\"publication_date\"])\n",
    "min_date = min(graph_data_prime[\"publication_date\"])\n",
    "avg_date = np.mean(graph_data_prime[\"publication_date\"])\n",
    "avg_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data_prime[\"score\"] = graph_data_prime.apply(lambda row: score_publication(row[\"publication_date\"], row[\"publication_type\"], row[\"number_of_authors\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph_data_prime.shape)\n",
    "graph_data_prime = graph_data_prime.drop_duplicates([\"title\",\"author\"])\n",
    "print(graph_data_prime.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data_prime.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data_prime[\"pub\"]=list(zip(graph_data_prime[\"index\"], graph_data_prime[\"score\"]))\n",
    "graph_data_prime = graph_data_prime.groupby([\"author\"])[\"pub\"].apply(list).reset_index(name='pubs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors2pubs = graph_data_prime.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([len(l[1]) for l in authors2pubs]) # average number of publications per author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a threshold function to only take t publications with highest score into account for author embedding\n",
    "def trim_by_threshold(embeddings, weights, t = 20):\n",
    "    if len(embeddings) <= t:\n",
    "        return embeddings, weights\n",
    "    else:\n",
    "        ordered_by_weight = sorted(zip(weights, embeddings), key=lambda pair: pair[0], reverse = True)\n",
    "        ordered_weights = [weight for weight,_ in ordered_by_weight]\n",
    "        ordered_embeddings = [embeddings for _,embeddings in ordered_by_weight]\n",
    "        return ordered_embeddings[:t], ordered_weights[:t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_embeddings_author(author, documents_embeddings, silence=False):\n",
    "    \"\"\" Returns the list of document embeddings and document weights for the given author.\n",
    "    The author is represented as the list [(pub, score)]\"\"\"\n",
    "    auth_embeddings = []\n",
    "    auth_weights = []\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    \n",
    "    for item in author[1]: \n",
    "        pub_id, weight = item\n",
    "        emb = documents_embeddings[pub_id-1]\n",
    "        auth_embeddings += [emb]\n",
    "        auth_weights += [weight]\n",
    "    \n",
    "    end = time.time()\n",
    "    if not silence: \n",
    "        print(\"Total time: \"+str(round(end-start,2))+\" s.\")\n",
    "    return auth_embeddings, auth_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally documents embeddings \n",
    "def get_convex_auth_embeddings(authors2pubs, documents_embeddings):\n",
    "    \"\"\" Runs the above function over a whole collection of documents\"\"\"\n",
    "    auths_embs = []\n",
    "    for author in authors2pubs:\n",
    "        # extract the emebddings for the words in the topic \n",
    "        auth_embeddings, auth_weights = get_list_embeddings_author(author, documents_embeddings, silence=True)\n",
    "        # combine the embeddings with a convex sum \n",
    "        # threshold to the most important publications\n",
    "        auth_embeddings_thres, auth_weights_thres = trim_by_threshold(auth_embeddings, auth_weights, t = 20)\n",
    "        auth_emb = get_convex_combination(auth_embeddings_thres, auth_weights_thres)\n",
    "        auths_embs += [auth_emb]\n",
    "    return auths_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_embeddings = get_convex_auth_embeddings(authors2pubs, documents_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_most_similar_authors(author_id, authors_embeddings, authors2pubs,  n=10):\n",
    "    \"\"\"Prints the names of the input author and its neareast neighbors.\"\"\"\n",
    "    nns = nearest_neighbors(authors_embeddings[author_id], authors_embeddings)[6:]\n",
    "    print(\"Author \"+ str(author_id))\n",
    "    author = authors2pubs[author_id]\n",
    "    print(\"Name : \"+author[0])\n",
    "    print(\"Some publications:  \\n- \"+\"\\n- \".join([abstracts_data.iloc[item[0]][\"title\"] for item in author[1]]))\n",
    "    print(\"-\"*10)\n",
    "    print(str(n)+\" most similar authors in the collection\")\n",
    "    for i in range(n):\n",
    "        print(\"\")\n",
    "        print(\"Author \"+str(nns[i+1]))\n",
    "        author = authors2pubs[nns[i+1]]\n",
    "        print(\"Name : \"+author[0])\n",
    "        print(\"Some publications:   \\n- \"+\"\\n - \".join([abstracts_data.iloc[item[0]][\"title\"] for item in author[1][0:5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors2pubs_dict = {name:pubs for name,pubs in authors2pubs}\n",
    "list(authors2pubs_dict.keys()).index(\"Ghaffari, Mohsen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_most_similar_authors(19795, authors_embeddings, authors2pubs, n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_most_similar_topics_author(author_id, authors_embeddings, \n",
    "                                         topics_embeddings, authors2pubs,topics2words, n=10):\n",
    "    \"\"\"Prints the topics that are closest to the given author in the vector space\"\"\"\n",
    "    nns = nearest_neighbors(authors_embeddings[author_id], topics_embeddings)\n",
    "    print(\"Author \"+ str(author_id))\n",
    "    author = authors2pubs[author_id]\n",
    "    print(\"Name : \"+author[0])\n",
    "    #print(\"Some publications:  \\n- \"+\"\\n- \".join([abstracts_data.iloc[item[0]][\"title\"] for item in author[1]]))\n",
    "    print(\"-\"*10)\n",
    "    print(str(n)+\" most similar topics\")\n",
    "    for i in range(n):\n",
    "        print(\"\")\n",
    "        print(\"Topic \"+str(nns[i+1]))\n",
    "        print(\" \".join(item[0] for item in topics2words[nns[i+1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualise_most_similar_topics_author(list(authors2pubs_dict.keys()).index(\"Ghaffari, Mohsen\"), \n",
    "                                     authors_embeddings, topics_embs, authors2pubs, topics2words, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Embedding a query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_embeddings_query(query, vocab, embedding, silence=False):\n",
    "    \"\"\" Query is a list of words\"\"\"\n",
    "    matched = 0 \n",
    "    total = 0\n",
    "    query_embeddings = []\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for word in word_tokenize(query.lower()): \n",
    "        total+=1\n",
    "        #check if the word appears in vocabulary \n",
    "        if word in vocab.values(): \n",
    "            matched+=1\n",
    "            emb = embedding[list(vocab.values()).index(word)]\n",
    "            query_embeddings += [emb]\n",
    "    \n",
    "    end = time.time()\n",
    "    if not silence: \n",
    "        print(\"Total time: \"+str(round(end-start,2))+\" s.\")\n",
    "        print(\"Proportion of matched words: \"+str(round(matched/total,2)))\n",
    "    return query_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_most_similar_topics_query(query, query_emb, topics_embeddings, topics2words, n=10):\n",
    "    \"\"\"Prints the topics that are closest to the given author in the vector space\"\"\"\n",
    "    nns = nearest_neighbors(query_emb, topics_embeddings)\n",
    "    print(\"Query: \"+ query)\n",
    "    print(\"-\"*10)\n",
    "    print(str(n)+\" most similar topics\")\n",
    "    for i in range(n):\n",
    "        print(\"\")\n",
    "        print(\"Research topics \"+str(nns[i+1]))\n",
    "        print(\" \".join(item[0] for item in topics2words[nns[i+1]][:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_most_similar_authors_query(query, query_emb, authors_embeddings, authors2pubs, n=10):\n",
    "    \"\"\"Prints the authors that are closest to the given author in the vector space\"\"\"\n",
    "    nns = nearest_neighbors(query_emb, authors_embeddings)\n",
    "    print(\"Query: \"+ query)\n",
    "    print(\"-\"*10)\n",
    "    print(str(n)+\" most similar authors in the collection\")\n",
    "    for i in range(n):\n",
    "        print(\"\")\n",
    "        print(\"Author \"+str(nns[i+1]))\n",
    "        author = authors2pubs[nns[i+1]]\n",
    "        print(\"Name : \"+author[0])\n",
    "        print(\"Some publications:   \\n- \"+\"\\n - \".join([abstracts_data.iloc[item[0]][\"title\"] for item in author[1][0:5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_most_similar_docs_query(query, query_emb, docs_embeddings, docs_collection, n=10):\n",
    "    \"\"\"Prints the authors that are closest to the given author in the vector space\"\"\"\n",
    "    nns = nearest_neighbors(query_emb, docs_embeddings)\n",
    "    print(\"Query: \"+ query)\n",
    "    print(\"-\"*10)\n",
    "    print(str(n)+\" most similar documents in the collection\")\n",
    "    for i in range(n):\n",
    "        print(\"\")\n",
    "        print(\"Document \"+str(nns[i+1]))\n",
    "        print((docs_collection[nns[i+1]]).partition(\"||\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"climate change\"\n",
    "query_embs = get_list_embeddings_query(query, glove_vocab, glove_embedding)\n",
    "# aggregating the query embeddings with a mean\n",
    "query_emb = np.mean(query_embs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualise_most_similar_topics_query(query, query_emb, topics_embs, topics2words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualise_most_similar_authors_query(query, query_emb, authors_embeddings, authors2pubs, n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_most_similar_docs_query(query, query_emb, documents_embeddings,abstracts_data[\"abstract\"] , n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we have to select a subset of topics and their words \n",
    "tn = 10 \n",
    "pn = 5 # number of words to plot per topic\n",
    "topics = topics_embs[0:tn]\n",
    "# collect the vocabulary for the selected topics -------------\n",
    "# and save the embeddings of their words \n",
    "words = []\n",
    "word_embs = []\n",
    "for item in topics2words[0:tn]: \n",
    "    for t2w in item[0:pn]: \n",
    "        word, _ = t2w\n",
    "        if word not in words: \n",
    "            try: \n",
    "                emb = glove_embedding[list(glove_vocab.values()).index(word)]\n",
    "                words +=[word]\n",
    "                word_embs +=[emb]\n",
    "            except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA to 2 dimensions \n",
    "pca = PCA(n_components=2)\n",
    "topics_embeddings_df = pd.DataFrame(topics)\n",
    "word_embeddings_df = pd.DataFrame(word_embs)\n",
    "pca_topics = pca.fit_transform(topics_embeddings_df.values)\n",
    "pca_words = pca.fit_transform(word_embeddings_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(13,7))\n",
    "plt.scatter(pca_topics[:,0], pca_topics[:,1],linewidths=1,color='red')\n",
    "for i, _ in enumerate(topics):\n",
    "    plt.annotate(\"Topic \"+str(i),xy=(pca_topics[i,0],pca_topics[i,1]))\n",
    "plt.xlabel(\"PC1\",size=15)\n",
    "plt.ylabel(\"PC2\",size=15)\n",
    "plt.title(\"Word Embedding Space\",size=20)\n",
    "plt.scatter(pca_words[:,0], pca_words[:,1],linewidths=1,color='blue')\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word,xy=(pca_words[i,0],pca_words[i,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3.7 install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting authors embeddings by department \n",
    "# We import sklearn and TSNE.\n",
    "import sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import umap.umap_ as umap\n",
    "\n",
    "\n",
    "# We'll hack a bit with the t-SNE code in sklearn.\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.manifold.t_sne import (_joint_probabilities,\n",
    "                                    _kl_divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use matplotlib for graphics.\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "# We import seaborn to make nice plots.\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('muted')\n",
    "sns.set_context(\"notebook\", font_scale=1.5,\n",
    "                rc={\"lines.linewidth\": 2.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get authors embeddings and labels (aka departments)\n",
    "auths2dept = graph_data.drop_duplicates([\"author\",\"department\"])[[\"author\",\"department\"]].dropna()\n",
    "auths2dept[\"department\"] = auths2dept.department.astype(\"category\")\n",
    "auths2dept[\"department_id\"] = pd.factorize(auths2dept[\"department\"])[0]\n",
    "authors_list = auths2dept[\"author\"].tolist()\n",
    "authors_labels = auths2dept[\"department\"].tolist()\n",
    "authors_labels_ids = auths2dept[\"department_id\"].tolist()\n",
    "authors_list_embs = [authors_embeddings[list(authors2pubs_dict.keys()).index(name)] for name in authors_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dept = max(auths2dept[\"department_id\"]) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_list_embs = np.asarray(authors_list_embs)\n",
    "rows_to_keep = ~np.isnan(authors_list_embs).any(axis=1)\n",
    "authors_list_embs = authors_list_embs[rows_to_keep]\n",
    "authors_labels = np.asarray(authors_labels)[rows_to_keep]\n",
    "authors_labels_ids = np.asarray(authors_labels_ids)[rows_to_keep]\n",
    "authors_list = np.asarray(authors_list)[rows_to_keep]\n",
    "authors_list_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auths_projected = TSNE(random_state=25111993).fit_transform(authors_list_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "auths_projected_pca = pca.fit_transform(authors_list_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()\n",
    "auths_projected_umap = reducer.fit_transform(authors_list_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter(x, colors, labels_list, labels=False):\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", num_dept))\n",
    "\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(15, 7))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=100,\n",
    "                    c=palette[colors.astype(np.int)])\n",
    "    ax.axis('on')\n",
    "    ax.axis('tight')\n",
    "\n",
    "    txts = []\n",
    "    if labels: \n",
    "    # We add the labels for each digit.\n",
    "        for i in range(11):\n",
    "            # Position of each label.\n",
    "            xtext, ytext = np.median(x[colors == i, :], axis=0)\n",
    "            txt = ax.text(xtext, ytext, str(labels_list[i]), fontsize=50)\n",
    "            txt.set_path_effects([\n",
    "                PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n",
    "                PathEffects.Normal()])\n",
    "            txts.append(txt)\n",
    "\n",
    "    return f, ax, sc, txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(authors_labels))\n",
    "sns.palplot(np.array(sns.color_palette(\"hls\", num_dept)))\n",
    "scatter(auths_projected_pca, authors_labels_ids, authors_labels)\n",
    "pass\n",
    "#plt.savefig('tsne-generated_18_clust_48_100.png', dpi=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Embedding topics - representation learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea: a simple autoencoder to reconstruct topics embeddings\n",
    "# The autoencoder takes as input the topic 2 word distribution and finds \n",
    "# a representation of the topic able to reconstruct it (+ idea: add noise in the input)\n",
    "# We will enforce similarity with the topic words by applying a regulariser that measures \n",
    "# the non-similarity with the topic words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class topicsLoss(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def topic_words_dissimilarity(self, hidden, topicwords_embs, topicwords_weights):\n",
    "        \"\"\" \n",
    "        topicwords_embs: torch tensor of size BATCH x NUM_WORDS x EMB_DIM \n",
    "        topicwords_weights: torch tensor of size BATCH x NUM_WORDS \n",
    "        \n",
    "        \"\"\"\n",
    "        # 1. normalise the weights \n",
    "        normalized_weights = weight_vec / np.sqrt(np.sum(weight_vec**2))\n",
    "        # 2. compute hidden - embeddings distances\n",
    "        distances = torch.cdist(topicwords_embs,hidden) # B x W\n",
    "        # 3. dot product of weights and distances \n",
    "        total loss = \n",
    "        return total_loss \n",
    "    \n",
    "    def forward(self, outputs, targets, hidden, topicswords_embs, topicswords_weights):\n",
    "        loss = self.mse(outputs,targets) + self.topic_words_dissimilarity(hidden, topicswords_embs, topicswords_weights)\n",
    "        return loss \n",
    "        \n",
    "    \n",
    "class topicsAE(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        \"\"\" \n",
    "        simply 4 shrinking linear layers with relu non linearities \n",
    "         - words2topic max -----> 150 \n",
    "         - 150 -----> 300\n",
    "         - 300 -----> 150\n",
    "         - 150 -----> words2topic max\n",
    "        \"\"\"\n",
    "        self.encoder_hidden_layer = nn.Linear(in_features=kwargs[\"input_shape\"], out_features=150)\n",
    "        self.encoder_output_layer = nn.Linear(in_features=150, out_features=300)\n",
    "        self.decoder_hidden_layer = nn.Linear(in_features=300, out_features=150)\n",
    "        self.decoder_output_layer = nn.Linear(in_features=150, out_features=kwargs[\"input_shape\"])\n",
    "\n",
    "    def forward(self, features):\n",
    "        activation = self.encoder_hidden_layer(features)\n",
    "        activation = torch.relu(activation)\n",
    "        code = self.encoder_output_layer(activation)\n",
    "        code = torch.relu(code)\n",
    "        activation = self.decoder_hidden_layer(code)\n",
    "        activation = torch.relu(activation)\n",
    "        activation = self.decoder_output_layer(activation)\n",
    "        reconstructed = torch.relu(activation)\n",
    "        return reconstructed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cdist(c, a).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(a.transpose(1,2),b.unsqueeze(2)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data \n",
    "# the input to the model is a set of words_indices vectors \n",
    "def get_input_data(topics2words, vocab, max_len):\n",
    "    \"\"\" Returns a list of word indices for each of the topics \n",
    "    given as input. \"\"\"\n",
    "    words_idxs=[]\n",
    "    topics_embs=[]\n",
    "    topics_weights=[]\n",
    "    for topic in topics2words: \n",
    "        twi = [0]*max_len\n",
    "        topic_embeddings, topic_weights = get_list_embeddings_topic(topic, glove_vocab, glove_embedding, silence=True)\n",
    "        # padding the vectors before adding them to the list \n",
    "        topics_embs += [F.pad(torch.tensor(topic_embeddings, requires_grad=False), [0,0,0,max_len-len(topic_embeddings)])] \n",
    "        topics_weights += [F.pad(torch.tensor(topic_weights,  requires_grad=False), \n",
    "                                 [0,max_len-len(topic_weights)])]\n",
    "        for i,item in enumerate(topic): \n",
    "            word = item[0]\n",
    "            try: # could fail if word not in the vocabulary\n",
    "                twi[i] = list(vocab.values()).index(word)\n",
    "            except Exception as e: pass\n",
    "        words_idxs += [twi] \n",
    "    return words_idxs, topics_embs, topics_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_INPUT_LEN = 50\n",
    "input_data, topics_embs, topics_weights = get_input_data(topics2words, glove_vocab, MAX_INPUT_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "NUM_TRAIN_BATCHES = 5 # 100 topics in 1 batch ---> 5 batches training \n",
    "train_batches = np.array_split(list(range(len(input_data))), NUM_TRAIN_BATCHES)\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = topicsAE(input_shape=MAX_INPUT_LEN).to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "criterion = topicsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training \n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for batch in train_batch_data:\n",
    "        # collecting input data\n",
    "        model_input = torch.tensor(input_data[batch[0]:batch[-1]], dtype=torch.float)\n",
    "        batch_topics_embs = torch.tensor(topics_embs[batch[0]:batch[-1]], requires_grad=False)\n",
    "        batch_topics_weights = torch.tensor(topics_weights[batch[0]:batch[-1]],  requires_grad=False)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(model_input)\n",
    "        # get tensor embedding from the autoencoder \n",
    "        activation = net.encoder_hidden_layer(model_input)\n",
    "        activation = torch.relu(activation)\n",
    "        code = net.encoder_output_layer(activation)\n",
    "        tensor_emb = torch.relu(code)\n",
    "        # finally pass everything into the loss \n",
    "        train_loss = criterion(outputs, batch_features)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += train_loss.item()\n",
    "    loss = loss / len(train_loader)\n",
    "    print(\"epoch : {}/{}, loss = {:.6f}\".format(epoch + 1, epochs, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
