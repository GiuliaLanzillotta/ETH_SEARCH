{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming LDA\n",
    "\n",
    "This notebook produces the final model used to enrich the **graph** or **embedding space**. Note that two separate models will need to be trained, one for each representation. When training the model used for the graph we use lemmatisation and stemming. This is the only difference - the hyperparameters used are the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk import word_tokenize, RegexpTokenizer,PunktSentenceTokenizer, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = pd.read_csv(\"abstracts_eng.csv\")\n",
    "abs_list = list(abstracts['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised = []\n",
    "count = 0\n",
    "for abstract in abs_list:\n",
    "    raw = abstract\n",
    "    tokens = gensim.utils.simple_preprocess(str(raw), deacc=True)\n",
    "    tokenised.append(tokens)\n",
    "    count += len(tokens)\n",
    "print(str(count)+\" tokens created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "for doc in tokenised: \n",
    "    c+=doc\n",
    "len(set(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "print(len(stop_words))\n",
    "stop_words = stop_words + stopwords.words('german')\n",
    "print(len(stop_words))\n",
    "stop_words = stop_words + stopwords.words('french')\n",
    "print(len(stop_words))\n",
    "tokenized_stop = [[word for word in doc if word not in stop_words] for doc in tokenised]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "for doc in tokenized_stop: \n",
    "    c+=doc\n",
    "len(set(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatiser = WordNetLemmatizer()\n",
    "word_stemmer = PorterStemmer()\n",
    "lemmatized = [[lemmatiser.lemmatize(word_stemmer.stem(word)) for word in doc] for doc in tokenized_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "for doc in lemmatized: \n",
    "    c+=doc\n",
    "len(set(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export file with full words, i.e. only tokenized and with stop word removal\n",
    "pickle.dump(tokenized_stop, open( \"collection_cleaned_fullwords.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export fully pre-processed collection\n",
    "pickle.dump(lemmatized, open( \"collection_cleaned.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Important: Select lemmatized dataset when training model for graph and tokenized_stop when training for the embeddings\n",
    "\n",
    "cleaned = lemmatized\n",
    "#cleaned = tokenized_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tomotopy as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 11\n",
    "random.seed(SEED)\n",
    "random.shuffle(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining hyperparameters\n",
    "tw = tp.TermWeight.IDF # term weighting scheme in TermWeight. The default value is TermWeight.ONE\n",
    "k = 100 # number of topics...\n",
    "min_cf=3 # minimum collection frequency of words. Words with a smaller collection frequency than min_cf are excluded from the model. The default value is 0, which means no words are excluded.\n",
    "min_df=0 # minimum document frequency of words. Words with a smaller document frequency than min_df are excluded from the model. The default value is 0, which means no words are excluded\n",
    "rm_top=5 # the number of top words to be removed. If you want to remove too common words from model, you can set this value to 1 or more. The default value is 0, which means no top words are removed.\n",
    "alpha = 0.1 # hyperparameter of Dirichlet distribution for document-topic\n",
    "eta = 0.01 # hyperparameter of Dirichlet distribution for topic-word\n",
    "seed = 41 # random seed\n",
    "model_burn_in = 100 \n",
    "train_updates = 1000\n",
    "train_iter = 10\n",
    "save_path = \"lda_model150.bin\" #.bin format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LDA(documents, k, tw, min_cf=0, min_df=0, rm_top=0, alpha=0.1, eta=0.01, model_burn_in=100, \n",
    "              train_updates = 1000, train_iter = 10, seed=41):\n",
    "    \n",
    "    # instantiate\n",
    "    model = tp.LDAModel(tw=tw, min_df=min_df, min_cf=min_cf, rm_top=rm_top, k=k, alpha = alpha, eta = eta, seed=seed)\n",
    "    \n",
    "    # add documents to model\n",
    "    for doc in documents: model.add_doc(doc)\n",
    "    \n",
    "    # training**\n",
    "    model.burn_in = model_burn_in\n",
    "    # initialising \n",
    "    model.train(iter=0)\n",
    "    print('Num docs:', len(model.docs), ', Vocab size:', len(model.used_vocabs), ', Num words:', model.num_words)\n",
    "    print('Removed top words:', model.removed_top_words)\n",
    "    print('Training...', file=sys.stderr, flush=True)\n",
    "    # actual training \n",
    "    time = []\n",
    "    LLs = []\n",
    "    for i in range(0, train_updates, train_iter):\n",
    "        model.train(train_iter)\n",
    "        if i%100==0:print('Iteration: {}'.format(i))\n",
    "        time.append(i)\n",
    "        LLs.append(model.ll_per_word)\n",
    "    \n",
    "    return model, LLs, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Models Topics from gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining hyperparameters\n",
    "tw = tp.TermWeight.IDF # term weighting scheme in TermWeight. The default value is TermWeight.ONE\n",
    "min_cf=3 # minimum collection frequency of words. Words with a smaller collection frequency than min_cf are excluded from the model. The default value is 0, which means no words are excluded.\n",
    "min_df=0 # minimum document frequency of words. Words with a smaller document frequency than min_df are excluded from the model. The default value is 0, which means no words are excluded\n",
    "rm_top=8 # the number of top words to be removed. If you want to remove too common words from model, you can set this value to 1 or more. The default value is 0, which means no top words are removed.\n",
    "seed = 41 # random seed\n",
    "model_burn_in = 100 \n",
    "train_updates = 1000\n",
    "train_iter = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Subset = 5000\n",
    "parameters = [{'k':175, 'alpha':5.71E-05,'eta':2.82E-05},{'k':150, 'alpha':0.000666667,'eta':2.82E-05},\n",
    "              {'k':125, 'alpha':0.0008,'eta':2.82E-05},{'k':100, 'alpha':0.0001,'eta':2.82E-05},\n",
    "              {'k':75, 'alpha':0.000133333,'eta':2.82E-05}]\n",
    "models = []\n",
    "LLs = []\n",
    "batch = cleaned[10000:3*Subset]\n",
    "for dicti in parameters:\n",
    "    model, loglikes, _ = train_LDA(batch, **dicti, tw=tw, min_cf=min_cf, rm_top=rm_top, \n",
    "                                 model_burn_in=model_burn_in, \n",
    "                                 train_updates = train_updates, train_iter = train_iter, seed = seed)\n",
    "    models.append(model)\n",
    "    LLs.append(loglikes)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating manually results from top 5 grid-search models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(models[2].summary())\n",
    "#print(LLs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = cleaned[15000:]\n",
    "test_inf=[models[3].make_doc(doc) for doc in test]\n",
    "tpc_dist, ll = models[3].infer(test_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(tpc_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_test = np.array(tpc_dist[1])\n",
    "np.argsort(topic_test, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models[3].get_topic_words(76)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "' '.join(cleaned[15001])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally training and storing best models for 4 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "LLs = []\n",
    "num_batches = 4\n",
    "batches = np.array_split(cleaned, num_batches)\n",
    "dicti = {'k':125, 'alpha':0.0008,'eta':2.82E-05}\n",
    "for batch_num in range(0, num_batches):\n",
    "\n",
    "    batch = batches[batch_num].tolist()\n",
    "    model, loglikes, _ = train_LDA(batch, **dicti, tw=tw, min_cf=min_cf, rm_top=rm_top, \n",
    "                                 model_burn_in=model_burn_in, \n",
    "                                 train_updates = train_updates, train_iter = train_iter)\n",
    "    models.append(model)\n",
    "    LLs.append(loglikes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['LDA1batch1.bin','LDA1batch2.bin','LDA1batch3.bin','LDA1batch4.bin']\n",
    "for i,model in enumerate(models):\n",
    "    model.save(names[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
