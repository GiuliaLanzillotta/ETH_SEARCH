{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "\n",
    "#/usr/bin/python\n",
    "from __future__ import print_function\n",
    "import time \n",
    "import argparse\n",
    "import pickle \n",
    "import numpy as np \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Torch-bearer\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from etm import ETM\n",
    "from utils import *\n",
    "from utils import nearest_neighbors, get_topic_coherence, get_topic_diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20494"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts = pd.read_csv(\"abstracts_eng.csv\") #Replace with latest version\n",
    "collection = list(abstracts['abstract'])\n",
    "len(collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training in batches \n",
    "The idea is to simulate the real-time data stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20494\n"
     ]
    }
   ],
   "source": [
    "seed = 11\n",
    "random.seed(seed)\n",
    "random.shuffle(collection)\n",
    "\n",
    "print(len(collection))\n",
    "streaming_batch_size = 20\n",
    "batch1 = collection[:streaming_batch_size]\n",
    "batch2 = collection[streaming_batch_size:2*streaming_batch_size]\n",
    "batch3 = collection[2*streaming_batch_size:3*streaming_batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The network components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The embedding layer\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the embeddings we'll use a pre-trained contextual model \n",
    "# Here we can play a bit to get a sense of its working \n",
    "\n",
    "#!pip install transformers \n",
    "from transformers import DistilBertTokenizerFast, DistilBertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why DistilBer? <br>\n",
    "The DistilBERT model was proposed in the blog post [Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT](https://medium.com/huggingface/distilbert-8cf3380435b5), and the paper DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT’s performances as measured on the GLUE language understanding benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased', return_dict=True)\n",
    "inputs = tokenizer(\"Hello, we are three cool data scientists\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 768)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state[0].data.cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now there's a bit of work to do to prepare the embedding matrix. <br>\n",
    "We want to use Bert to get the embedding for each word in our corpus. However, being Bert a contextual embedding we could end up with more than one embedding vector for each word. To solve this problem we associate each word a different token for each different embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will work during the developing phase on a subset of the collection because \n",
    "# otherwise the memory requirements will be to high.\n",
    "# We should later apply the same computation that I apply here to all the batches inside the \n",
    "# collection.\n",
    "subset_size = 10 # len(collection)\n",
    "collection_subset = collection[0:subset_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_collection = tokenizer(collection_subset, return_tensors=\"pt\", truncation=False, padding=True)\n",
    "tokenised_collection[\"input_ids\"][0] # we have added padding so that we can process the whole collection in a batch - we gain in speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_collection = model(**tokenised_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally start to build our vocabulary. <br>\n",
    "We'll look at each word and add it to the vocabulary only if there's not already the same embedding_vector in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = embedded_collection.last_hidden_state.size()\n",
    "padding_size = size[1]\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {} # vocabulary in the form (int,word) pairs\n",
    "idx2bertIdx = {} # each index in our vocabulary is mapped to the Bert vocabulary index \n",
    "set_of_embeddings = set()\n",
    "\n",
    "\n",
    "idx = 0 # initialise dictionary index \n",
    "new_token_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the idea of this for loop is the following: \n",
    "\n",
    "# input : [[4535,564,2342,...],[423423,32432,...],...] sequence of bert tokens for our collection \n",
    "#         [[embedding1, embedding2, ...],[...],...] and respective embedding vectors \n",
    "\n",
    "\n",
    "# output: [[0, 1, 2, 1, 4, ...], [0, 4, ...], ...] sequence of our tokens for our collection \n",
    "#         [[ embedding of token 0 ]\n",
    "#          [ embedding of token 1 ]\n",
    "#          [ embedding of token 2 ]\n",
    "#          [ embedding of token 3 ]\n",
    "#          [ embedding of token 4 ]\n",
    "#                   \n",
    "#                    ...\n",
    "#\n",
    "#          [ embedding of last token ]]\n",
    "\n",
    "# Note: different tokens in our vocabulary (say 1 and 3) can refer to the same word if the embedding vector is different \n",
    "\n",
    "# The model will take as input the new tokens' collection (in some processed form maybe) and the embedding matrix\n",
    "start = time.time()\n",
    "for i in range(subset_size):\n",
    "    t1 = time.time()\n",
    "    embedded_doc = embedded_collection.last_hidden_state[i][tokenised_collection[\"attention_mask\"][i].bool()] # removing padding using the attention mask \n",
    "    tokens_ids = tokenised_collection[\"input_ids\"][i]\n",
    "    new_token_ids_doc = []\n",
    "    for j,emb_vector in enumerate(embedded_doc):\n",
    "        token_id = tokens_ids[j].cpu().numpy() # bert current token \n",
    "        word = tokenizer.convert_ids_to_tokens([token_id])[0] # corresponding word\n",
    "        bool_list = [torch.all(torch.eq(emb_vector, other)) for other in set_of_embeddings]\n",
    "        if not any(bool_list): \n",
    "            # add new embedding to the set \n",
    "            set_of_embeddings.add(emb_vector)\n",
    "            # increase the index and save the word in the dictionary \n",
    "            idx2word[idx] = word # save it in our vocabulary\n",
    "            idx2bertIdx[idx] = token_id # save in id to bert id mapping\n",
    "            new_token_ids_doc += [idx]\n",
    "            idx += 1\n",
    "        else: # find the right id for the word and add it to our new tokenisation\n",
    "            word_id = list(idx2word.values()).index(word)\n",
    "            new_token_ids_doc += [word_id]\n",
    "    new_token_ids += [new_token_ids_doc]\n",
    "    t2 = time.time()\n",
    "    print(\"Document \"+str(i)+\" done. Time: \"+str(round(t2-t1,2))+\" s.\")\n",
    "end = time.time()\n",
    "print(\"Total time: \"+str(round(end-start,2))+\" s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set_of_embeddings) == len(idx2word.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now we write the loop that will process one batch in our collection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add 2 functionalities here: \n",
    "- **Cosine similarity clustering**. We add a filtering of the embedding vectors for each word based on cosine similarity, i.e. we only keep one of the vectors that are \"*too close*\" to each other. \n",
    "- **Stop-words removal**. We avoid to represent the stop-word in our embedding since they wouldn't anyway carry meaning for the topic and they would occupy a lot of memory since for [this](http://ai.stanford.edu/blog/contextual/) article they tend to be the most context dependent words in contextual embeddings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english') # note: the words in this list are only lower case but distilbert tokenizer incorporates lower casing so we should be fine! read more here: https://huggingface.co/transformers/_modules/transformers/tokenization_distilbert_fast.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subset_cosine(doc_subset, tokenizer, model, set_of_embeddings, \n",
    "                          idx2word, new_token_ids, threshold=0.9):\n",
    "    \"\"\" \n",
    "    Processing of a subset of the batch using cosine similarity clustering. \n",
    "    \n",
    "    Parameters \n",
    "    ----- \n",
    "    doc_subset: list of documents (aka list of strings)\n",
    "    tokenizer: instance of Bert tokenizer \n",
    "    model: instance of Bert model \n",
    "    set_of_embeddings: set containing the embedding vectors already in the vocabulary \n",
    "    idx2word: vocabulary mapping our token ids to the corresponding word. \n",
    "            Notice that each word can be mapped to multiple token ids.\n",
    "    new_token_ids: representation of the collection with our token ids. \n",
    "    threshold: cosine similarity threshold. \n",
    "            Vectors with cosine similarity above the threshold are considered equal. \n",
    "            \n",
    "    Returns \n",
    "    -----\n",
    "    Updated versions of set_of_embeddings, idx2word and new_token_ids\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    tokenised_collection = tokenizer(doc_subset, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    print(\"tokenisation done\")\n",
    "    embedded_collection = model(**tokenised_collection)\n",
    "    print(\"embeddings done\")\n",
    "    \n",
    "    \n",
    "    ##  preparing the variables we need ---------\n",
    "    if len(idx2word) == 0:idx = 0\n",
    "    else:idx = len(set_of_embeddings)\n",
    "        \n",
    "    cos = torch.nn.CosineSimilarity(dim = 0)\n",
    "    \n",
    "    subset_size = len(doc_subset)\n",
    "    start = time.time()\n",
    "    \n",
    "    ## processing the collection document by document ----------\n",
    "    for i in range(subset_size):\n",
    "        t1 = time.time()\n",
    "        embedded_doc = embedded_collection.last_hidden_state[i][tokenised_collection[\"attention_mask\"][i].bool()] # removing padding using the attention mask \n",
    "        tokens_ids = tokenised_collection[\"input_ids\"][i]\n",
    "        new_token_ids_doc = []\n",
    "        for j,emb_vector in enumerate(embedded_doc):\n",
    "            \n",
    "            token_id = tokens_ids[j].cpu().numpy() # bert current token \n",
    "            word = tokenizer.convert_ids_to_tokens([token_id])[0] # corresponding word\n",
    "            # jump to the next token if the word is a stopword \n",
    "            if word in stop_words: continue \n",
    "            \n",
    "            # cosine similarity clustering \n",
    "            bool_list = [cos(emb_vector, other) >= threshold for other in set_of_embeddings] \n",
    "            \n",
    "            if (not any(bool_list)) or (word not in idx2word.values()): # we add the embedding anyway if we haven't encountered that word previously \n",
    "                # add new embedding to the set \n",
    "                set_of_embeddings.add(emb_vector)\n",
    "                # increase the index and save the word in the dictionary \n",
    "                idx2word[idx] = word # save it in our vocabulary\n",
    "                new_token_ids_doc += [idx]\n",
    "                idx += 1\n",
    "            else: # find the right id for the word and add it to our new tokenisation\n",
    "                word_id = list(idx2word.values()).index(word)\n",
    "                new_token_ids_doc += [word_id]\n",
    "                \n",
    "        new_token_ids += [new_token_ids_doc]\n",
    "        t2 = time.time()\n",
    "        if i%(subset_size//3)==0:print(\"Document \"+str(i)+\" done. Time: \"+str(round(t2-t1,2))+\" s.\")\n",
    "            \n",
    "    end = time.time()\n",
    "    print(\"Total time for the subset: \"+str(round(end-start,2))+\" s.\")\n",
    "    \n",
    "    return set_of_embeddings, idx2word, new_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING on a few example sentences \n",
    "idx2word = {} \n",
    "set_of_embeddings = set()\n",
    "idx = 0 \n",
    "new_token_ids = []\n",
    "\n",
    "sen1 = \"This mouse loves cheese\"\n",
    "sen2 = \"I am using this mouse as a pointer\"\n",
    "doc = [sen1, sen2]\n",
    "tokenised_collection = tokenizer(doc, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "embedded_collection = model(**tokenised_collection)\n",
    "set_of_embeddings, idx2word, new_token_ids = process_subset_cosine(doc, tokenizer, model, set_of_embeddings, idx2word, idx2bertIdx, new_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch, subset_size, tokeniser, model):\n",
    "    \"\"\" Processing of a batch of documents in the collection. \"\"\"\n",
    "    \n",
    "    ## initialisation \n",
    "    idx2word = {} \n",
    "    set_of_embeddings = set()\n",
    "    idx = 0 \n",
    "    new_token_ids = []\n",
    "    subset_size = 25 \n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    ## processing the batch one subset at a time\n",
    "    for s in range(0,batch_size,subset_size):\n",
    "        print(\"Processing subset \"+str(s//subset_size + 1))\n",
    "        if s+subset_size < len(batch):batch_subset = batch[s:s+subset_size]\n",
    "        else: batch_subset = batch[s:]\n",
    "        set_of_embeddings, idx2word, new_token_ids = process_subset_cosine(batch_subset, tokenizer, model, \n",
    "                                                                           set_of_embeddings, idx2word, \n",
    "                                                                           new_token_ids, threshold=0.9)\n",
    "        print(\"Number of word vectors so far: \"+str(len(idx2word)))    \n",
    "        print()\n",
    "        \n",
    "    end = time.time()\n",
    "    print(\"Total time: \"+str(round(end-start,2))+\" s.\")\n",
    "    \n",
    "    return set_of_embeddings, idx2word, new_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subset 1\n",
      "tokenisation done\n",
      "embeddings done\n",
      "Document 0 done. Time: 0.7 s.\n",
      "Document 6 done. Time: 26.51 s.\n",
      "Document 12 done. Time: 47.25 s.\n",
      "Document 18 done. Time: 64.05 s.\n",
      "Total time for the subset: 596.8 s.\n",
      "Number of word vectors so far: 2854\n",
      "\n",
      "Total time: 613.63 s.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased', return_dict=True)\n",
    "set_of_embeddings, idx2word, new_token_ids = process_batch(batch2, 25, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2854, 768])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the embedding matrix from complete 'set_of_embeddings'\n",
    "embedding = torch.stack(list(set_of_embeddings))\n",
    "embedding.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "So now we finally have a vocabulary and an embedding matrix: everything is ready for our model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2854"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(idx2word)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the get_batch function so as to get the documents x vocabulary matrix \n",
    "# required by the model starting from the complete 'new_token_ids' list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(corpus, ind, vocab_size, device, emsize=300):\n",
    "    \"\"\"\n",
    "    This function takes as input a list of tokenised documents (corpus)\n",
    "    and the indices of the documents in the batch (ind)\n",
    "    and returns as output the torch tensor to feed into the net. \n",
    "    The list of documents defines the batch to work on. \n",
    "    \"\"\"\n",
    "    batch_size = len(ind)\n",
    "    data_batch = np.zeros((batch_size, vocab_size))\n",
    "    \n",
    "    for i, doc_id in enumerate(ind):\n",
    "        doc = corpus[doc_id]\n",
    "        L = len(doc)\n",
    "        if doc_id != -1:\n",
    "            for word_id in doc:\n",
    "                counts = doc.count(word_id)\n",
    "                data_batch[i, word_id] = counts\n",
    "    data_batch = torch.from_numpy(data_batch).float().to(device)\n",
    "    return data_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "### model-related arguments\n",
    "num_topics = 5\n",
    "rho_size = 768 # dimension of rho \n",
    "emb_size = 768 # dimension of embeddings \n",
    "t_hidden_size = 600 # dimension of hidden space of q(theta)\n",
    "theta_act = 'relu' # either tanh, softplus, relu, rrelu, leakyrelu, elu, selu, glu\n",
    "train_embeddings = False\n",
    "seed = 11\n",
    "\n",
    "### optimization-related arguments\n",
    "lr = 0.005\n",
    "lr_factor = 4.0 #divide learning rate by this\n",
    "epochs = 20 \n",
    "mode = \"train\"\n",
    "enc_drop = 0.0 # dropout rate on encoder\n",
    "clip = 0.0 # gradient clipping\n",
    "nonmono = 10 # number of bad hits allowed ...?\n",
    "weight_decay = 1.2e-6\n",
    "anneal_lr = False # whether to anneal the learning rate or not\n",
    "bow_norm = True # normalize the bows or not \n",
    "_optimizer = \"adam\"\n",
    "\n",
    "### evaluation, visualization, and logging-related arguments\n",
    "num_docs_train = 18\n",
    "num_words = 10 #number of words for topic viz'\n",
    "log_interval = 2 #when to log training\n",
    "visualize_every = 10 #when to visualize results\n",
    "tc = False # whether to compute topic coherence or not\n",
    "td = False # whether to compute topic diversity or not\n",
    "\n",
    "### data and file related arguments\n",
    "save_path = './results'\n",
    "batch_size = 20\n",
    "eval_batch_size = 20 #input batch size for evaluation\n",
    "load_from = \"\" #the name of the ckpt to run evaluation from\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Setting the random seed \n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "if mode == 'eval':\n",
    "    ckpt = load_from \n",
    "else:\n",
    "    ckpt = os.path.join(save_path, \n",
    "        'etm_K_{}_Htheta_{}_Optim_{}_Clip_{}_ThetaAct_{}_Lr_{}_Bsz_{}_RhoSize_{}_trainEmbeddings_{}'.format(\n",
    "         num_topics, t_hidden_size, _optimizer, clip, theta_act, lr, batch_size, rho_size, train_embeddings))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _optimizer == 'adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=lr_factor)\n",
    "elif _optimizer == 'adagrad':\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=lr, weight_decay=lr_factor)\n",
    "elif _optimizer == 'adadelta':\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=lr, weight_decay=lr_factor)\n",
    "elif _optimizer == 'rmsprop':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=lr_factor)\n",
    "elif _optimizer == 'asgd':\n",
    "    optimizer = optim.ASGD(model.parameters(), lr=lr, t0=0, lambd=0., weight_decay=lr_factor)\n",
    "else:\n",
    "    print('Defaulting to vanilla SGD')\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.0, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (alphas): Linear(in_features=768, out_features=5, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=2854, out_features=600, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=600, out_features=600, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=600, out_features=5, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=600, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "etm_model = ETM(num_topics = num_topics, \n",
    "            vocab_size = vocab_size, \n",
    "            t_hidden_size = t_hidden_size, \n",
    "            rho_size = rho_size, \n",
    "            emsize = emb_size, \n",
    "            theta_act = theta_act, \n",
    "            embeddings = embedding,\n",
    "            train_embeddings = train_embeddings, \n",
    "            enc_drop = enc_drop).to(device)\n",
    "\n",
    "print('model: {}'.format(etm_model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = new_token_ids[:num_docs_train]\n",
    "test_corpus = new_token_ids[num_docs_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, corpus, num_docs_train=num_docs_train, batch_size=batch_size, vocab_size=vocab_size, \n",
    "          bow_norm=bow_norm, clip=clip, log_interval=log_interval):\n",
    "    \"\"\" Just the training function ... \"\"\"\n",
    "    \n",
    "    model.train() #setting the model in training mode\n",
    "    # preparing all the data structures \n",
    "    acc_loss = 0\n",
    "    acc_kl_theta_loss = 0\n",
    "    cnt = 0\n",
    "    indices = torch.randperm(num_docs_train)\n",
    "    indices = torch.split(indices, batch_size)\n",
    "    \n",
    "    for idx, ind in enumerate(indices): # all our batches \n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        data_batch = get_batch(corpus, ind, vocab_size, device)\n",
    "        sums = data_batch.sum(1).unsqueeze(1) # what are we summing ?? \n",
    "        \n",
    "        # maybe normalising the input \n",
    "        if bow_norm: normalized_data_batch = data_batch / sums\n",
    "        else: normalized_data_batch = data_batch\n",
    "        # loss on the batch \n",
    "        recon_loss, kld_theta = model(data_batch, normalized_data_batch)\n",
    "        total_loss = recon_loss + kld_theta\n",
    "        total_loss.backward() # compute backpropagation\n",
    "        # maybe clip the gradient \n",
    "        if clip > 0: torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step() # finally update the weights \n",
    "        # accumulate the total loss \n",
    "        acc_loss += torch.sum(recon_loss).item()\n",
    "        acc_kl_theta_loss += torch.sum(kld_theta).item()\n",
    "        cnt += 1\n",
    "        \n",
    "        # visualisation/print time! ('cur' stands for current ...)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            cur_loss = round(acc_loss / cnt, 2) \n",
    "            cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n",
    "            cur_real_loss = round(cur_loss + cur_kl_theta, 2)\n",
    "            print('Epoch: {} .. batch: {}/{} .. LR: {} .. KL_theta: {} .. Rec_loss: {} .. NELBO: {}'.format(\n",
    "                epoch, idx, len(indices), optimizer.param_groups[0]['lr'], cur_kl_theta, cur_loss, cur_real_loss))\n",
    "    \n",
    "    # Wrapping up the results of the epoch! \n",
    "    cur_loss = round(acc_loss / cnt, 2) \n",
    "    cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n",
    "    cur_real_loss = round(cur_loss + cur_kl_theta, 2)\n",
    "    print('*'*100)\n",
    "    print('Epoch----->{} .. LR: {} .. KL_theta: {} .. Rec_loss: {} .. NELBO: {}'.format(\n",
    "            epoch, optimizer.param_groups[0]['lr'], cur_kl_theta, cur_loss, cur_real_loss))\n",
    "    print('*'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(m, num_topics=num_topics, num_words=num_words, \n",
    "              vocab=idx2word, show_emb=True, tokenizer=tokenizer, bert_model=model):\n",
    "    \"\"\" This is a cool visualisation function. \n",
    "    Takes as input the model so far and shows the discovered embeddings! \"\"\"\n",
    "    \n",
    "    # We're going to save our results here \n",
    "    # TODO: parametrize this path \n",
    "    if not os.path.exists('./results'):\n",
    "        os.makedirs('./results')\n",
    "\n",
    "    m.eval() #set the net in evaluation mode \n",
    "    # set a few words to query \n",
    "    queries = ['andrew', 'computer', 'sports', 'religion', 'man', 'love', \n",
    "                'intelligence', 'money', 'politics', 'health', 'people', 'family']\n",
    "\n",
    "    ## visualize topics using monte carlo (sampling from the posterior I guess)\n",
    "    with torch.no_grad(): # no gradients computation - makes forward pass lighter \n",
    "        print('#'*100)\n",
    "        print('Visualize topics...')\n",
    "        topics_words = []\n",
    "        gammas = m.get_beta() # topics distributions \n",
    "        for k in range(num_topics):\n",
    "            gamma = gammas[k]\n",
    "            top_words = list(gamma.cpu().numpy().argsort()[-num_words+1:][::-1])\n",
    "            topic_words = [vocab[a] for a in top_words] \n",
    "            topics_words.append(' '.join(topic_words))\n",
    "            print('Topic {}: {}'.format(k, topic_words))\n",
    "\n",
    "        if show_emb:\n",
    "            ## visualize word embeddings by using V to get nearest neighbors\n",
    "            print('#'*100)\n",
    "            print('Visualize word embeddings by using output embedding matrix')\n",
    "            \n",
    "            # extract the embeddings from the model! \n",
    "            try:embeddings = m.rho.weight  # Vocab_size x E\n",
    "            except:embeddings = m.rho         # Vocab_size x E\n",
    "            \n",
    "            \n",
    "            for word in queries:\n",
    "                # extracting Bert representation of the word\n",
    "                inputs = tokenizer(word, return_tensors=\"pt\")\n",
    "                outputs = bert_model(**inputs).last_hidden_state[0]\n",
    "                if outputs.size()[0]>1: #aggregate\n",
    "                    outputs = torch.sum(outputs, dim=0)\n",
    "                nns = utils.nearest_neighbors(q=outputs, \n",
    "                                        embeddings=embeddings, vocab=list(vocab.values()))\n",
    "                print('word: {} .. neighbors: {}'.format(word, nns)) # utility function \n",
    "            print('#'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(m, corpus, num_docs_test, tc=tc, td=td, \n",
    "             eval_batch_size=eval_batch_size, vocab_size=vocab_size, \n",
    "             bow_norm=bow_norm):\n",
    "    \"\"\"\n",
    "    Evaluating the trained model on the test set using either perplexity, or coherence and diversity. \n",
    "    Compute perplexity on document completion.\n",
    "    \"\"\"\n",
    "    \n",
    "    m.eval() # set model in evaluation mode \n",
    "    with torch.no_grad():\n",
    "        indices = torch.split(torch.tensor(range(num_docs_test)), eval_batch_size)\n",
    "        \n",
    "        ## get \\beta here\n",
    "        beta = m.get_beta()\n",
    "\n",
    "        ### do dc and tc here\n",
    "        acc_loss = 0\n",
    "        cnt = 0\n",
    "        \n",
    "        for idx, ind in enumerate(indices):\n",
    "            data_batch = get_batch(corpus, ind, vocab_size, device)\n",
    "            sums = data_batch.sum(1).unsqueeze(1)\n",
    "            if bow_norm: normalized_data_batch = data_batch / sums\n",
    "            else: normalized_data_batch = data_batch\n",
    "                \n",
    "            ## get theta\n",
    "            theta, _ = m.get_theta(normalized_data_batch)\n",
    "            ## get prediction loss\n",
    "            res = torch.mm(theta, beta)\n",
    "            preds = torch.log(res)\n",
    "            recon_loss = -(preds * data_batch).sum(1)\n",
    "            loss = recon_loss / sums.squeeze()\n",
    "            loss = loss.mean().item()\n",
    "            acc_loss += loss\n",
    "            cnt += 1\n",
    "        \n",
    "        # Calculate final loss \n",
    "        cur_loss = acc_loss / cnt\n",
    "        ppl_dc = round(math.exp(cur_loss), 1)\n",
    "        print('*'*100)\n",
    "        print('{} Doc Completion PPL: {}'.format(source.upper(), ppl_dc))\n",
    "        print('*'*100)\n",
    "        \n",
    "        \n",
    "        if tc or td: # calculate topic coherence or topic diversity \n",
    "            beta = beta.data.cpu().numpy()\n",
    "            if tc:\n",
    "                print('Computing topic coherence...')\n",
    "                get_topic_coherence(beta, train_tokens, vocab)\n",
    "            if td:\n",
    "                print('Computing topic diversity...')\n",
    "                get_topic_diversity(beta, 25)\n",
    "        return ppl_dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code that actually launches the training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "utils = importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Visualizing model quality before training...\n",
      "####################################################################################################\n",
      "Visualize topics...\n",
      "Topic 0: ['community', 'production', 'around', '##le', 'prices', '-', 'arrest', 'american', 'rates']\n",
      "Topic 1: [')', 'bottom', 'conditions', '##irs', 'hypothesis', 'elbow', 'due', '##tify', '##ts']\n",
      "Topic 2: ['function', 'date', '(', '##ening', '##yn', 'performance', 'inside', '##c', 'com']\n",
      "Topic 3: ['stress', '(', 'structures', '’', ',', '##ors', 'aged', 'data', '##cl']\n",
      "Topic 4: ['##s', '##uti', 'retain', 'greater', 'new', 'convergence', 'indicate', '##fp', 'compared']\n",
      "####################################################################################################\n",
      "Visualize word embeddings by using output embedding matrix\n",
      "vectors:  (2854, 768)\n",
      "query:  (768,)\n",
      "word: andrew .. neighbors: ['uniform', ')', '##agne', '##fen', 'com', 'loading', 'oils', 'discrete', 'wave', 'different', '(', 'grazing', 'million', 'call', '##ate', 'consolidated', 'city', 'architects', 'potential', '-']\n",
      "vectors:  (2854, 768)\n",
      "query:  (768,)\n",
      "word: computer .. neighbors: [')', 'uniform', '##agne', '##fen', 'com', 'wave', 'oils', 'loading', 'discrete', '(', 'different', 'grazing', 'million', 'call', 'bulk', 'city', 'consolidated', '-', 'potentially', 'potential']\n",
      "vectors:  (2854, 768)\n",
      "query:  (768,)\n",
      "word: sports .. neighbors: ['##fen', ')', 'uniform', 'loading', '##agne', 'wave', 'oils', 'com', 'discrete', 'different', '(', 'grazing', 'call', 'million', 'criterion', 'potentially', '##ate', 'consolidated', 'bulk', 'potential']\n",
      "vectors:  (2854, 768)\n",
      "query:  (768,)\n",
      "word: religion .. neighbors: ['##fen', 'uniform', ')', 'loading', 'oils', 'wave', '##agne', 'com', 'discrete', '(', 'different', 'call', 'grazing', 'criterion', 'potentially', 'consolidated', 'million', '##ate', '-', 'city']\n",
      "vectors:  (2854, 768)\n",
      "query:  (768,)\n",
      "word: man .. neighbors: [')', '##agne', 'uniform', '##fen', 'com', 'loading', 'discrete', 'oils', 'wave', 'different', 'grazing', '(', 'million', 'consolidated', 'architects', 'city', '-', 'call', 'potential', 'force']\n",
      "vectors:  (2854, 768)\n",
      "query:  (768,)\n",
      "word: love .. neighbors: [')', 'uniform', '##agne', '##fen', 'com', 'oils', 'discrete', 'loading', 'wave', 'different', '(', 'grazing', 'million', 'consolidated', 'call', 'potential', 'force', 'architects', 'city', 'potentially']\n",
      "vectors:  (2854, 768)\n",
      "query:  (768,)\n",
      "word: intelligence .. neighbors: ['uniform', '##fen', ')', 'loading', 'oils', '##agne', 'wave', 'com', 'discrete', 'different', '(', 'grazing', 'call', 'potentially', 'million', '##ate', 'criterion', 'bulk', 'consolidated', '-']\n",
      "vectors:  (2854, 768)\n",
      "query:  (768,)\n",
      "word: money .. neighbors: ['##fen', 'uniform', ')', 'oils', 'loading', 'wave', '##agne', 'com', 'discrete', 'different', '(', 'grazing', 'call', 'million', 'criterion', '##ate', 'potentially', 'consolidated', '-', 'city']\n",
      "vectors:  (2854, 768)\n",
      "query:  (768,)\n",
      "word: politics .. neighbors: ['##fen', 'wave', ')', 'uniform', 'loading', 'oils', '##agne', 'com', 'discrete', 'different', '(', 'call', 'grazing', 'potentially', '##ate', 'criterion', 'consolidated', 'million', 'bulk', 'force']\n",
      "vectors:  (2854, 768)\n",
      "query:  (768,)\n",
      "word: health .. neighbors: ['##fen', 'loading', 'oils', 'wave', ')', 'uniform', '##agne', 'com', 'discrete', 'different', '(', 'call', 'potentially', 'grazing', '##ate', 'criterion', 'million', 'consolidated', 'kerr', 'force']\n",
      "vectors:  (2854, 768)\n",
      "query:  (768,)\n",
      "word: people .. neighbors: [')', 'uniform', '##fen', '##agne', 'loading', 'wave', 'oils', 'com', 'discrete', 'different', '(', 'grazing', 'million', 'call', 'consolidated', 'criterion', 'potentially', '-', '##ate', 'city']\n",
      "vectors:  (2854, 768)\n",
      "query:  (768,)\n",
      "word: family .. neighbors: [')', 'uniform', '##agne', '##fen', 'com', 'wave', 'loading', 'oils', 'discrete', 'different', '(', 'grazing', 'million', 'call', 'consolidated', '-', 'potentially', 'city', 'architects', 'potential']\n",
      "####################################################################################################\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-287-d0fcbbdb16d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metm_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_corpus\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mnum_docs_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_token_ids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnum_docs_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mval_ppl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metm_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_corpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_docs_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# evaluate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-283-2e5a4e54dfa4>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, epoch, corpus, num_docs_train, batch_size, vocab_size, bow_norm, clip, log_interval)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mrecon_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkld_theta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalized_data_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecon_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mkld_theta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mtotal_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# compute backpropagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[1;31m# maybe clip the gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mclip\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\giulia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\giulia\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "# Initialising the data structures \n",
    "best_epoch = 0\n",
    "best_val_ppl = 1e9\n",
    "all_val_ppls = []\n",
    "\n",
    "# Let's get a sense of how bad the model is before training \n",
    "print('\\n')\n",
    "print('Visualizing model quality before training...')\n",
    "visualize(etm_model)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs):\n",
    "    \n",
    "    train(etm_model, epoch, train_corpus) # train \n",
    "    num_docs_test = len(new_token_ids) - num_docs_train\n",
    "    val_ppl = evaluate(etm_model, test_corpus, num_docs_test) # evaluate \n",
    "    \n",
    "    # only saving the model if it's the best so far \n",
    "    if val_ppl < best_val_ppl: \n",
    "        with open(ckpt, 'wb') as f:\n",
    "            torch.save(model, f)\n",
    "        best_epoch = epoch \n",
    "        best_val_ppl = val_ppl\n",
    "        \n",
    "    else:\n",
    "        ## check whether to anneal lr (aka decreasing it by a constant factor )\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        if args.anneal_lr and (len(all_val_ppls) > args.nonmono and val_ppl > min(all_val_ppls[:-args.nonmono]) and lr > 1e-5):\n",
    "            optimizer.param_groups[0]['lr'] /= args.lr_factor\n",
    "            \n",
    "    #maybe visualise \n",
    "    if epoch % args.visualize_every == 0:\n",
    "        visualize(etm_model)\n",
    "        \n",
    "    #save perplexities \n",
    "    all_val_ppls.append(val_ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the code that launches the final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained model and evaluate it  \n",
    "with open(ckpt, 'rb') as f:\n",
    "    etm_model = torch.load(f)\n",
    "etm_model = etm_model.to(device)\n",
    "etm_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    ## ---------------\n",
    "    ## Idea : get document completion perplexities\n",
    "    test_ppl = evaluate(etm_model) \n",
    "\n",
    "    ## ----------------\n",
    "    ## Idea : get most used topics\n",
    "    indices = torch.tensor(range(num_docs_train)) # training documents indices \n",
    "    indices = torch.split(indices, batch_size)\n",
    "    #just initialising data structures \n",
    "    thetaAvg = torch.zeros(1, args.num_topics).to(device)\n",
    "    thetaWeightedAvg = torch.zeros(1, args.num_topics).to(device)\n",
    "    cnt = 0\n",
    "    for idx, ind in enumerate(indices):\n",
    "        data_batch = data.get_batch(ind, args.vocab_size, device) # TODO: fix here \n",
    "        sums = data_batch.sum(1).unsqueeze(1) \n",
    "        cnt += sums.sum(0).squeeze().cpu().numpy()\n",
    "        # maybe normalise \n",
    "        if args.bow_norm:normalized_data_batch = data_batch / sums\n",
    "        else: normalized_data_batch = data_batch\n",
    "        # get the theta \n",
    "        theta, _ = model.get_theta(normalized_data_batch)\n",
    "        thetaAvg += theta.sum(0).unsqueeze(0) / args.num_docs_train\n",
    "        weighed_theta = sums * theta\n",
    "        thetaWeightedAvg += weighed_theta.sum(0).unsqueeze(0)\n",
    "        # let's print the progress as we go \n",
    "        if idx % 100 == 0 and idx > 0:\n",
    "            print('batch: {}/{}'.format(idx, len(indices)))\n",
    "    # finally the results are in \n",
    "    thetaWeightedAvg = thetaWeightedAvg.squeeze().cpu().numpy() / cnt\n",
    "    print('\\nThe 10 most used topics are {}'.format(thetaWeightedAvg.argsort()[::-1][:10]))\n",
    "\n",
    "    # Now we show the topics\n",
    "    # A nice visualisation is always welcome \n",
    "    beta = model.get_beta()\n",
    "    topic_indices = list(np.random.choice(num_topics, 10)) # 10 random topics\n",
    "    print('\\n')\n",
    "    for k in range(num_topics):#topic_indices:\n",
    "        gamma = beta[k]\n",
    "        top_words = list(gamma.cpu().numpy().argsort()[-num_words+1:][::-1])\n",
    "        topic_words = [vocab[a] for a in top_words]\n",
    "        print('Topic {}: {}'.format(k, topic_words))\n",
    "\n",
    "    # Why not, also showing a few embeddings \n",
    "    if train_embeddings:\n",
    "        # get embeddings from the model \n",
    "        try:rho_etm = model.rho.weight.cpu()\n",
    "        except:rho_etm = model.rho.cpu()\n",
    "        queries = ['andrew', 'woman', 'computer', 'sports', 'religion', 'man', 'love', \n",
    "                        'intelligence', 'money', 'politics', 'health', 'people', 'family']\n",
    "        print('\\n')\n",
    "        print('ETM embeddings...')\n",
    "        for word in queries:\n",
    "            print('word: {} .. etm neighbors: {}'.format(word, nearest_neighbors(word, rho_etm, vocab)))\n",
    "        print('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
