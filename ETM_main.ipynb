{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "\n",
    "#/usr/bin/python\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import pickle \n",
    "import numpy as np \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import sys\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Torch-bearer\n",
    "#import torch\n",
    "# from torch import nn, optim\n",
    "# from torch.nn import functional as F\n",
    "\n",
    "#from etm import ETM\n",
    "#from utils import nearest_neighbors, get_topic_coherence, get_topic_diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"../abstracts_processed.csv\"\n",
    "with open(input_path, \"rb\") as fp:   \n",
    "    # Unpickling\n",
    "    documents = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20494\n"
     ]
    }
   ],
   "source": [
    "# Train and test (70-30)\n",
    "print(len(documents))\n",
    "set1_size = int(0.7*len(documents)) \n",
    "set2_size = int(0.3*len(documents)) \n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "set1_docs = documents[0:set1_size]\n",
    "set2_docs = documents[set1_size:]\n",
    "\n",
    "len(set1_docs) + len(set2_docs) == len(documents)\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "def prepare_gensim_input(docs, old_dict=None):\n",
    "    # TODO: add possibility to remove extremes (too frequent/rare words)\n",
    "    dictionary = Dictionary(docs)\n",
    "    if old_dict is not None: \n",
    "        old_dict_copy = deepcopy(old_dict)\n",
    "        old_dict_copy.merge_with(dictionary)\n",
    "        dictionary = old_dict_copy\n",
    "    corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in docs] # bag of words corpus \n",
    "    return dictionary, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary1, corpus1 = prepare_gensim_input(set1_docs) #train\n",
    "dictionary2, corpus2 = prepare_gensim_input(set2_docs) #test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(corpus, ind, vocab_size):#, device, emsize=300):\n",
    "    \"\"\"fetch input data by batch.\"\"\"\n",
    "    batch_size = len(ind)\n",
    "    data_batch = np.zeros((batch_size, vocab_size))\n",
    "    \n",
    "    for i, doc_id in enumerate(ind):\n",
    "        doc = corpus[doc_id]\n",
    "        L = len(doc)\n",
    "        if doc_id != -1:\n",
    "            for unique_word in doc:\n",
    "                data_batch[i, unique_word[0]] = unique_word[1]\n",
    "    #data_batch = torch.from_numpy(data_batch).float().to(device)\n",
    "    return data_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_batch(corpus1,[0,3,6,8],len(dictionary1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py @ file:///C:/ci/absl-py_1603893176799/work\n",
      "alabaster==0.7.12\n",
      "anaconda-client==1.7.2\n",
      "anaconda-navigator==1.10.0\n",
      "anaconda-project==0.8.3\n",
      "argon2-cffi @ file:///C:/ci/argon2-cffi_1596828549974/work\n",
      "arviz==0.7.0\n",
      "asn1crypto @ file:///tmp/build/80754af9/asn1crypto_1596577642040/work\n",
      "astor==0.8.1\n",
      "astroid @ file:///C:/ci/astroid_1592481955828/work\n",
      "astropy==4.0.2\n",
      "astunparse==1.6.3\n",
      "async-generator==1.10\n",
      "atomicwrites==1.4.0\n",
      "attrs @ file:///tmp/build/80754af9/attrs_1600298409949/work\n",
      "Babel==2.8.0\n",
      "backcall==0.2.0\n",
      "backports.functools-lru-cache==1.6.1\n",
      "backports.os==0.1.1\n",
      "backports.shutil-get-terminal-size==1.0.0\n",
      "backports.tempfile==1.0\n",
      "backports.weakref==1.0.post1\n",
      "beautifulsoup4 @ file:///tmp/build/80754af9/beautifulsoup4_1601924105527/work\n",
      "bidict==0.18.3\n",
      "biosppy==0.6.1\n",
      "bitarray @ file:///C:/ci/bitarray_1603203287144/work\n",
      "bkcharts==0.2\n",
      "bleach @ file:///tmp/build/80754af9/bleach_1600439572647/work\n",
      "bokeh @ file:///C:/ci/bokeh_1603297932359/work\n",
      "boto==2.49.0\n",
      "Bottleneck==1.3.2\n",
      "brotlipy==0.7.0\n",
      "cachetools==4.0.0\n",
      "certifi==2020.6.20\n",
      "cffi @ file:///C:/ci/cffi_1600699250966/work\n",
      "cftime==1.1.1.1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Could not generate requirement for distribution -atplotlib 3.1.1 (c:\\users\\danie\\anaconda3\\lib\\site-packages): Parse error at \"'-atplotl'\": Expected W:(abcd...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "chardet==3.0.4\n",
      "click==7.1.2\n",
      "cloudpickle @ file:///tmp/build/80754af9/cloudpickle_1598884132938/work\n",
      "clyent==1.2.2\n",
      "colorama @ file:///tmp/build/80754af9/colorama_1603211150991/work\n",
      "comtypes==1.1.7\n",
      "conda==4.9.1\n",
      "conda-build==3.20.5\n",
      "conda-package-handling @ file:///C:/ci/conda-package-handling_1603018162806/work\n",
      "conda-verify==3.4.2\n",
      "contextlib2==0.6.0.post1\n",
      "cryptography @ file:///C:/ci/cryptography_1601046913206/work\n",
      "cycler==0.10.0\n",
      "Cython @ file:///C:/ci/cython_1594834055134/work\n",
      "cytoolz==0.11.0\n",
      "dask @ file:///tmp/build/80754af9/dask-core_1602083700509/work\n",
      "decorator==4.4.2\n",
      "defusedxml==0.6.0\n",
      "distributed @ file:///C:/ci/distributed_1602065865185/work\n",
      "docutils==0.16\n",
      "entrypoints==0.3\n",
      "et-xmlfile==1.0.1\n",
      "fastcache==1.1.0\n",
      "filelock==3.0.12\n",
      "Flask==1.1.2\n",
      "fracridge==1.2.1\n",
      "fsspec @ file:///tmp/build/80754af9/fsspec_1602684995936/work\n",
      "funcy==1.15\n",
      "future==0.18.2\n",
      "gast @ file:///tmp/build/80754af9/gast_1597433534803/work\n",
      "gensim==3.8.3\n",
      "gevent @ file:///C:/ci/gevent_1601397624717/work\n",
      "glob2==0.7\n",
      "google-auth==1.11.2\n",
      "google-auth-oauthlib==0.4.1\n",
      "google-pasta==0.1.8\n",
      "greenlet @ file:///C:/ci/greenlet_1600874076725/work\n",
      "grpcio @ file:///C:/ci/grpcio_1597406403308/work\n",
      "h5py==2.10.0\n",
      "HeapDict==1.0.1\n",
      "hmmlearn==0.2.3\n",
      "html5lib @ file:///tmp/build/80754af9/html5lib_1593446221756/work\n",
      "hypothesis @ file:///tmp/build/80754af9/hypothesis_1604100955198/work\n",
      "idna @ file:///tmp/build/80754af9/idna_1593446292537/work\n",
      "imageio @ file:///tmp/build/80754af9/imageio_1594161405741/work\n",
      "imagesize==1.2.0\n",
      "imbalanced-learn==0.6.1\n",
      "imblearn==0.0\n",
      "importlib-metadata @ file:///tmp/build/80754af9/importlib-metadata_1602276842396/work\n",
      "iniconfig @ file:///tmp/build/80754af9/iniconfig_1602780191262/work\n",
      "ipykernel @ file:///C:/ci/ipykernel_1596208728219/work/dist/ipykernel-5.3.4-py3-none-any.whl\n",
      "ipython @ file:///C:/ci/ipython_1598883894321/work\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets @ file:///tmp/build/80754af9/ipywidgets_1601490159889/work\n",
      "isort @ file:///tmp/build/80754af9/isort_1602603989581/work\n",
      "itsdangerous==1.1.0\n",
      "jdcal==1.4.1\n",
      "jedi==0.15.1\n",
      "Jinja2==2.11.2\n",
      "joblib @ file:///tmp/build/80754af9/joblib_1601912903842/work\n",
      "json5==0.9.5\n",
      "jsonschema @ file:///tmp/build/80754af9/jsonschema_1602607155483/work\n",
      "jupyter==1.0.0\n",
      "jupyter-client @ file:///tmp/build/80754af9/jupyter_client_1601311786391/work\n",
      "jupyter-console @ file:///tmp/build/80754af9/jupyter_console_1598884538475/work\n",
      "jupyter-core==4.6.3\n",
      "jupyterlab==2.2.6\n",
      "jupyterlab-pygments @ file:///tmp/build/80754af9/jupyterlab_pygments_1601490720602/work\n",
      "jupyterlab-server @ file:///tmp/build/80754af9/jupyterlab_server_1594164409481/work\n",
      "keras==2.3.1\n",
      "Keras-Applications @ file:///tmp/build/80754af9/keras-applications_1594366238411/work\n",
      "Keras-Preprocessing==1.1.2\n",
      "keyring @ file:///C:/ci/keyring_1601488971022/work\n",
      "kiwisolver @ file:///C:/ci/kiwisolver_1603996595157/work\n",
      "lazy-object-proxy==1.4.3\n",
      "libarchive-c==2.9\n",
      "llvmlite==0.34.0\n",
      "locket==0.2.0\n",
      "lxml @ file:///C:/ci/lxml_1603216364379/work\n",
      "Markdown @ file:///C:/ci/markdown_1603216569530/work\n",
      "MarkupSafe @ file:///C:/ci/markupsafe_1594405949945/work\n",
      "matplotlib @ file:///C:/ci/matplotlib-base_1603356257853/work\n",
      "mccabe==0.6.1\n",
      "menuinst==1.4.16\n",
      "mistune @ file:///C:/ci/mistune_1594373272338/work\n",
      "mkl-fft==1.2.0\n",
      "mkl-random==1.1.1\n",
      "mkl-service==2.3.0\n",
      "mock==4.0.2\n",
      "more-itertools @ file:///tmp/build/80754af9/more-itertools_1598884071673/work\n",
      "mp20-project4-skeleton==0.2\n",
      "mpmath==1.1.0\n",
      "msgpack==1.0.0\n",
      "multipledispatch==0.6.0\n",
      "navigator-updater==0.2.1\n",
      "nbclient @ file:///tmp/build/80754af9/nbclient_1602783176460/work\n",
      "nbconvert @ file:///C:/ci/nbconvert_1601914921407/work\n",
      "nbformat @ file:///tmp/build/80754af9/nbformat_1602783287752/work\n",
      "nest-asyncio @ file:///tmp/build/80754af9/nest-asyncio_1601499549014/work\n",
      "netCDF4==1.5.3\n",
      "networkx @ file:///tmp/build/80754af9/networkx_1598376031484/work\n",
      "nltk @ file:///tmp/build/80754af9/nltk_1592496090529/work\n",
      "nose @ file:///C:/ci/nose_1594377973236/work\n",
      "notebook @ file:///C:/ci/notebook_1601494955742/work\n",
      "numba==0.51.2\n",
      "numexpr==2.7.1\n",
      "numpy @ file:///C:/ci/numpy_and_numpy_base_1603468620949/work\n",
      "numpydoc @ file:///tmp/build/80754af9/numpydoc_1594166760263/work\n",
      "oauthlib==3.1.0\n",
      "olefile==0.46\n",
      "opencv-python==4.2.0.34\n",
      "openpyxl @ file:///tmp/build/80754af9/openpyxl_1598113097404/work\n",
      "opt-einsum==3.2.0\n",
      "packaging==20.4\n",
      "pandas @ file:///C:/ci/pandas_1602088205210/work\n",
      "pandocfilters==1.4.2\n",
      "parso @ file:///tmp/build/80754af9/parso_1596826841367/work\n",
      "partd==1.1.0\n",
      "path @ file:///C:/ci/path_1596940809557/work\n",
      "pathlib2 @ file:///C:/ci/pathlib2_1594381094851/work\n",
      "patsy==0.5.1\n",
      "pep8==1.7.1\n",
      "pexpect==4.8.0\n",
      "pickleshare @ file:///C:/ci/pickleshare_1594374056827/work\n",
      "Pillow @ file:///C:/ci/pillow_1603821929285/work\n",
      "pkginfo==1.6.0\n",
      "pluggy==0.13.1\n",
      "ply==3.11\n",
      "prometheus-client==0.8.0\n",
      "prompt-toolkit @ file:///tmp/build/80754af9/prompt-toolkit_1602688806899/work\n",
      "protobuf==3.13.0\n",
      "psutil @ file:///C:/ci/psutil_1598352273540/work\n",
      "ptyprocess==0.6.0\n",
      "py @ file:///tmp/build/80754af9/py_1593446248552/work\n",
      "py-cpuinfo==7.0.0\n",
      "pyasn1==0.4.8\n",
      "pyasn1-modules==0.2.8\n",
      "pycodestyle==2.6.0\n",
      "pycosat==0.6.3\n",
      "pycparser @ file:///tmp/build/80754af9/pycparser_1594388511720/work\n",
      "pycrypto==2.6.1\n",
      "pycurl==7.43.0.6\n",
      "pyflakes==2.2.0\n",
      "pygal==2.4.0\n",
      "Pygments @ file:///tmp/build/80754af9/pygments_1604103097372/work\n",
      "pyLDAvis==2.1.2\n",
      "pylint @ file:///C:/ci/pylint_1598605958085/work\n",
      "pymc3==3.8\n",
      "pyodbc===4.0.0-unsupported\n",
      "pyOpenSSL @ file:///tmp/build/80754af9/pyopenssl_1594392929924/work\n",
      "pyparsing==2.4.7\n",
      "pyreadline==2.1\n",
      "pyrsistent @ file:///C:/ci/pyrsistent_1600123688363/work\n",
      "PySocks @ file:///C:/ci/pysocks_1594394709107/work\n",
      "pytest==0.0.0\n",
      "pytest-arraydiff==0.3\n",
      "pytest-astropy==0.8.0\n",
      "pytest-astropy-header==0.1.2\n",
      "pytest-doctestplus @ file:///tmp/build/80754af9/pytest-doctestplus_1598624048543/work\n",
      "pytest-openfiles==0.5.0\n",
      "pytest-remotedata==0.3.2\n",
      "python-dateutil==2.8.1\n",
      "pytz==2020.1\n",
      "PyWavelets @ file:///C:/ci/pywavelets_1601658407053/work\n",
      "pywin32==227\n",
      "pywin32-ctypes @ file:///C:/ci/pywin32-ctypes_1594392691209/work\n",
      "pywinpty==0.5.7\n",
      "PyYAML==5.3.1\n",
      "pyzmq==19.0.2\n",
      "QtAwesome @ file:///tmp/build/80754af9/qtawesome_1602272867890/work\n",
      "qtconsole @ file:///tmp/build/80754af9/qtconsole_1600870028330/work\n",
      "QtPy==1.9.0\n",
      "regex @ file:///C:/ci/regex_1602782614108/work\n",
      "requests @ file:///tmp/build/80754af9/requests_1592841827918/work\n",
      "requests-oauthlib==1.3.0\n",
      "rope @ file:///tmp/build/80754af9/rope_1602264064449/work\n",
      "rsa==4.0\n",
      "ruamel-yaml==0.15.87\n",
      "scikit-image==0.17.2\n",
      "scikit-learn @ file:///C:/ci/scikit-learn_1598376983131/work\n",
      "scipy @ file:///C:/ci/scipy_1597686737426/work\n",
      "seaborn @ file:///tmp/build/80754af9/seaborn_1600553570093/work\n",
      "Send2Trash==1.5.0\n",
      "setuptools-scm==4.1.2\n",
      "shortuuid==0.5.0\n",
      "simplegeneric==0.8.1\n",
      "singledispatch @ file:///tmp/build/80754af9/singledispatch_1602523705405/work\n",
      "six==1.15.0\n",
      "smart-open==3.0.0\n",
      "snowballstemmer==2.0.0\n",
      "sortedcollections==1.2.1\n",
      "sortedcontainers==2.2.2\n",
      "soupsieve==2.0.1\n",
      "Sphinx @ file:///tmp/build/80754af9/sphinx_1597428793432/work\n",
      "sphinxcontrib-applehelp==1.0.2\n",
      "sphinxcontrib-devhelp==1.0.2\n",
      "sphinxcontrib-htmlhelp==1.0.3\n",
      "sphinxcontrib-jsmath==1.0.1\n",
      "sphinxcontrib-qthelp==1.0.3\n",
      "sphinxcontrib-serializinghtml==1.1.4\n",
      "sphinxcontrib-websupport @ file:///tmp/build/80754af9/sphinxcontrib-websupport_1597081412696/work\n",
      "spyder==3.3.6\n",
      "spyder-kernels==0.5.2\n",
      "SQLAlchemy @ file:///C:/ci/sqlalchemy_1603818968375/work\n",
      "stata-kernel==1.12.0\n",
      "statsmodels==0.12.0\n",
      "sympy @ file:///C:/ci/sympy_1597083371142/work\n",
      "tables==3.6.1\n",
      "tblib @ file:///tmp/build/80754af9/tblib_1597928476713/work\n",
      "tensorboard==2.3.0\n",
      "tensorboard-plugin-wit==1.6.0.post3\n",
      "tensorflow==2.3.1\n",
      "tensorflow-estimator==2.3.0\n",
      "termcolor==1.1.0\n",
      "terminado==0.9.1\n",
      "testpath==0.4.4\n",
      "Theano==1.0.4\n",
      "threadpoolctl @ file:///tmp/tmp9twdgx9k/threadpoolctl-2.1.0-py3-none-any.whl\n",
      "tifffile==2020.10.1\n",
      "toml @ file:///tmp/build/80754af9/toml_1592853716807/work\n",
      "tomotopy==0.9.1\n",
      "toolz @ file:///tmp/build/80754af9/toolz_1601054250827/work\n",
      "tornado==6.0.4\n",
      "tqdm @ file:///tmp/build/80754af9/tqdm_1602185206534/work\n",
      "traitlets @ file:///tmp/build/80754af9/traitlets_1602787416690/work\n",
      "typed-ast==1.4.1\n",
      "typing-extensions @ file:///tmp/build/80754af9/typing_extensions_1598376058250/work\n",
      "unicodecsv==0.14.1\n",
      "urllib3 @ file:///tmp/build/80754af9/urllib3_1603305693037/work\n",
      "wcwidth @ file:///tmp/build/80754af9/wcwidth_1593447189090/work\n",
      "webencodings==0.5.1\n",
      "Werkzeug==1.0.1\n",
      "widgetsnbextension==3.5.1\n",
      "win-inet-pton==1.1.0\n",
      "win-unicode-console==0.5\n",
      "wincertstore==0.2\n",
      "wordcloud==1.8.0\n",
      "wrapt==1.11.2\n",
      "xarray==0.15.0\n",
      "xgboost==0.90\n",
      "xlrd==1.2.0\n",
      "XlsxWriter @ file:///tmp/build/80754af9/xlsxwriter_1602692860603/work\n",
      "xlwings==0.20.8\n",
      "xlwt==1.3.0\n",
      "xmltodict==0.12.0\n",
      "zict==2.0.0\n",
      "zipp @ file:///tmp/build/80754af9/zipp_1604001098328/work\n",
      "zope.event==4.5.0\n",
      "zope.interface @ file:///C:/ci/zope.interface_1602002496598/work\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('\\n')\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "embeddings = None\n",
    "if not args.train_embeddings:\n",
    "    emb_path = args.emb_path\n",
    "    vect_path = os.path.join(args.data_path.split('/')[0], 'embeddings.pkl')   \n",
    "    vectors = {}\n",
    "    with open(emb_path, 'rb') as f:\n",
    "        for l in f:\n",
    "            line = l.decode().split()\n",
    "            word = line[0]\n",
    "            if word in vocab:\n",
    "                vect = np.array(line[1:]).astype(np.float)\n",
    "                vectors[word] = vect\n",
    "    embeddings = np.zeros((vocab_size, args.emb_size))\n",
    "    words_found = 0\n",
    "    for i, word in enumerate(vocab):\n",
    "        try: \n",
    "            embeddings[i] = vectors[word]\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            embeddings[i] = np.random.normal(scale=0.6, size=(args.emb_size, ))\n",
    "    embeddings = torch.from_numpy(embeddings).to(device)\n",
    "    args.embeddings_dim = embeddings.size()\n",
    "\n",
    "print('=*'*100)\n",
    "print('Training an Embedded Topic Model on {} with the following settings: {}'.format(args.dataset.upper(), args))\n",
    "print('=*'*100)\n",
    "\n",
    "## define checkpoint\n",
    "if not os.path.exists(args.save_path):\n",
    "    os.makedirs(args.save_path)\n",
    "\n",
    "if args.mode == 'eval':\n",
    "    ckpt = args.load_from\n",
    "else:\n",
    "    ckpt = os.path.join(args.save_path, \n",
    "        'etm_{}_K_{}_Htheta_{}_Optim_{}_Clip_{}_ThetaAct_{}_Lr_{}_Bsz_{}_RhoSize_{}_trainEmbeddings_{}'.format(\n",
    "        args.dataset, args.num_topics, args.t_hidden_size, args.optimizer, args.clip, args.theta_act, \n",
    "            args.lr, args.batch_size, args.rho_size, args.train_embeddings))\n",
    "\n",
    "## define model and optimizer\n",
    "model = ETM(args.num_topics, vocab_size, args.t_hidden_size, args.rho_size, args.emb_size, \n",
    "                args.theta_act, embeddings, args.train_embeddings, args.enc_drop).to(device)\n",
    "\n",
    "print('model: {}'.format(model))\n",
    "\n",
    "if args.optimizer == 'adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "elif args.optimizer == 'adagrad':\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "elif args.optimizer == 'adadelta':\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "elif args.optimizer == 'rmsprop':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "elif args.optimizer == 'asgd':\n",
    "    optimizer = optim.ASGD(model.parameters(), lr=args.lr, t0=0, lambd=0., weight_decay=args.wdecay)\n",
    "else:\n",
    "    print('Defaulting to vanilla SGD')\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    acc_loss = 0\n",
    "    acc_kl_theta_loss = 0\n",
    "    cnt = 0\n",
    "    indices = torch.randperm(args.num_docs_train)\n",
    "    indices = torch.split(indices, args.batch_size)\n",
    "    for idx, ind in enumerate(indices):\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        data_batch = data.get_batch(train_tokens, train_counts, ind, args.vocab_size, device)\n",
    "        sums = data_batch.sum(1).unsqueeze(1)\n",
    "        if args.bow_norm:\n",
    "            normalized_data_batch = data_batch / sums\n",
    "        else:\n",
    "            normalized_data_batch = data_batch\n",
    "        recon_loss, kld_theta = model(data_batch, normalized_data_batch)\n",
    "        total_loss = recon_loss + kld_theta\n",
    "        total_loss.backward()\n",
    "\n",
    "        if args.clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        acc_loss += torch.sum(recon_loss).item()\n",
    "        acc_kl_theta_loss += torch.sum(kld_theta).item()\n",
    "        cnt += 1\n",
    "\n",
    "        if idx % args.log_interval == 0 and idx > 0:\n",
    "            cur_loss = round(acc_loss / cnt, 2) \n",
    "            cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n",
    "            cur_real_loss = round(cur_loss + cur_kl_theta, 2)\n",
    "\n",
    "            print('Epoch: {} .. batch: {}/{} .. LR: {} .. KL_theta: {} .. Rec_loss: {} .. NELBO: {}'.format(\n",
    "                epoch, idx, len(indices), optimizer.param_groups[0]['lr'], cur_kl_theta, cur_loss, cur_real_loss))\n",
    "    \n",
    "    cur_loss = round(acc_loss / cnt, 2) \n",
    "    cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n",
    "    cur_real_loss = round(cur_loss + cur_kl_theta, 2)\n",
    "    print('*'*100)\n",
    "    print('Epoch----->{} .. LR: {} .. KL_theta: {} .. Rec_loss: {} .. NELBO: {}'.format(\n",
    "            epoch, optimizer.param_groups[0]['lr'], cur_kl_theta, cur_loss, cur_real_loss))\n",
    "    print('*'*100)\n",
    "\n",
    "def visualize(m, show_emb=True):\n",
    "    if not os.path.exists('./results'):\n",
    "        os.makedirs('./results')\n",
    "\n",
    "    m.eval()\n",
    "\n",
    "    queries = ['andrew', 'computer', 'sports', 'religion', 'man', 'love', \n",
    "                'intelligence', 'money', 'politics', 'health', 'people', 'family']\n",
    "\n",
    "    ## visualize topics using monte carlo\n",
    "    with torch.no_grad():\n",
    "        print('#'*100)\n",
    "        print('Visualize topics...')\n",
    "        topics_words = []\n",
    "        gammas = m.get_beta()\n",
    "        for k in range(args.num_topics):\n",
    "            gamma = gammas[k]\n",
    "            top_words = list(gamma.cpu().numpy().argsort()[-args.num_words+1:][::-1])\n",
    "            topic_words = [vocab[a] for a in top_words]\n",
    "            topics_words.append(' '.join(topic_words))\n",
    "            print('Topic {}: {}'.format(k, topic_words))\n",
    "\n",
    "        if show_emb:\n",
    "            ## visualize word embeddings by using V to get nearest neighbors\n",
    "            print('#'*100)\n",
    "            print('Visualize word embeddings by using output embedding matrix')\n",
    "            try:\n",
    "                embeddings = m.rho.weight  # Vocab_size x E\n",
    "            except:\n",
    "                embeddings = m.rho         # Vocab_size x E\n",
    "            neighbors = []\n",
    "            for word in queries:\n",
    "                print('word: {} .. neighbors: {}'.format(\n",
    "                    word, nearest_neighbors(word, embeddings, vocab)))\n",
    "            print('#'*100)\n",
    "\n",
    "def evaluate(m, source, tc=False, td=False):\n",
    "    \"\"\"Compute perplexity on document completion.\n",
    "    \"\"\"\n",
    "    m.eval()\n",
    "    with torch.no_grad():\n",
    "        if source == 'val':\n",
    "            indices = torch.split(torch.tensor(range(args.num_docs_valid)), args.eval_batch_size)\n",
    "            tokens = valid_tokens\n",
    "            counts = valid_counts\n",
    "        else: \n",
    "            indices = torch.split(torch.tensor(range(args.num_docs_test)), args.eval_batch_size)\n",
    "            tokens = test_tokens\n",
    "            counts = test_counts\n",
    "\n",
    "        ## get \\beta here\n",
    "        beta = m.get_beta()\n",
    "\n",
    "        ### do dc and tc here\n",
    "        acc_loss = 0\n",
    "        cnt = 0\n",
    "        indices_1 = torch.split(torch.tensor(range(args.num_docs_test_1)), args.eval_batch_size)\n",
    "        for idx, ind in enumerate(indices_1):\n",
    "            ## get theta from first half of docs\n",
    "            data_batch_1 = data.get_batch(test_1_tokens, test_1_counts, ind, args.vocab_size, device)\n",
    "            sums_1 = data_batch_1.sum(1).unsqueeze(1)\n",
    "            if args.bow_norm:\n",
    "                normalized_data_batch_1 = data_batch_1 / sums_1\n",
    "            else:\n",
    "                normalized_data_batch_1 = data_batch_1\n",
    "            theta, _ = m.get_theta(normalized_data_batch_1)\n",
    "\n",
    "            ## get prediction loss using second half\n",
    "            data_batch_2 = data.get_batch(test_2_tokens, test_2_counts, ind, args.vocab_size, device)\n",
    "            sums_2 = data_batch_2.sum(1).unsqueeze(1)\n",
    "            res = torch.mm(theta, beta)\n",
    "            preds = torch.log(res)\n",
    "            recon_loss = -(preds * data_batch_2).sum(1)\n",
    "            \n",
    "            loss = recon_loss / sums_2.squeeze()\n",
    "            loss = loss.mean().item()\n",
    "            acc_loss += loss\n",
    "            cnt += 1\n",
    "        cur_loss = acc_loss / cnt\n",
    "        ppl_dc = round(math.exp(cur_loss), 1)\n",
    "        print('*'*100)\n",
    "        print('{} Doc Completion PPL: {}'.format(source.upper(), ppl_dc))\n",
    "        print('*'*100)\n",
    "        if tc or td:\n",
    "            beta = beta.data.cpu().numpy()\n",
    "            if tc:\n",
    "                print('Computing topic coherence...')\n",
    "                get_topic_coherence(beta, train_tokens, vocab)\n",
    "            if td:\n",
    "                print('Computing topic diversity...')\n",
    "                get_topic_diversity(beta, 25)\n",
    "        return ppl_dc\n",
    "\n",
    "if args.mode == 'train':\n",
    "    ## train model on data \n",
    "    best_epoch = 0\n",
    "    best_val_ppl = 1e9\n",
    "    all_val_ppls = []\n",
    "    print('\\n')\n",
    "    print('Visualizing model quality before training...')\n",
    "    visualize(model)\n",
    "    print('\\n')\n",
    "    for epoch in range(1, args.epochs):\n",
    "        train(epoch)\n",
    "        val_ppl = evaluate(model, 'val')\n",
    "        if val_ppl < best_val_ppl:\n",
    "            with open(ckpt, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_epoch = epoch\n",
    "            best_val_ppl = val_ppl\n",
    "        else:\n",
    "            ## check whether to anneal lr\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            if args.anneal_lr and (len(all_val_ppls) > args.nonmono and val_ppl > min(all_val_ppls[:-args.nonmono]) and lr > 1e-5):\n",
    "                optimizer.param_groups[0]['lr'] /= args.lr_factor\n",
    "        if epoch % args.visualize_every == 0:\n",
    "            visualize(model)\n",
    "        all_val_ppls.append(val_ppl)\n",
    "    with open(ckpt, 'rb') as f:\n",
    "        model = torch.load(f)\n",
    "    model = model.to(device)\n",
    "    val_ppl = evaluate(model, 'val')\n",
    "else:   \n",
    "    with open(ckpt, 'rb') as f:\n",
    "        model = torch.load(f)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ## get document completion perplexities\n",
    "        test_ppl = evaluate(model, 'test', tc=args.tc, td=args.td)\n",
    "\n",
    "        ## get most used topics\n",
    "        indices = torch.tensor(range(args.num_docs_train))\n",
    "        indices = torch.split(indices, args.batch_size)\n",
    "        thetaAvg = torch.zeros(1, args.num_topics).to(device)\n",
    "        thetaWeightedAvg = torch.zeros(1, args.num_topics).to(device)\n",
    "        cnt = 0\n",
    "        for idx, ind in enumerate(indices):\n",
    "            data_batch = data.get_batch(train_tokens, train_counts, ind, args.vocab_size, device)\n",
    "            sums = data_batch.sum(1).unsqueeze(1)\n",
    "            cnt += sums.sum(0).squeeze().cpu().numpy()\n",
    "            if args.bow_norm:\n",
    "                normalized_data_batch = data_batch / sums\n",
    "            else:\n",
    "                normalized_data_batch = data_batch\n",
    "            theta, _ = model.get_theta(normalized_data_batch)\n",
    "            thetaAvg += theta.sum(0).unsqueeze(0) / args.num_docs_train\n",
    "            weighed_theta = sums * theta\n",
    "            thetaWeightedAvg += weighed_theta.sum(0).unsqueeze(0)\n",
    "            if idx % 100 == 0 and idx > 0:\n",
    "                print('batch: {}/{}'.format(idx, len(indices)))\n",
    "        thetaWeightedAvg = thetaWeightedAvg.squeeze().cpu().numpy() / cnt\n",
    "        print('\\nThe 10 most used topics are {}'.format(thetaWeightedAvg.argsort()[::-1][:10]))\n",
    "\n",
    "        ## show topics\n",
    "        beta = model.get_beta()\n",
    "        topic_indices = list(np.random.choice(args.num_topics, 10)) # 10 random topics\n",
    "        print('\\n')\n",
    "        for k in range(args.num_topics):#topic_indices:\n",
    "            gamma = beta[k]\n",
    "            top_words = list(gamma.cpu().numpy().argsort()[-args.num_words+1:][::-1])\n",
    "            topic_words = [vocab[a] for a in top_words]\n",
    "            print('Topic {}: {}'.format(k, topic_words))\n",
    "\n",
    "        if args.train_embeddings:\n",
    "            ## show etm embeddings \n",
    "            try:\n",
    "                rho_etm = model.rho.weight.cpu()\n",
    "            except:\n",
    "                rho_etm = model.rho.cpu()\n",
    "            queries = ['andrew', 'woman', 'computer', 'sports', 'religion', 'man', 'love', \n",
    "                            'intelligence', 'money', 'politics', 'health', 'people', 'family']\n",
    "            print('\\n')\n",
    "            print('ETM embeddings...')\n",
    "            for word in queries:\n",
    "                print('word: {} .. etm neighbors: {}'.format(word, nearest_neighbors(word, rho_etm, vocab)))\n",
    "            print('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
