{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "\n",
    "#/usr/bin/python\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import pickle \n",
    "import numpy as np \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Torch-bearer\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from etm import ETM\n",
    "from utils import nearest_neighbors, get_topic_coherence, get_topic_diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20494"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts = pd.read_csv(\"abstracts_eng.csv\") #Replace with latest version\n",
    "collection = list(abstracts['abstract'])\n",
    "len(collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20494\n"
     ]
    }
   ],
   "source": [
    "# NOTE: we need the whole collection for the embedding \n",
    "# Train and test (70-30)\n",
    "seed = 11\n",
    "random.seed(seed)\n",
    "random.shuffle(collection)\n",
    "\n",
    "print(len(collection))\n",
    "set_size = 500\n",
    "batch1 = collection[:set_size]\n",
    "batch2 = collection[set_size:2*set_size]\n",
    "batch3 = collection[2*set_size:3*set_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: REWRITE THIS ONE for our new representation of the collection which is a list of lists of word ids\n",
    "def get_batch(corpus, ind, vocab_size, device, emsize=300):\n",
    "    \"\"\"\n",
    "    This function takes as input a corpus and a list of documents \n",
    "    and returns as output the torch tensor to feed into the net. \n",
    "    The list of documents defines the batch to work on. \n",
    "    \"\"\"\n",
    "    batch_size = len(ind)\n",
    "    data_batch = np.zeros((batch_size, vocab_size))\n",
    "    \n",
    "    for i, doc_id in enumerate(ind):\n",
    "        doc = corpus[doc_id]\n",
    "        L = len(doc)\n",
    "        if doc_id != -1:\n",
    "            for unique_word in doc:\n",
    "                data_batch[i, unique_word[0]] = unique_word[1]\n",
    "    data_batch = torch.from_numpy(data_batch).float().to(device)\n",
    "    return data_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# testing the function correctly works \n",
    "get_batch(corpus1,[0,3,6,8],len(dictionary1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The network components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The embedding layer\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the embeddings we'll use a pre-trained contextual model \n",
    "# Here we can play a bit to get a sense of its working \n",
    "\n",
    "#!pip install transformers \n",
    "from transformers import DistilBertTokenizerFast, DistilBertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why DistilBer? <br>\n",
    "The DistilBERT model was proposed in the blog post [Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT](https://medium.com/huggingface/distilbert-8cf3380435b5), and the paper DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERTâ€™s performances as measured on the GLUE language understanding benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased', return_dict=True)\n",
    "inputs = tokenizer(\"Hello, we are three cool data scientists\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.size() # this is the embedding of the above sentence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now there's a bit of work to do to prepare the embedding matrix. <br>\n",
    "We want to use Bert to get the embedding for each word in our corpus. However, being Bert a contextual embedding we could end up with more than one embedding vector for each word. To solve this problem we associate each word a different token for each different embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will work during the developing phase on a subset of the collection because \n",
    "# otherwise the memory requirements will be to high.\n",
    "# We should later apply the same computation that I apply here to all the batches inside the \n",
    "# collection.\n",
    "subset_size = 10 # len(collection)\n",
    "collection_subset = collection[0:subset_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  1996,  2622,  1523,  5318,  1011,  2327,  2401,  1524,  2001,\n",
       "         2241,  2006,  1037,  5257,  1997,  1996,  6745,  8332,  4106,  1998,\n",
       "         2433,  1011,  4531, 15078,  5906,  2007,  3151,  1010, 10036,  1998,\n",
       "         4621,  2810,  5461,  1012,  2009,  2003,  1996,  2765,  1997,  8144,\n",
       "         2000,  2954,  2114,  5166,  1998,  2051,  1012,  1996,  3988,  5166,\n",
       "         2001, 11910, 19329,  1998,  2069,  2698,  3134,  2001,  1996,  2051,\n",
       "         2000,  2298,  2005, 13162,  1010,  2640,  1996, 10531,  1010,  2933,\n",
       "         1996,  2810, 12335,  1998,  3857,  2009,  1012,  1996,  2878,  2832,\n",
       "         1997, 12697,  1010,  3247,  2006,  1996,  4475,  1010,  8332,  4106,\n",
       "         1998,  2810,  2003,  3591,  1999,  1996,  3259,  1010,  2164,  8993,\n",
       "         2006,  2047,  2433,  1011,  4531,  4725,  2000, 25136,  1037,  2622,\n",
       "         1999, 26179,  1998,  2470,  2006,  1037,  2047,  2433,  6198,  2291,\n",
       "         2478,  8040, 10354, 21508,  1010, 19747,  1010,  7318,  1998,  3886,\n",
       "        19485,  1998,  2383,  1037, 16343,  2004,  2364,  6994,  1012,   102,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenised_collection = tokenizer(collection_subset, return_tensors=\"pt\", truncation=False, padding=True)\n",
    "tokenised_collection[\"input_ids\"][0] # we have added padding so that we can process the whole collection in a batch - we gain in speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_collection = model(**tokenised_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally start to build our vocabulary. <br>\n",
    "We'll look at each word and add it to the vocabulary only if there's not already the same embedding_vector in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 193, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = embedded_collection.last_hidden_state.size()\n",
    "padding_size = size[1]\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {} # vocabulary in the form (int,word) pairs\n",
    "idx2bertIdx = {} # each index in our vocabulary is mapped to the Bert vocabulary index \n",
    "set_of_embeddings = set()\n",
    "\n",
    "\n",
    "idx = 0 # initialise dictionary index \n",
    "new_token_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 done. Time: 0.03 s.\n",
      "Document 1 done. Time: 0.21 s.\n",
      "Document 2 done. Time: 0.04 s.\n",
      "Document 3 done. Time: 0.04 s.\n",
      "Document 4 done. Time: 0.37 s.\n",
      "Document 5 done. Time: 0.22 s.\n",
      "Document 6 done. Time: 0.22 s.\n",
      "Document 7 done. Time: 0.63 s.\n",
      "Document 8 done. Time: 1.37 s.\n",
      "Document 9 done. Time: 0.3 s.\n",
      "Total time: 3.43 s.\n"
     ]
    }
   ],
   "source": [
    "# the idea of this for loop is the following: \n",
    "\n",
    "# input : [[4535,564,2342,...],[423423,32432,...],...] sequence of bert tokens for our collection \n",
    "#         [[embedding1, embedding2, ...],[...],...] and respective embedding vectors \n",
    "\n",
    "\n",
    "# output: [[0, 1, 2, 1, 4, ...], [0, 4, ...], ...] sequence of our tokens for our collection \n",
    "#         [[ embedding of token 0 ]\n",
    "#          [ embedding of token 1 ]\n",
    "#          [ embedding of token 2 ]\n",
    "#          [ embedding of token 3 ]\n",
    "#          [ embedding of token 4 ]\n",
    "#                   \n",
    "#                    ...\n",
    "#\n",
    "#          [ embedding of last token ]]\n",
    "\n",
    "# Note: different tokens in our vocabulary (say 1 and 3) can refer to the same word if the embedding vector is different \n",
    "\n",
    "# The model will take as input the new tokens' collection (in some processed form maybe) and the embedding matrix\n",
    "start = time.time()\n",
    "for i in range(subset_size):\n",
    "    t1 = time.time()\n",
    "    embedded_doc = embedded_collection.last_hidden_state[i][tokenised_collection[\"attention_mask\"][i].bool()] # removing padding using the attention mask \n",
    "    tokens_ids = tokenised_collection[\"input_ids\"][i]\n",
    "    new_token_ids_doc = []\n",
    "    for j,emb_vector in enumerate(embedded_doc):\n",
    "        token_id = tokens_ids[j].cpu().numpy() # bert current token \n",
    "        word = tokenizer.convert_ids_to_tokens([token_id])[0] # corresponding word\n",
    "        bool_list = [torch.all(torch.eq(emb_vector, other)) for other in set_of_embeddings]\n",
    "        if not any(bool_list): \n",
    "            # add new embedding to the set \n",
    "            set_of_embeddings.add(emb_vector)\n",
    "            # increase the index and save the word in the dictionary \n",
    "            idx2word[idx] = word # save it in our vocabulary\n",
    "            idx2bertIdx[idx] = token_id # save in id to bert id mapping\n",
    "            new_token_ids_doc += [idx]\n",
    "            idx += 1\n",
    "        else: # find the right id for the word and add it to our new tokenisation\n",
    "            word_id = list(idx2word.values()).index(word)\n",
    "            new_token_ids_doc += [word_id]\n",
    "    new_token_ids += [new_token_ids_doc]\n",
    "    t2 = time.time()\n",
    "    print(\"Document \"+str(i)+\" done. Time: \"+str(round(t2-t1,2))+\" s.\")\n",
    "end = time.time()\n",
    "print(\"Total time: \"+str(round(end-start,2))+\" s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set_of_embeddings) == len(idx2word.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([351, 768])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_collection.last_hidden_state[1][tokenised_collection[\"attention_mask\"][1].bool()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subset(doc_subset, tokenizer, model, set_of_embeddings, idx2word, idx2bertIdx, new_token_ids):\n",
    "    \n",
    "    tokenised_collection = tokenizer(doc_subset, return_tensors=\"pt\", truncation=False, padding=True)\n",
    "    print(\"tokenisation done\")\n",
    "    embedded_collection = model(**tokenised_collection)\n",
    "    print(\"embeddings done\")\n",
    "    \n",
    "    if len(idx2word) == 0:\n",
    "        idx = 0\n",
    "    else:\n",
    "        idx = len(set_of_embeddings)\n",
    "    \n",
    "    subset_size = len(doc_subset)\n",
    "    start = time.time()\n",
    "    for i in range(subset_size):\n",
    "        t1 = time.time()\n",
    "        embedded_doc = embedded_collection.last_hidden_state[i][tokenised_collection[\"attention_mask\"][i].bool()] # removing padding using the attention mask \n",
    "        tokens_ids = tokenised_collection[\"input_ids\"][i]\n",
    "        new_token_ids_doc = []\n",
    "        for j,emb_vector in enumerate(embedded_doc):\n",
    "            token_id = tokens_ids[j].cpu().numpy() # bert current token \n",
    "            word = tokenizer.convert_ids_to_tokens([token_id])[0] # corresponding word\n",
    "            bool_list = [torch.all(torch.eq(emb_vector, other)) for other in set_of_embeddings]\n",
    "            if not any(bool_list): \n",
    "                # add new embedding to the set \n",
    "                set_of_embeddings.add(emb_vector)\n",
    "                # increase the index and save the word in the dictionary \n",
    "                idx2word[idx] = word # save it in our vocabulary\n",
    "                idx2bertIdx[idx] = token_id # save in id to bert id mapping\n",
    "                new_token_ids_doc += [idx]\n",
    "                idx += 1\n",
    "            else: # find the right id for the word and add it to our new tokenisation\n",
    "                word_id = list(idx2word.values()).index(word)\n",
    "                new_token_ids_doc += [word_id]\n",
    "        new_token_ids += [new_token_ids_doc]\n",
    "        t2 = time.time()\n",
    "        if i%20==0:\n",
    "            print(\"Document \"+str(i)+\" done. Time: \"+str(round(t2-t1,2))+\" s.\")\n",
    "    end = time.time()\n",
    "    print(\"Total time: \"+str(round(end-start,2))+\" s.\")\n",
    "    \n",
    "    return set_of_embeddings, idx2word, idx2bertIdx, new_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenisation done\n",
      "embeddings done\n",
      "Document 0 done. Time: 0.42 s.\n",
      "Document 20 done. Time: 17.31 s.\n",
      "Total time: 172.0 s.\n"
     ]
    }
   ],
   "source": [
    "set_of_embeddings, idx2word, idx2bertIdx, new_token_ids = process_subset(batch1[:25], tokenizer, model, set_of_embeddings, idx2word, idx2bertIdx, new_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenisation done\n",
      "embeddings done\n",
      "Document 0 done. Time: 0.0 s.\n",
      "Total time: 0.0 s.\n"
     ]
    }
   ],
   "source": [
    "idx2word = {} # vocabulary in the form (int,word) pairs\n",
    "idx2bertIdx = {} # each index in our vocabulary is mapped to the Bert vocabulary index \n",
    "set_of_embeddings = set()\n",
    "\n",
    "\n",
    "idx = 0 # initialise dictionary index \n",
    "new_token_ids = []\n",
    "#sen1 = \"I grew up in Italy\"\n",
    "#sen2 = \"I grew up in Sweden\"\n",
    "#sen1 = \"This mouse loves cheese\"\n",
    "#sen2 = \"I am using this mouse as a pointer\"\n",
    "#sen1 = \"I left my wallet when I left the house through the left door\"\n",
    "#sen2 = \"I turned left with the car\"\n",
    "sen1 = \"I fed my dog\"\n",
    "sen2 = \"I fed my cat\"\n",
    "doc = [sen1, sen2]\n",
    "set_of_embeddings, idx2word, idx2bertIdx, new_token_ids = process_subset(doc, tokenizer, model, set_of_embeddings, idx2word, idx2bertIdx, new_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenisation done\n",
      "embeddings done\n"
     ]
    }
   ],
   "source": [
    "tokenised_collection = tokenizer(doc, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "print(\"tokenisation done\")\n",
    "embedded_collection = model(**tokenised_collection)\n",
    "print(\"embeddings done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '[CLS]', 1: 'i', 2: 'fed', 3: 'my', 4: 'dog', 5: '[SEP]', 6: '[CLS]', 7: 'i', 8: 'fed', 9: 'my', 10: 'cat', 11: '[SEP]'}\n"
     ]
    }
   ],
   "source": [
    "print(idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8994, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos = torch.nn.CosineSimilarity(dim = 0)\n",
    "cos(embedded_collection.last_hidden_state[0][4], embedded_collection.last_hidden_state[1][4])\n",
    "# stop words get very different score with even same meaning\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try cosine similarity clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subset_cosine(doc_subset, tokenizer, model, set_of_embeddings, idx2word, idx2bertIdx, new_token_ids):\n",
    "    \n",
    "    tokenised_collection = tokenizer(doc_subset, return_tensors=\"pt\", truncation=False, padding=True)\n",
    "    print(\"tokenisation done\")\n",
    "    embedded_collection = model(**tokenised_collection)\n",
    "    print(\"embeddings done\")\n",
    "    \n",
    "    if len(idx2word) == 0:\n",
    "        idx = 0\n",
    "    else:\n",
    "        idx = len(set_of_embeddings)\n",
    "        \n",
    "    cos = torch.nn.CosineSimilarity(dim = 0)\n",
    "    threshold = 0.9\n",
    "    \n",
    "    subset_size = len(doc_subset)\n",
    "    start = time.time()\n",
    "    for i in range(subset_size):\n",
    "        t1 = time.time()\n",
    "        embedded_doc = embedded_collection.last_hidden_state[i][tokenised_collection[\"attention_mask\"][i].bool()] # removing padding using the attention mask \n",
    "        tokens_ids = tokenised_collection[\"input_ids\"][i]\n",
    "        new_token_ids_doc = []\n",
    "        for j,emb_vector in enumerate(embedded_doc):\n",
    "            token_id = tokens_ids[j].cpu().numpy() # bert current token \n",
    "            word = tokenizer.convert_ids_to_tokens([token_id])[0] # corresponding word\n",
    "            bool_list = [cos(emb_vector, other) >= threshold for other in set_of_embeddings] \n",
    "            if not any(bool_list): \n",
    "                # add new embedding to the set \n",
    "                set_of_embeddings.add(emb_vector)\n",
    "                # increase the index and save the word in the dictionary \n",
    "                idx2word[idx] = word # save it in our vocabulary\n",
    "                idx2bertIdx[idx] = token_id # save in id to bert id mapping\n",
    "                new_token_ids_doc += [idx]\n",
    "                idx += 1\n",
    "            else: # find the right id for the word and add it to our new tokenisation\n",
    "                if (word not in idx2word.values()):\n",
    "                    # add new embedding to the set \n",
    "                    set_of_embeddings.add(emb_vector)\n",
    "                    # increase the index and save the word in the dictionary \n",
    "                    idx2word[idx] = word # save it in our vocabulary\n",
    "                    idx2bertIdx[idx] = token_id # save in id to bert id mapping\n",
    "                    new_token_ids_doc += [idx]\n",
    "                    idx += 1\n",
    "                else: \n",
    "                    word_id = list(idx2word.values()).index(word)\n",
    "                    new_token_ids_doc += [word_id]\n",
    "        new_token_ids += [new_token_ids_doc]\n",
    "        t2 = time.time()\n",
    "        if i%20==0:\n",
    "            print(\"Document \"+str(i)+\" done. Time: \"+str(round(t2-t1,2))+\" s.\")\n",
    "    end = time.time()\n",
    "    print(\"Total time: \"+str(round(end-start,2))+\" s.\")\n",
    "    \n",
    "    return set_of_embeddings, idx2word, idx2bertIdx, new_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenisation done\n",
      "embeddings done\n",
      "tokenisation done\n",
      "embeddings done\n",
      "Document 0 done. Time: 0.01 s.\n",
      "Total time: 0.02 s.\n"
     ]
    }
   ],
   "source": [
    "idx2word = {} # vocabulary in the form (int,word) pairs\n",
    "idx2bertIdx = {} # each index in our vocabulary is mapped to the Bert vocabulary index \n",
    "set_of_embeddings = set()\n",
    "\n",
    "\n",
    "tokenised_collection = tokenizer(doc, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "print(\"tokenisation done\")\n",
    "embedded_collection = model(**tokenised_collection)\n",
    "print(\"embeddings done\")\n",
    "idx = 0 # initialise dictionary index \n",
    "new_token_ids = []\n",
    "#sen1 = \"I grew up in Italy\"\n",
    "#sen2 = \"I grew up in Sweden\"\n",
    "sen1 = \"This mouse loves cheese\"\n",
    "sen2 = \"I am using this mouse as a pointer\"\n",
    "#sen1 = \"I left my wallet when I left the house through the left door\"\n",
    "#sen2 = \"I turned left with the car\"\n",
    "#sen1 = \"I fed my dog\"\n",
    "#sen2 = \"I fed my cat\"\n",
    "doc = [sen1, sen2]\n",
    "set_of_embeddings, idx2word, idx2bertIdx, new_token_ids = process_subset_cosine(doc, tokenizer, model, set_of_embeddings, idx2word, idx2bertIdx, new_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array(101),\n",
       " 1: array(2023),\n",
       " 2: array(8000),\n",
       " 3: array(7459),\n",
       " 4: array(8808),\n",
       " 5: array(102),\n",
       " 6: array(1045),\n",
       " 7: array(2572),\n",
       " 8: array(2478),\n",
       " 9: array(2023),\n",
       " 10: array(8000),\n",
       " 11: array(2004),\n",
       " 12: array(1037),\n",
       " 13: array(20884)}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2bertIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(idx2bertIdx[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '[CLS]', 1: 'this', 2: 'mouse', 3: 'loves', 4: 'cheese', 5: '[SEP]', 6: 'i', 7: 'am', 8: 'using', 9: 'this', 10: 'mouse', 11: 'as', 12: 'a', 13: 'pointer'}\n"
     ]
    }
   ],
   "source": [
    "print(idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 768])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_collection.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3757, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos = torch.nn.CosineSimilarity(dim = 0)\n",
    "cos(embedded_collection.last_hidden_state[0][2], embedded_collection.last_hidden_state[1][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: wrap the above code in an loop that goes through the whole collection of documents \n",
    "# in order to get the 'new_token_ids' list and 'set_of_embeddings' set \n",
    "# for the whole collection (now it only represents the first few documents)\n",
    "\n",
    "# divide data in batches\n",
    "# build model for each batch, including tokenizing, getting embeddings and mapping to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: build the embedding matrix from complete 'set_of_embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: adjust the get_batch function so as to get the documents x vocabulary matrix \n",
    "# required by the model starting from the complete 'new_token_ids' list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### model-related arguments\n",
    "num_topics = 50\n",
    "rho_size = 300 # dimension of rho \n",
    "emb_size = 768 # dimension of embeddings \n",
    "t_hidden_size = 800 # dimension of hidden space of q(theta)\n",
    "theta_act = 'relu' # either tanh, softplus, relu, rrelu, leakyrelu, elu, selu, glu\n",
    "train_embeddings = False\n",
    "vocab_size = ... # tbd \n",
    "seed = 11\n",
    "\n",
    "### optimization-related arguments\n",
    "lr = 0.005\n",
    "lr_factor = 4.0 #divide learning rate by this\n",
    "epochs = 20 \n",
    "mode = \"train\"\n",
    "enc_drop = 0.0 # dropout rate on encoder\n",
    "clip = 0.0 # gradient clipping\n",
    "nonmono = 10 # number of bad hits allowed ...?\n",
    "weight_decay = 1.2e-6\n",
    "anneal_lr = False # whether to anneal the learning rate or not\n",
    "bow_norm = True # normalize the bows or not \n",
    "_optimizer = \"adam\"\n",
    "\n",
    "### evaluation, visualization, and logging-related arguments\n",
    "num_docs_train = set1_size\n",
    "num_docs_test = set2_size\n",
    "num_words = 10 #number of words for topic viz'\n",
    "log_interval = 2 #when to log training\n",
    "visualize_every = 10 #when to visualize results\n",
    "tc = False # whether to compute topic coherence or not\n",
    "td = False # whether to compute topic diversity or not\n",
    "\n",
    "### data and file related arguments\n",
    "save_path = './results'\n",
    "batch_size = 1000\n",
    "eval_batch_size = 1000 #input batch size for evaluation\n",
    "load_from = \"\" #the name of the ckpt to run evaluation from\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Setting the random seed \n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "if mode == 'eval':\n",
    "    ckpt = load_from \n",
    "else:\n",
    "    ckpt = os.path.join(save_path, \n",
    "        'etm_K_{}_Htheta_{}_Optim_{}_Clip_{}_ThetaAct_{}_Lr_{}_Bsz_{}_RhoSize_{}_trainEmbeddings_{}'.format(\n",
    "         num_topics, t_hidden_size, optimizer, clip, theta_act, lr, batch_size, rho_size, train_embeddings))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _optimizer == 'adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "elif _optimizer == 'adagrad':\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "elif _optimizer == 'adadelta':\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "elif _optimizer == 'rmsprop':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "elif _optimizer == 'asgd':\n",
    "    optimizer = optim.ASGD(model.parameters(), lr=args.lr, t0=0, lambd=0., weight_decay=args.wdecay)\n",
    "else:\n",
    "    print('Defaulting to vanilla SGD')\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = ETM(num_topics = num_topics, \n",
    "            vocab_size = vocab_size, #TODO: remove this \n",
    "            t_hidden_size = t_hidden_size, \n",
    "            rho_size = rho_size, \n",
    "            emb_size = emb_size, \n",
    "            theta_act = theta_act, \n",
    "            embeddings = embeddings, # TODO remove this \n",
    "            train_embeddings = train_embeddings, \n",
    "            enc_drop = enc_drop).to(device)\n",
    "\n",
    "print('model: {}'.format(model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, num_docs_train, batch_size, vocab_size, bow_norm, clip, log_interval):\n",
    "    \"\"\" Just the training function ... \"\"\"\n",
    "    \n",
    "    model.train() #setting the model in training mode\n",
    "    # preparing all the data structures \n",
    "    acc_loss = 0\n",
    "    acc_kl_theta_loss = 0\n",
    "    cnt = 0\n",
    "    indices = torch.randperm(num_docs_train)\n",
    "    indices = torch.split(indices, batch_size)\n",
    "    \n",
    "    for idx, ind in enumerate(indices): # all our batches \n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        # TODO: modify this one --------------\n",
    "        data_batch = data.get_batch(train_tokens, train_counts, ind, vocab_size, device)\n",
    "        # --------------------\n",
    "        sums = data_batch.sum(1).unsqueeze(1) # what are we summing ?? \n",
    "        \n",
    "        # maybe normalising the input \n",
    "        if bow_norm: normalized_data_batch = data_batch / sums\n",
    "        else: normalized_data_batch = data_batch\n",
    "        # loss on the batch \n",
    "        recon_loss, kld_theta = model(data_batch, normalized_data_batch)\n",
    "        total_loss = recon_loss + kld_theta\n",
    "        total_loss.backward() # compute backpropagation\n",
    "        # maybe clip the gradient \n",
    "        if clip > 0: torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step() # finally update the weights \n",
    "        # accumulate the total loss \n",
    "        acc_loss += torch.sum(recon_loss).item()\n",
    "        acc_kl_theta_loss += torch.sum(kld_theta).item()\n",
    "        cnt += 1\n",
    "        \n",
    "        # visualisation/print time! ('cur' stands for current ...)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            cur_loss = round(acc_loss / cnt, 2) \n",
    "            cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n",
    "            cur_real_loss = round(cur_loss + cur_kl_theta, 2)\n",
    "            print('Epoch: {} .. batch: {}/{} .. LR: {} .. KL_theta: {} .. Rec_loss: {} .. NELBO: {}'.format(\n",
    "                epoch, idx, len(indices), optimizer.param_groups[0]['lr'], cur_kl_theta, cur_loss, cur_real_loss))\n",
    "    \n",
    "    # Wrapping up the results of the epoch! \n",
    "    cur_loss = round(acc_loss / cnt, 2) \n",
    "    cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n",
    "    cur_real_loss = round(cur_loss + cur_kl_theta, 2)\n",
    "    print('*'*100)\n",
    "    print('Epoch----->{} .. LR: {} .. KL_theta: {} .. Rec_loss: {} .. NELBO: {}'.format(\n",
    "            epoch, optimizer.param_groups[0]['lr'], cur_kl_theta, cur_loss, cur_real_loss))\n",
    "    print('*'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(m, num_topics, num_words, vocab, show_emb=True):\n",
    "    \"\"\" This is a cool visualisation function. \n",
    "    Takes as input the model so far and shows the discovered embeddings! \"\"\"\n",
    "    \n",
    "    # We're going to save our results here \n",
    "    # TODO: parametrize this path \n",
    "    if not os.path.exists('./results'):\n",
    "        os.makedirs('./results')\n",
    "\n",
    "    m.eval() #set the net in evaluation mode \n",
    "    # set a few words to query \n",
    "    queries = ['andrew', 'computer', 'sports', 'religion', 'man', 'love', \n",
    "                'intelligence', 'money', 'politics', 'health', 'people', 'family']\n",
    "\n",
    "    ## visualize topics using monte carlo (sampling from the posterior I guess)\n",
    "    with torch.no_grad(): # no gradients computation - makes forward pass lighter \n",
    "        print('#'*100)\n",
    "        print('Visualize topics...')\n",
    "        topics_words = []\n",
    "        gammas = m.get_beta() # topics distributions \n",
    "        for k in range(num_topics):\n",
    "            gamma = gammas[k]\n",
    "            top_words = list(gamma.cpu().numpy().argsort()[-num_words+1:][::-1])\n",
    "            topic_words = [vocab[a] for a in top_words] \n",
    "            topics_words.append(' '.join(topic_words))\n",
    "            print('Topic {}: {}'.format(k, topic_words))\n",
    "\n",
    "        if show_emb:\n",
    "            ## visualize word embeddings by using V to get nearest neighbors\n",
    "            print('#'*100)\n",
    "            print('Visualize word embeddings by using output embedding matrix')\n",
    "            \n",
    "            # extract the embeddings from the model! \n",
    "            try:embeddings = m.rho.weight  # Vocab_size x E\n",
    "            except:embeddings = m.rho         # Vocab_size x E\n",
    "            \n",
    "            for word in queries:\n",
    "                print('word: {} .. neighbors: {}'.format(\n",
    "                    word, nearest_neighbors(word, embeddings, vocab))) # utility function \n",
    "            print('#'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(m, num_docs_test=num_docs_test, tc=tc, td=td, eval_batch_size=eval_batch_size, vocab_size=vocab_size, bow_norm=bow_norm):\n",
    "    \"\"\"\n",
    "    Evaluating the trained model on the test set using either perplexity, or coherence and diversity. \n",
    "    Compute perplexity on document completion.\n",
    "    \"\"\"\n",
    "    \n",
    "    m.eval() # set model in evaluation mode \n",
    "    # load the data ... TODO change this \n",
    "    with torch.no_grad():\n",
    "        indices = torch.split(torch.tensor(range(num_docs_test)), eval_batch_size)\n",
    "        \n",
    "        ## get \\beta here\n",
    "        beta = m.get_beta()\n",
    "\n",
    "        ### do dc and tc here\n",
    "        acc_loss = 0\n",
    "        cnt = 0\n",
    "        \n",
    "        for idx, ind in enumerate(indices):\n",
    "            data_batch = data.get_batch(ind, vocab_size, device)\n",
    "            sums = data_batch.sum(1).unsqueeze(1)\n",
    "            if bow_norm: normalized_data_batch = data_batch / sums\n",
    "            else: normalized_data_batch = data_batch\n",
    "                \n",
    "            ## get theta\n",
    "            theta, _ = m.get_theta(normalized_data_batch)\n",
    "            ## get prediction loss\n",
    "            res = torch.mm(theta, beta)\n",
    "            preds = torch.log(res)\n",
    "            recon_loss = -(preds * data_batch).sum(1)\n",
    "            loss = recon_loss / sums.squeeze()\n",
    "            loss = loss.mean().item()\n",
    "            acc_loss += loss\n",
    "            cnt += 1\n",
    "        \n",
    "        # Calculate final loss \n",
    "        cur_loss = acc_loss / cnt\n",
    "        ppl_dc = round(math.exp(cur_loss), 1)\n",
    "        print('*'*100)\n",
    "        print('{} Doc Completion PPL: {}'.format(source.upper(), ppl_dc))\n",
    "        print('*'*100)\n",
    "        \n",
    "        \n",
    "        if tc or td: # calculate topic coherence or topic diversity \n",
    "            beta = beta.data.cpu().numpy()\n",
    "            if tc:\n",
    "                print('Computing topic coherence...')\n",
    "                get_topic_coherence(beta, train_tokens, vocab)\n",
    "            if td:\n",
    "                print('Computing topic diversity...')\n",
    "                get_topic_diversity(beta, 25)\n",
    "        return ppl_dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code that actually launches the training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the data structures \n",
    "best_epoch = 0\n",
    "best_val_ppl = 1e9\n",
    "all_val_ppls = []\n",
    "\n",
    "# Let's get a sense of how bad the model is before training \n",
    "print('\\n')\n",
    "print('Visualizing model quality before training...')\n",
    "visualize(model)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs):\n",
    "    \n",
    "    train(epoch) # train \n",
    "    val_ppl = evaluate(model, 'val') # evaluate \n",
    "    \n",
    "    # only saving the model if it's the best so far \n",
    "    if val_ppl < best_val_ppl: \n",
    "        with open(ckpt, 'wb') as f:\n",
    "            torch.save(model, f)\n",
    "        best_epoch = epoch \n",
    "        best_val_ppl = val_ppl\n",
    "        \n",
    "    else:\n",
    "        ## check whether to anneal lr (aka decreasing it by a constant factor )\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        if args.anneal_lr and (len(all_val_ppls) > args.nonmono and val_ppl > min(all_val_ppls[:-args.nonmono]) and lr > 1e-5):\n",
    "            optimizer.param_groups[0]['lr'] /= args.lr_factor\n",
    "            \n",
    "    #maybe visualise \n",
    "    if epoch % args.visualize_every == 0:\n",
    "        visualize(model)\n",
    "        \n",
    "    #save perplexities \n",
    "    all_val_ppls.append(val_ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the code that launches the final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained model and evaluate it  \n",
    "with open(ckpt, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    ## ---------------\n",
    "    ## Idea : get document completion perplexities\n",
    "    test_ppl = evaluate(model) \n",
    "\n",
    "    ## ----------------\n",
    "    ## Idea : get most used topics\n",
    "    indices = torch.tensor(range(num_docs_train)) # training documents indices \n",
    "    indices = torch.split(indices, batch_size)\n",
    "    #just initialising data structures \n",
    "    thetaAvg = torch.zeros(1, args.num_topics).to(device)\n",
    "    thetaWeightedAvg = torch.zeros(1, args.num_topics).to(device)\n",
    "    cnt = 0\n",
    "    for idx, ind in enumerate(indices):\n",
    "        data_batch = data.get_batch(ind, args.vocab_size, device) # TODO: fix here \n",
    "        sums = data_batch.sum(1).unsqueeze(1) \n",
    "        cnt += sums.sum(0).squeeze().cpu().numpy()\n",
    "        # maybe normalise \n",
    "        if args.bow_norm:normalized_data_batch = data_batch / sums\n",
    "        else: normalized_data_batch = data_batch\n",
    "        # get the theta \n",
    "        theta, _ = model.get_theta(normalized_data_batch)\n",
    "        thetaAvg += theta.sum(0).unsqueeze(0) / args.num_docs_train\n",
    "        weighed_theta = sums * theta\n",
    "        thetaWeightedAvg += weighed_theta.sum(0).unsqueeze(0)\n",
    "        # let's print the progress as we go \n",
    "        if idx % 100 == 0 and idx > 0:\n",
    "            print('batch: {}/{}'.format(idx, len(indices)))\n",
    "    # finally the results are in \n",
    "    thetaWeightedAvg = thetaWeightedAvg.squeeze().cpu().numpy() / cnt\n",
    "    print('\\nThe 10 most used topics are {}'.format(thetaWeightedAvg.argsort()[::-1][:10]))\n",
    "\n",
    "    # Now we show the topics\n",
    "    # A nice visualisation is always welcome \n",
    "    beta = model.get_beta()\n",
    "    topic_indices = list(np.random.choice(num_topics, 10)) # 10 random topics\n",
    "    print('\\n')\n",
    "    for k in range(num_topics):#topic_indices:\n",
    "        gamma = beta[k]\n",
    "        top_words = list(gamma.cpu().numpy().argsort()[-num_words+1:][::-1])\n",
    "        topic_words = [vocab[a] for a in top_words]\n",
    "        print('Topic {}: {}'.format(k, topic_words))\n",
    "\n",
    "    # Why not, also showing a few embeddings \n",
    "    if train_embeddings:\n",
    "        # get embeddings from the model \n",
    "        try:rho_etm = model.rho.weight.cpu()\n",
    "        except:rho_etm = model.rho.cpu()\n",
    "        queries = ['andrew', 'woman', 'computer', 'sports', 'religion', 'man', 'love', \n",
    "                        'intelligence', 'money', 'politics', 'health', 'people', 'family']\n",
    "        print('\\n')\n",
    "        print('ETM embeddings...')\n",
    "        for word in queries:\n",
    "            print('word: {} .. etm neighbors: {}'.format(word, nearest_neighbors(word, rho_etm, vocab)))\n",
    "        print('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
